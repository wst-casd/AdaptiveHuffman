

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Scrapy 0.24.0 documentation</title>
  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  
    <link rel="top" title="None" href="index.html#document-index"/>
 
<!-- RTD Extra Head -->

  <!-- 
  Always link to the latest version, as canonical.
  http://docs.readthedocs.org/en/latest/canonical.html
  -->
  <link rel="canonical" href="http://doc.scrapy.org/en/latest/" />
  

<script type="text/javascript">
  // This is included here because other places don't have access to the pagename variable.
  var READTHEDOCS_DATA = {
    project: "scrapy",
    version: "0.24",
    language: "en",
    page: "index",
    theme: "sphinx_rtd_theme",
    docroot: "/docs/",
    source_suffix: ".rst",
    api_host: "https://readthedocs.org"
  }
  // Old variables
  var doc_version = "0.24";
  var doc_slug = "scrapy";
  var page_name = "index";
  var html_theme = "sphinx_rtd_theme";
</script>
<!-- RTD Analytics Code -->
<!-- Included in the header because you don't have a footer block. -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-17997319-1']);
  _gaq.push(['_trackPageview']);

  // User Analytics Code
  _gaq.push(['user._setAccount', 'UA-10231918-2']);
  _gaq.push(['user._trackPageview']);
  // End User Analytics Code


  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
<!-- end RTD Analytics Code -->
<!-- end RTD <extrahead> -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        <a href="index.html#document-index" class="fa fa-home"> Scrapy</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
        
            <ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/overview">Scrapy at a glance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/overview#pick-a-website">Pick a website</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/overview#define-the-data-you-want-to-scrape">Define the data you want to scrape</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/overview#write-a-spider-to-extract-the-data">Write a Spider to extract the data</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/overview#run-the-spider-to-extract-the-data">Run the spider to extract the data</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/overview#review-scraped-data">Review scraped data</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/overview#what-else">What else?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/overview#what-s-next">What&#8217;s next?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/install">Installation guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/install#pre-requisites">Pre-requisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/install#installing-scrapy">Installing Scrapy</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/install#platform-specific-installation-notes">Platform specific installation notes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/tutorial">Scrapy Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/tutorial#creating-a-project">Creating a project</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/tutorial#defining-our-item">Defining our Item</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/tutorial#our-first-spider">Our first Spider</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/tutorial#storing-the-scraped-data">Storing the scraped data</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-intro/tutorial#next-steps">Next steps</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/examples">Examples</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/commands">Command line tool</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/commands#default-structure-of-scrapy-projects">Default structure of Scrapy projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/commands#using-the-scrapy-tool">Using the <tt class="docutils literal"><span class="pre">scrapy</span></tt> tool</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/commands#available-tool-commands">Available tool commands</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/commands#custom-project-commands">Custom project commands</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/items">Items</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/items#declaring-items">Declaring Items</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/items#item-fields">Item Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/items#working-with-items">Working with Items</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/items#extending-items">Extending Items</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/items#item-objects">Item objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/items#field-objects">Field objects</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/spiders">Spiders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/spiders#spider-arguments">Spider arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/spiders#built-in-spiders-reference">Built-in spiders reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/selectors">Selectors</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/selectors#using-selectors">Using selectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/selectors#module-scrapy.selector">Built-in Selectors reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/loaders">Item Loaders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/loaders#using-item-loaders-to-populate-items">Using Item Loaders to populate items</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/loaders#input-and-output-processors">Input and Output processors</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/loaders#declaring-item-loaders">Declaring Item Loaders</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/loaders#declaring-input-and-output-processors">Declaring Input and Output Processors</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/loaders#item-loader-context">Item Loader Context</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/loaders#itemloader-objects">ItemLoader objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/loaders#reusing-and-extending-item-loaders">Reusing and extending Item Loaders</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/loaders#module-scrapy.contrib.loader.processor">Available built-in processors</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/shell">Scrapy shell</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/shell#launch-the-shell">Launch the shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/shell#using-the-shell">Using the shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/shell#example-of-shell-session">Example of shell session</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/shell#invoking-the-shell-from-spiders-to-inspect-responses">Invoking the shell from spiders to inspect responses</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/item-pipeline">Item Pipeline</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/item-pipeline#writing-your-own-item-pipeline">Writing your own item pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/item-pipeline#item-pipeline-example">Item pipeline example</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/item-pipeline#activating-an-item-pipeline-component">Activating an Item Pipeline component</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/feed-exports">Feed exports</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/feed-exports#serialization-formats">Serialization formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/feed-exports#storages">Storages</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/feed-exports#storage-uri-parameters">Storage URI parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/feed-exports#storage-backends">Storage backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/feed-exports#settings">Settings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/link-extractors">Link Extractors</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/link-extractors#module-scrapy.contrib.linkextractors">Built-in link extractors reference</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/logging">Logging</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/logging#log-levels">Log levels</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/logging#how-to-set-the-log-level">How to set the log level</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/logging#how-to-log-messages">How to log messages</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/logging#logging-from-spiders">Logging from Spiders</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/logging#module-scrapy.log">scrapy.log module</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/logging#logging-settings">Logging settings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/stats">Stats Collection</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/stats#common-stats-collector-uses">Common Stats Collector uses</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/stats#available-stats-collectors">Available Stats Collectors</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/email">Sending e-mail</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/email#quick-example">Quick example</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/email#mailsender-class-reference">MailSender class reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/email#mail-settings">Mail settings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/telnetconsole">Telnet Console</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/telnetconsole#how-to-access-the-telnet-console">How to access the telnet console</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/telnetconsole#available-variables-in-the-telnet-console">Available variables in the telnet console</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/telnetconsole#telnet-console-usage-examples">Telnet console usage examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/telnetconsole#telnet-console-signals">Telnet Console signals</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/telnetconsole#telnet-settings">Telnet settings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/webservice">Web Service</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/webservice#web-service-resources">Web service resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/webservice#web-service-settings">Web service settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/webservice#writing-a-web-service-resource">Writing a web service resource</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/webservice#examples-of-web-service-resources">Examples of web service resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/webservice#example-of-web-service-client">Example of web service client</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-faq">Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#how-does-scrapy-compare-to-beautifulsoup-or-lxml">How does Scrapy compare to BeautifulSoup or lxml?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#what-python-versions-does-scrapy-support">What Python versions does Scrapy support?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#does-scrapy-work-with-python-3">Does Scrapy work with Python 3?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#did-scrapy-steal-x-from-django">Did Scrapy &#8220;steal&#8221; X from Django?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#does-scrapy-work-with-http-proxies">Does Scrapy work with HTTP proxies?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#how-can-i-scrape-an-item-with-attributes-in-different-pages">How can I scrape an item with attributes in different pages?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#scrapy-crashes-with-importerror-no-module-named-win32api">Scrapy crashes with: ImportError: No module named win32api</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#how-can-i-simulate-a-user-login-in-my-spider">How can I simulate a user login in my spider?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#does-scrapy-crawl-in-breadth-first-or-depth-first-order">Does Scrapy crawl in breadth-first or depth-first order?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#my-scrapy-crawler-has-memory-leaks-what-can-i-do">My Scrapy crawler has memory leaks. What can I do?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#how-can-i-make-scrapy-consume-less-memory">How can I make Scrapy consume less memory?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#can-i-use-basic-http-authentication-in-my-spiders">Can I use Basic HTTP Authentication in my spiders?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#why-does-scrapy-download-pages-in-english-instead-of-my-native-language">Why does Scrapy download pages in English instead of my native language?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#where-can-i-find-some-example-scrapy-projects">Where can I find some example Scrapy projects?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#can-i-run-a-spider-without-creating-a-project">Can I run a spider without creating a project?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#i-get-filtered-offsite-request-messages-how-can-i-fix-them">I get &#8220;Filtered offsite request&#8221; messages. How can I fix them?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-production">What is the recommended way to deploy a Scrapy crawler in production?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#can-i-use-json-for-large-exports">Can I use JSON for large exports?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#can-i-return-twisted-deferreds-from-signal-handlers">Can I return (Twisted) deferreds from signal handlers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#what-does-the-response-status-code-999-means">What does the response status code 999 means?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#can-i-call-pdb-set-trace-from-my-spiders-to-debug-them">Can I call <tt class="docutils literal"><span class="pre">pdb.set_trace()</span></tt> from my spiders to debug them?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#simplest-way-to-dump-all-my-scraped-items-into-a-json-csv-xml-file">Simplest way to dump all my scraped items into a JSON/CSV/XML file?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms">What&#8217;s this huge cryptic <tt class="docutils literal"><span class="pre">__VIEWSTATE</span></tt> parameter used in some forms?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#what-s-the-best-way-to-parse-big-xml-csv-data-feeds">What&#8217;s the best way to parse big XML/CSV data feeds?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#does-scrapy-manage-cookies-automatically">Does Scrapy manage cookies automatically?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#how-can-i-see-the-cookies-being-sent-and-received-from-scrapy">How can I see the cookies being sent and received from Scrapy?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#how-can-i-instruct-a-spider-to-stop-itself">How can I instruct a spider to stop itself?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#how-can-i-prevent-my-scrapy-bot-from-getting-banned">How can I prevent my Scrapy bot from getting banned?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#should-i-use-spider-arguments-or-settings-to-configure-my-spider">Should I use spider arguments or settings to configure my spider?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#i-m-scraping-a-xml-document-and-my-xpath-selector-doesn-t-return-any-items">I&#8217;m scraping a XML document and my XPath selector doesn&#8217;t return any items</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-faq#i-m-getting-an-error-cannot-import-name-crawler">I&#8217;m getting an error: &#8220;cannot import name crawler&#8221;</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/debug">Debugging Spiders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/debug#parse-command">Parse Command</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/debug#scrapy-shell">Scrapy Shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/debug#open-in-browser">Open in browser</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/debug#logging">Logging</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/contracts">Spiders Contracts</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/contracts#custom-contracts">Custom Contracts</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/practices">Common Practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/practices#run-scrapy-from-a-script">Run Scrapy from a script</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/practices#running-multiple-spiders-in-the-same-process">Running multiple spiders in the same process</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/practices#distributed-crawls">Distributed crawls</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/practices#avoiding-getting-banned">Avoiding getting banned</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/practices#dynamic-creation-of-item-classes">Dynamic Creation of Item Classes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/broad-crawls">Broad Crawls</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/broad-crawls#increase-concurrency">Increase concurrency</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/broad-crawls#reduce-log-level">Reduce log level</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/broad-crawls#disable-cookies">Disable cookies</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/broad-crawls#disable-retries">Disable retries</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/broad-crawls#reduce-download-timeout">Reduce download timeout</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/broad-crawls#disable-redirects">Disable redirects</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/broad-crawls#enable-crawling-of-ajax-crawlable-pages">Enable crawling of &#8220;Ajax Crawlable Pages&#8221;</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/firefox">Using Firefox for scraping</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/firefox#caveats-with-inspecting-the-live-browser-dom">Caveats with inspecting the live browser DOM</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/firefox#useful-firefox-add-ons-for-scraping">Useful Firefox add-ons for scraping</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/firebug">Using Firebug for scraping</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/firebug#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/firebug#getting-links-to-follow">Getting links to follow</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/firebug#extracting-the-data">Extracting the data</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/leaks">Debugging memory leaks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/leaks#common-causes-of-memory-leaks">Common causes of memory leaks</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/leaks#debugging-memory-leaks-with-trackref">Debugging memory leaks with <tt class="docutils literal"><span class="pre">trackref</span></tt></a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/leaks#debugging-memory-leaks-with-guppy">Debugging memory leaks with Guppy</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/leaks#leaks-without-leaks">Leaks without leaks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/images">Downloading Item Images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/images#using-the-images-pipeline">Using the Images Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/images#usage-example">Usage example</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/images#enabling-your-images-pipeline">Enabling your Images Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/images#images-storage">Images Storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/images#additional-features">Additional features</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/images#module-scrapy.contrib.pipeline.images">Implementing your custom Images Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/images#custom-images-pipeline-example">Custom Images pipeline example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/ubuntu">Ubuntu packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/scrapyd">Scrapyd</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/autothrottle">AutoThrottle extension</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/autothrottle#design-goals">Design goals</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/autothrottle#how-it-works">How it works</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/autothrottle#throttling-algorithm">Throttling algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/autothrottle#settings">Settings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/benchmarking">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/jobs">Jobs: pausing and resuming crawls</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/jobs#job-directory">Job directory</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/jobs#how-to-use-it">How to use it</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/jobs#keeping-persistent-state-between-batches">Keeping persistent state between batches</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/jobs#persistence-gotchas">Persistence gotchas</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/djangoitem">DjangoItem</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/djangoitem#using-djangoitem">Using DjangoItem</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/djangoitem#djangoitem-caveats">DjangoItem caveats</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/djangoitem#django-settings-set-up">Django settings set up</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/architecture">Architecture overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/architecture#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/architecture#components">Components</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/architecture#data-flow">Data flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/architecture#event-driven-networking">Event-driven networking</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/downloader-middleware">Downloader Middleware</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/downloader-middleware#activating-a-downloader-middleware">Activating a downloader middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/downloader-middleware#writing-your-own-downloader-middleware">Writing your own downloader middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/downloader-middleware#built-in-downloader-middleware-reference">Built-in downloader middleware reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/spider-middleware">Spider Middleware</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/spider-middleware#activating-a-spider-middleware">Activating a spider middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/spider-middleware#writing-your-own-spider-middleware">Writing your own spider middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/spider-middleware#built-in-spider-middleware-reference">Built-in spider middleware reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/extensions">Extensions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/extensions#extension-settings">Extension settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/extensions#loading-activating-extensions">Loading &amp; activating extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/extensions#available-enabled-and-disabled-extensions">Available, enabled and disabled extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/extensions#disabling-an-extension">Disabling an extension</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/extensions#writing-your-own-extension">Writing your own extension</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/extensions#built-in-extensions-reference">Built-in extensions reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/api">Core API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/api#crawler-api">Crawler API</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/api#module-scrapy.settings">Settings API</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/api#module-scrapy.signalmanager">Signals API</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/api#stats-collector-api">Stats Collector API</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/request-response">Requests and Responses</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/request-response#request-objects">Request objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/request-response#request-meta-special-keys">Request.meta special keys</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/request-response#request-subclasses">Request subclasses</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/request-response#response-objects">Response objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/request-response#response-subclasses">Response subclasses</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/settings">Settings</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/settings#designating-the-settings">Designating the settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/settings#populating-the-settings">Populating the settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/settings#how-to-access-settings">How to access settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/settings#rationale-for-setting-names">Rationale for setting names</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/settings#built-in-settings-reference">Built-in settings reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/signals">Signals</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/signals#deferred-signal-handlers">Deferred signal handlers</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/signals#module-scrapy.signals">Built-in signals reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/exceptions">Exceptions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/exceptions#built-in-exceptions-reference">Built-in Exceptions reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/exporters">Item Exporters</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/exporters#using-item-exporters">Using Item Exporters</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/exporters#serialization-of-item-fields">Serialization of item fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-topics/exporters#built-in-item-exporters-reference">Built-in Item Exporters reference</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-news">Release notes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id1">0.24.0 (2014-06-26)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2014-02-14">0.22.2 (released 2014-02-14)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2014-02-08">0.22.1 (released 2014-02-08)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2014-01-17">0.22.0 (released 2014-01-17)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-12-09">0.20.2 (released 2013-12-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-11-28">0.20.1 (released 2013-11-28)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-11-08">0.20.0 (released 2013-11-08)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-10-10">0.18.4 (released 2013-10-10)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-10-03">0.18.3 (released 2013-10-03)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-09-03">0.18.2 (released 2013-09-03)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-08-27">0.18.1 (released 2013-08-27)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-08-09">0.18.0 (released 2013-08-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-05-30">0.16.5 (released 2013-05-30)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2013-01-23">0.16.4 (released 2013-01-23)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2012-12-07">0.16.3 (released 2012-12-07)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2012-11-09">0.16.2 (released 2012-11-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2012-10-26">0.16.1 (released 2012-10-26)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#released-2012-10-18">0.16.0 (released 2012-10-18)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id5">0.14.4</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id6">0.14.3</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id7">0.14.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id8">0.14.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id9">0.14</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id10">0.12</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id11">0.10</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id14">0.9</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id17">0.8</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-news#id18">0.7</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-contributing">Contributing to Scrapy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-contributing#reporting-bugs">Reporting bugs</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-contributing#writing-patches">Writing patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-contributing#submitting-patches">Submitting patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-contributing#coding-style">Coding style</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-contributing#scrapy-contrib">Scrapy Contrib</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-contributing#documentation-policies">Documentation policies</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-contributing#tests">Tests</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-versioning">Versioning and API Stability</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-versioning#id1">Versioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-versioning#api-stability">API Stability</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-experimental/index">Experimental features</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-experimental/index#add-commands-using-external-libraries">Add commands using external libraries</a></li>
</ul>
</li>
</ul>

        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html#document-index">Scrapy</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html#document-index">Docs</a> &raquo;</li>
      
    <li>Scrapy 0.24.0 documentation</li>
      <li class="wy-breadcrumbs-aside">
        
          <a href="https://github.com/scrapy/scrapy/blob/0.24/docs/index.rst" class="fa fa-github"> Edit on GitHub</a>
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            
  <div class="section" id="scrapy-version-documentation">
<span id="topics-index"></span><h1>Scrapy 0.24 documentation<a class="headerlink" href="#scrapy-version-documentation" title="Permalink to this headline">¶</a></h1>
<p>This documentation contains everything you need to know about Scrapy.</p>
<div class="section" id="getting-help">
<h2>Getting help<a class="headerlink" href="#getting-help" title="Permalink to this headline">¶</a></h2>
<p>Having trouble? We&#8217;d like to help!</p>
<ul class="simple">
<li>Try the <a class="reference internal" href="index.html#document-faq"><em>FAQ</em></a> &#8211; it&#8217;s got answers to some common questions.</li>
<li>Looking for specific information? Try the <a class="reference internal" href="genindex.html"><em>Index</em></a> or <a class="reference internal" href="py-modindex.html"><em>Module Index</em></a>.</li>
<li>Search for information in the <a class="reference external" href="http://groups.google.com/group/scrapy-users/">archives of the scrapy-users mailing list</a>, or
<a class="reference external" href="http://groups.google.com/group/scrapy-users/">post a question</a>.</li>
<li>Ask a question in the <a class="reference external" href="irc://irc.freenode.net/scrapy">#scrapy IRC channel</a>.</li>
<li>Report bugs with Scrapy in our <a class="reference external" href="https://github.com/scrapy/scrapy/issues">issue tracker</a>.</li>
</ul>
</div>
<div class="section" id="first-steps">
<h2>First steps<a class="headerlink" href="#first-steps" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-intro/overview"></span><div class="section" id="scrapy-at-a-glance">
<span id="intro-overview"></span><h3>Scrapy at a glance<a class="headerlink" href="#scrapy-at-a-glance" title="Permalink to this headline">¶</a></h3>
<p>Scrapy is an application framework for crawling web sites and extracting
structured data which can be used for a wide range of useful applications, like
data mining, information processing or historical archival.</p>
<p>Even though Scrapy was originally designed for <a class="reference external" href="http://en.wikipedia.org/wiki/Screen_scraping">screen scraping</a> (more
precisely, <a class="reference external" href="http://en.wikipedia.org/wiki/Web_scraping">web scraping</a>), it can also be used to extract data using APIs
(such as <a class="reference external" href="http://aws.amazon.com/associates/">Amazon Associates Web Services</a>) or as a general purpose web
crawler.</p>
<p>The purpose of this document is to introduce you to the concepts behind Scrapy
so you can get an idea of how it works and decide if Scrapy is what you need.</p>
<p>When you&#8217;re ready to start a project, you can <a class="reference internal" href="index.html#intro-tutorial"><em>start with the tutorial</em></a>.</p>
<div class="section" id="pick-a-website">
<h4>Pick a website<a class="headerlink" href="#pick-a-website" title="Permalink to this headline">¶</a></h4>
<p>So you need to extract some information from a website, but the website doesn&#8217;t
provide any API or mechanism to access that info programmatically.  Scrapy can
help you extract that information.</p>
<p>Let&#8217;s say we want to extract the URL, name, description and size of all torrent
files added today in the <a class="reference external" href="http://www.mininova.org">Mininova</a> site.</p>
<p>The list of all torrents added today can be found on this page:</p>
<blockquote>
<div><a class="reference external" href="http://www.mininova.org/today">http://www.mininova.org/today</a></div></blockquote>
</div>
<div class="section" id="define-the-data-you-want-to-scrape">
<span id="intro-overview-item"></span><h4>Define the data you want to scrape<a class="headerlink" href="#define-the-data-you-want-to-scrape" title="Permalink to this headline">¶</a></h4>
<p>The first thing is to define the data we want to scrape. In Scrapy, this is
done through <a class="reference internal" href="index.html#topics-items"><em>Scrapy Items</em></a> (Torrent files, in this case).</p>
<p>This would be our Item:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">TorrentItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">description</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="write-a-spider-to-extract-the-data">
<h4>Write a Spider to extract the data<a class="headerlink" href="#write-a-spider-to-extract-the-data" title="Permalink to this headline">¶</a></h4>
<p>The next thing is to write a Spider which defines the start URL
(<a class="reference external" href="http://www.mininova.org/today">http://www.mininova.org/today</a>), the rules for following links and the rules
for extracting the data from pages.</p>
<p>If we take a look at that page content we&#8217;ll see that all torrent URLs are like
<tt class="docutils literal"><span class="pre">http://www.mininova.org/tor/NUMBER</span></tt> where <tt class="docutils literal"><span class="pre">NUMBER</span></tt> is an integer. We&#8217;ll use
that to construct the regular expression for the links to follow: <tt class="docutils literal"><span class="pre">/tor/\d+</span></tt>.</p>
<p>We&#8217;ll use <a class="reference external" href="http://www.w3.org/TR/xpath">XPath</a> for selecting the data to extract from the web page HTML
source. Let&#8217;s take one of those torrent pages:</p>
<blockquote>
<div><a class="reference external" href="http://www.mininova.org/tor/2676093">http://www.mininova.org/tor/2676093</a></div></blockquote>
<p>And look at the page HTML source to construct the XPath to select the data we
want which is: torrent name, description and size.</p>
<p>By looking at the page HTML source we can see that the file name is contained
inside a <tt class="docutils literal"><span class="pre">&lt;h1&gt;</span></tt> tag:</p>
<div class="highlight-html"><div class="highlight"><pre><span class="nt">&lt;h1&gt;</span>Darwin - The Evolution Of An Exhibition<span class="nt">&lt;/h1&gt;</span>
</pre></div>
</div>
<p>An XPath expression to extract the name could be:</p>
<div class="highlight-none"><div class="highlight"><pre>//h1/text()
</pre></div>
</div>
<p>And the description is contained inside a <tt class="docutils literal"><span class="pre">&lt;div&gt;</span></tt> tag with <tt class="docutils literal"><span class="pre">id=&quot;description&quot;</span></tt>:</p>
<div class="highlight-html"><div class="highlight"><pre><span class="nt">&lt;h2&gt;</span>Description:<span class="nt">&lt;/h2&gt;</span>

<span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&quot;description&quot;</span><span class="nt">&gt;</span>
Short documentary made for Plymouth City Museum and Art Gallery regarding the setup of an exhibit about Charles Darwin in conjunction with the 200th anniversary of his birth.

...
</pre></div>
</div>
<p>An XPath expression to select the description could be:</p>
<div class="highlight-none"><div class="highlight"><pre>//div[@id=&#39;description&#39;]
</pre></div>
</div>
<p>Finally, the file size is contained in the second <tt class="docutils literal"><span class="pre">&lt;p&gt;</span></tt> tag inside the <tt class="docutils literal"><span class="pre">&lt;div&gt;</span></tt>
tag with <tt class="docutils literal"><span class="pre">id=specifications</span></tt>:</p>
<div class="highlight-html"><div class="highlight"><pre><span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&quot;specifications&quot;</span><span class="nt">&gt;</span>

<span class="nt">&lt;p&gt;</span>
<span class="nt">&lt;strong&gt;</span>Category:<span class="nt">&lt;/strong&gt;</span>
<span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">&quot;/cat/4&quot;</span><span class="nt">&gt;</span>Movies<span class="nt">&lt;/a&gt;</span> <span class="ni">&amp;gt;</span> <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">&quot;/sub/35&quot;</span><span class="nt">&gt;</span>Documentary<span class="nt">&lt;/a&gt;</span>
<span class="nt">&lt;/p&gt;</span>

<span class="nt">&lt;p&gt;</span>
<span class="nt">&lt;strong&gt;</span>Total size:<span class="nt">&lt;/strong&gt;</span>
150.62<span class="ni">&amp;nbsp;</span>megabyte<span class="nt">&lt;/p&gt;</span>
</pre></div>
</div>
<p>An XPath expression to select the file size could be:</p>
<div class="highlight-none"><div class="highlight"><pre>//div[@id=&#39;specifications&#39;]/p[2]/text()[2]
</pre></div>
</div>
<p>For more information about XPath see the <a class="reference external" href="http://www.w3.org/TR/xpath">XPath reference</a>.</p>
<p>Finally, here&#8217;s the spider code:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span><span class="p">,</span> <span class="n">Rule</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.linkextractors</span> <span class="kn">import</span> <span class="n">LinkExtractor</span>

<span class="k">class</span> <span class="nc">MininovaSpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;mininova&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;mininova.org&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.mininova.org/today&#39;</span><span class="p">]</span>
    <span class="n">rules</span> <span class="o">=</span> <span class="p">[</span><span class="n">Rule</span><span class="p">(</span><span class="n">LinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;/tor/\d+&#39;</span><span class="p">]),</span> <span class="s">&#39;parse_torrent&#39;</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">parse_torrent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">torrent</span> <span class="o">=</span> <span class="n">TorrentItem</span><span class="p">()</span>
        <span class="n">torrent</span><span class="p">[</span><span class="s">&#39;url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
        <span class="n">torrent</span><span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//h1/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="n">torrent</span><span class="p">[</span><span class="s">&#39;description&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//div[@id=&#39;description&#39;]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="n">torrent</span><span class="p">[</span><span class="s">&#39;size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//div[@id=&#39;info-left&#39;]/p[2]/text()[2]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torrent</span>
</pre></div>
</div>
<p>The <tt class="docutils literal"><span class="pre">TorrentItem</span></tt> class is <a class="reference internal" href="index.html#intro-overview-item"><em>defined above</em></a>.</p>
</div>
<div class="section" id="run-the-spider-to-extract-the-data">
<h4>Run the spider to extract the data<a class="headerlink" href="#run-the-spider-to-extract-the-data" title="Permalink to this headline">¶</a></h4>
<p>Finally, we&#8217;ll run the spider to crawl the site and output the file
<tt class="docutils literal"><span class="pre">scraped_data.json</span></tt> with the scraped data in JSON format:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy crawl mininova -o scraped_data.json
</pre></div>
</div>
<p>This uses <a class="reference internal" href="index.html#topics-feed-exports"><em>feed exports</em></a> to generate the JSON file.
You can easily change the export format (XML or CSV, for example) or the
storage backend (FTP or <a class="reference external" href="http://aws.amazon.com/s3/">Amazon S3</a>, for example).</p>
<p>You can also write an <a class="reference internal" href="index.html#topics-item-pipeline"><em>item pipeline</em></a> to store the
items in a database very easily.</p>
</div>
<div class="section" id="review-scraped-data">
<h4>Review scraped data<a class="headerlink" href="#review-scraped-data" title="Permalink to this headline">¶</a></h4>
<p>If you check the <tt class="docutils literal"><span class="pre">scraped_data.json</span></tt> file after the process finishes, you&#8217;ll
see the scraped items there:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[{</span><span class="s">&quot;url&quot;</span><span class="p">:</span> <span class="s">&quot;http://www.mininova.org/tor/2676093&quot;</span><span class="p">,</span> <span class="s">&quot;name&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;Darwin - The Evolution Of An Exhibition&quot;</span><span class="p">],</span> <span class="s">&quot;description&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;Short documentary made for Plymouth ...&quot;</span><span class="p">],</span> <span class="s">&quot;size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;150.62 megabyte&quot;</span><span class="p">]},</span>
<span class="c"># ... other items ...</span>
<span class="p">]</span>
</pre></div>
</div>
<p>You&#8217;ll notice that all field values (except for the <tt class="docutils literal"><span class="pre">url</span></tt> which was assigned
directly) are actually lists. This is because the <a class="reference internal" href="index.html#topics-selectors"><em>selectors</em></a> return lists. You may want to store single values, or
perform some additional parsing/cleansing to the values. That&#8217;s what
<a class="reference internal" href="index.html#topics-loaders"><em>Item Loaders</em></a> are for.</p>
</div>
<div class="section" id="what-else">
<span id="topics-whatelse"></span><h4>What else?<a class="headerlink" href="#what-else" title="Permalink to this headline">¶</a></h4>
<p>You&#8217;ve seen how to extract and store items from a website using Scrapy, but
this is just the surface. Scrapy provides a lot of powerful features for making
scraping easy and efficient, such as:</p>
<ul class="simple">
<li>Built-in support for <a class="reference internal" href="index.html#topics-selectors"><em>selecting and extracting</em></a> data
from HTML and XML sources</li>
<li>Built-in support for cleaning and sanitizing the scraped data using a
collection of reusable filters (called <a class="reference internal" href="index.html#topics-loaders"><em>Item Loaders</em></a>)
shared between all the spiders.</li>
<li>Built-in support for <a class="reference internal" href="index.html#topics-feed-exports"><em>generating feed exports</em></a> in
multiple formats (JSON, CSV, XML) and storing them in multiple backends (FTP,
S3, local filesystem)</li>
<li>A media pipeline for <a class="reference internal" href="index.html#topics-images"><em>automatically downloading images</em></a>
(or any other media) associated with the scraped items</li>
<li>Support for <a class="reference internal" href="#extending-scrapy"><em>extending Scrapy</em></a> by plugging
your own functionality using <a class="reference internal" href="index.html#topics-signals"><em>signals</em></a> and a
well-defined API (middlewares, <a class="reference internal" href="index.html#topics-extensions"><em>extensions</em></a>, and
<a class="reference internal" href="index.html#topics-item-pipeline"><em>pipelines</em></a>).</li>
<li>Wide range of built-in middlewares and extensions for:<ul>
<li>cookies and session handling</li>
<li>HTTP compression</li>
<li>HTTP authentication</li>
<li>HTTP cache</li>
<li>user-agent spoofing</li>
<li>robots.txt</li>
<li>crawl depth restriction</li>
<li>and more</li>
</ul>
</li>
<li>Robust encoding support and auto-detection, for dealing with foreign,
non-standard and broken encoding declarations.</li>
<li>Support for creating spiders based on pre-defined templates, to speed up
spider creation and make their code more consistent on large projects. See
<a class="reference internal" href="index.html#std:command-genspider"><tt class="xref std std-command docutils literal"><span class="pre">genspider</span></tt></a> command for more details.</li>
<li>Extensible <a class="reference internal" href="index.html#topics-stats"><em>stats collection</em></a> for multiple spider
metrics, useful for monitoring the performance of your spiders and detecting
when they get broken</li>
<li>An <a class="reference internal" href="index.html#topics-shell"><em>Interactive shell console</em></a> for trying XPaths, very
useful for writing and debugging your spiders</li>
<li>A <a class="reference internal" href="index.html#topics-scrapyd"><em>System service</em></a> designed to ease the deployment and
run of your spiders in production.</li>
<li>A built-in <a class="reference internal" href="index.html#topics-webservice"><em>Web service</em></a> for monitoring and
controlling your bot</li>
<li>A <a class="reference internal" href="index.html#topics-telnetconsole"><em>Telnet console</em></a> for hooking into a Python
console running inside your Scrapy process, to introspect and debug your
crawler</li>
<li><a class="reference internal" href="index.html#topics-logging"><em>Logging</em></a> facility that you can hook on to for catching
errors during the scraping process.</li>
<li>Support for crawling based on URLs discovered through <a class="reference external" href="http://www.sitemaps.org">Sitemaps</a></li>
<li>A caching DNS resolver</li>
</ul>
</div>
<div class="section" id="what-s-next">
<h4>What&#8217;s next?<a class="headerlink" href="#what-s-next" title="Permalink to this headline">¶</a></h4>
<p>The next obvious steps are for you to <a class="reference external" href="http://scrapy.org/download/">download Scrapy</a>, read <a class="reference internal" href="index.html#intro-tutorial"><em>the
tutorial</em></a> and join <a class="reference external" href="http://scrapy.org/community/">the community</a>. Thanks for your
interest!</p>
</div>
</div>
<span id="document-intro/install"></span><div class="section" id="installation-guide">
<span id="intro-install"></span><h3>Installation guide<a class="headerlink" href="#installation-guide" title="Permalink to this headline">¶</a></h3>
<div class="section" id="pre-requisites">
<h4>Pre-requisites<a class="headerlink" href="#pre-requisites" title="Permalink to this headline">¶</a></h4>
<p>The installation steps assume that you have the following things installed:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.python.org">Python</a> 2.7</li>
<li><a class="reference external" href="http://lxml.de/">lxml</a>. Most Linux distributions ships prepackaged versions of lxml. Otherwise refer to <a class="reference external" href="http://lxml.de/installation.html">http://lxml.de/installation.html</a></li>
<li><a class="reference external" href="https://pypi.python.org/pypi/pyOpenSSL">OpenSSL</a>. This comes preinstalled in all operating systems except Windows (see <a class="reference internal" href="index.html#intro-install-platform-notes"><em>Platform specific installation notes</em></a>)</li>
<li><a class="reference external" href="http://www.pip-installer.org/en/latest/installing.html">pip</a> or <a class="reference external" href="http://pypi.python.org/pypi/setuptools">easy_install</a> Python package managers</li>
</ul>
</div>
<div class="section" id="installing-scrapy">
<h4>Installing Scrapy<a class="headerlink" href="#installing-scrapy" title="Permalink to this headline">¶</a></h4>
<p>You can install Scrapy using easy_install or pip (which is the canonical way to
distribute and install Python packages).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Check <a class="reference internal" href="index.html#intro-install-platform-notes"><em>Platform specific installation notes</em></a> first.</p>
</div>
<p>To install using pip:</p>
<div class="highlight-python"><div class="highlight"><pre>pip install Scrapy
</pre></div>
</div>
<p>To install using easy_install:</p>
<div class="highlight-python"><div class="highlight"><pre>easy_install Scrapy
</pre></div>
</div>
</div>
<div class="section" id="platform-specific-installation-notes">
<span id="intro-install-platform-notes"></span><h4>Platform specific installation notes<a class="headerlink" href="#platform-specific-installation-notes" title="Permalink to this headline">¶</a></h4>
<div class="section" id="windows">
<h5>Windows<a class="headerlink" href="#windows" title="Permalink to this headline">¶</a></h5>
<p>After installing Python, follow these steps before installing Scrapy:</p>
<ul class="simple">
<li>add the <tt class="docutils literal"><span class="pre">C:\python27\Scripts</span></tt> and <tt class="docutils literal"><span class="pre">C:\python27</span></tt> folders to the system
path by adding those directories to the <tt class="docutils literal"><span class="pre">PATH</span></tt> environment variable from
the <a class="reference external" href="http://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/sysdm_advancd_environmnt_addchange_variable.mspx">Control Panel</a>.</li>
<li>install OpenSSL by following these steps:<ol class="arabic">
<li>go to <a class="reference external" href="http://slproweb.com/products/Win32OpenSSL.html">Win32 OpenSSL page</a></li>
<li>download Visual C++ 2008 redistributables for your Windows and architecture</li>
<li>download OpenSSL for your Windows and architecture (the regular version, not the light one)</li>
<li>add the <tt class="docutils literal"><span class="pre">c:\openssl-win32\bin</span></tt> (or similar) directory to your <tt class="docutils literal"><span class="pre">PATH</span></tt>, the same way you added <tt class="docutils literal"><span class="pre">python27</span></tt> in the first step`` in the first step</li>
</ol>
</li>
<li>some binary packages that Scrapy depends on (like Twisted, lxml and pyOpenSSL) require a compiler available to install, and fail if you don&#8217;t have Visual Studio installed. You can find Windows installers for those in the following links. Make sure you respect your Python version and Windows architecture.<ul>
<li>pywin32: <a class="reference external" href="http://sourceforge.net/projects/pywin32/files/">http://sourceforge.net/projects/pywin32/files/</a></li>
<li>Twisted: <a class="reference external" href="http://twistedmatrix.com/trac/wiki/Downloads">http://twistedmatrix.com/trac/wiki/Downloads</a></li>
<li>zope.interface: download the egg from <a class="reference external" href="http://pypi.python.org/pypi/zope.interface">zope.interface pypi page</a> and install it by running <tt class="docutils literal"><span class="pre">easy_install</span> <span class="pre">file.egg</span></tt></li>
<li>lxml: <a class="reference external" href="http://pypi.python.org/pypi/lxml/">http://pypi.python.org/pypi/lxml/</a></li>
<li>pyOpenSSL: <a class="reference external" href="https://launchpad.net/pyopenssl">https://launchpad.net/pyopenssl</a></li>
</ul>
</li>
</ul>
<p>Finally, this page contains many precompiled Python binary libraries, which may
come handy to fulfill Scrapy dependencies:</p>
<blockquote>
<div><a class="reference external" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/">http://www.lfd.uci.edu/~gohlke/pythonlibs/</a></div></blockquote>
<div class="section" id="ubuntu-9-10-or-above">
<h6>Ubuntu 9.10 or above<a class="headerlink" href="#ubuntu-9-10-or-above" title="Permalink to this headline">¶</a></h6>
<p><strong>Don&#8217;t</strong> use the <tt class="docutils literal"><span class="pre">python-scrapy</span></tt> package provided by Ubuntu, they are
typically too old and slow to catch up with latest Scrapy.</p>
<p>Instead, use the official <a class="reference internal" href="index.html#topics-ubuntu"><em>Ubuntu Packages</em></a>, which already
solve all dependencies for you and are continuously updated with the latest bug
fixes.</p>
</div>
</div>
</div>
</div>
<span id="document-intro/tutorial"></span><div class="section" id="scrapy-tutorial">
<span id="intro-tutorial"></span><h3>Scrapy Tutorial<a class="headerlink" href="#scrapy-tutorial" title="Permalink to this headline">¶</a></h3>
<p>In this tutorial, we&#8217;ll assume that Scrapy is already installed on your system.
If that&#8217;s not the case, see <a class="reference internal" href="index.html#intro-install"><em>Installation guide</em></a>.</p>
<p>We are going to use <a class="reference external" href="http://www.dmoz.org/">Open directory project (dmoz)</a> as
our example domain to scrape.</p>
<p>This tutorial will walk you through these tasks:</p>
<ol class="arabic simple">
<li>Creating a new Scrapy project</li>
<li>Defining the Items you will extract</li>
<li>Writing a <a class="reference internal" href="index.html#topics-spiders"><em>spider</em></a> to crawl a site and extract
<a class="reference internal" href="index.html#topics-items"><em>Items</em></a></li>
<li>Writing an <a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a> to store the
extracted Items</li>
</ol>
<p>Scrapy is written in <a class="reference external" href="http://www.python.org">Python</a>. If you&#8217;re new to the language you might want to
start by getting an idea of what the language is like, to get the most out of
Scrapy.  If you&#8217;re already familiar with other languages, and want to learn
Python quickly, we recommend <a class="reference external" href="http://learnpythonthehardway.org/book/">Learn Python The Hard Way</a>.  If you&#8217;re new to programming
and want to start with Python, take a look at <a class="reference external" href="http://wiki.python.org/moin/BeginnersGuide/NonProgrammers">this list of Python resources
for non-programmers</a>.</p>
<div class="section" id="creating-a-project">
<h4>Creating a project<a class="headerlink" href="#creating-a-project" title="Permalink to this headline">¶</a></h4>
<p>Before you start scraping, you will have set up a new Scrapy project. Enter a
directory where you&#8217;d like to store your code and then run:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy startproject tutorial
</pre></div>
</div>
<p>This will create a <tt class="docutils literal"><span class="pre">tutorial</span></tt> directory with the following contents:</p>
<div class="highlight-python"><div class="highlight"><pre>tutorial/
    scrapy.cfg
    tutorial/
        __init__.py
        items.py
        pipelines.py
        settings.py
        spiders/
            __init__.py
            ...
</pre></div>
</div>
<p>These are basically:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">scrapy.cfg</span></tt>: the project configuration file</li>
<li><tt class="docutils literal"><span class="pre">tutorial/</span></tt>: the project&#8217;s python module, you&#8217;ll later import your code from
here.</li>
<li><tt class="docutils literal"><span class="pre">tutorial/items.py</span></tt>: the project&#8217;s items file.</li>
<li><tt class="docutils literal"><span class="pre">tutorial/pipelines.py</span></tt>: the project&#8217;s pipelines file.</li>
<li><tt class="docutils literal"><span class="pre">tutorial/settings.py</span></tt>: the project&#8217;s settings file.</li>
<li><tt class="docutils literal"><span class="pre">tutorial/spiders/</span></tt>: a directory where you&#8217;ll later put your spiders.</li>
</ul>
</div>
<div class="section" id="defining-our-item">
<h4>Defining our Item<a class="headerlink" href="#defining-our-item" title="Permalink to this headline">¶</a></h4>
<p><cite>Items</cite> are containers that will be loaded with the scraped data; they work
like simple python dicts but provide additional protection against populating
undeclared fields, to prevent typos.</p>
<p>They are declared by creating a <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.Item</span></tt></a> class and defining
its attributes as <a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.Field</span></tt></a> objects, like you will in an ORM
(don&#8217;t worry if you&#8217;re not familiar with ORMs, you will see that this is an
easy task).</p>
<p>We begin by modeling the item that we will use to hold the sites data obtained
from dmoz.org, as we want to capture the name, url and description of the
sites, we define fields for each of these three attributes. To do that, we edit
<tt class="docutils literal"><span class="pre">items.py</span></tt>, found in the <tt class="docutils literal"><span class="pre">tutorial</span></tt> directory. Our Item class looks like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">DmozItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">link</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">desc</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<p>This may seem complicated at first, but defining the item allows you to use other handy
components of Scrapy that need to know how your item looks.</p>
</div>
<div class="section" id="our-first-spider">
<h4>Our first Spider<a class="headerlink" href="#our-first-spider" title="Permalink to this headline">¶</a></h4>
<p>Spiders are user-written classes used to scrape information from a domain (or group
of domains).</p>
<p>They define an initial list of URLs to download, how to follow links, and how
to parse the contents of those pages to extract <a class="reference internal" href="index.html#topics-items"><em>items</em></a>.</p>
<p>To create a Spider, you must subclass <a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.Spider</span></tt></a> and
define the three main mandatory attributes:</p>
<ul>
<li><p class="first"><a class="reference internal" href="index.html#scrapy.spider.Spider.name" title="scrapy.spider.Spider.name"><tt class="xref py py-attr docutils literal"><span class="pre">name</span></tt></a>: identifies the Spider. It must be
unique, that is, you can&#8217;t set the same name for different Spiders.</p>
</li>
<li><p class="first"><a class="reference internal" href="index.html#scrapy.spider.Spider.start_urls" title="scrapy.spider.Spider.start_urls"><tt class="xref py py-attr docutils literal"><span class="pre">start_urls</span></tt></a>: is a list of URLs where the
Spider will begin to crawl from.  So, the first pages downloaded will be those
listed here. The subsequent URLs will be generated successively from data
contained in the start URLs.</p>
</li>
<li><p class="first"><a class="reference internal" href="index.html#scrapy.spider.Spider.parse" title="scrapy.spider.Spider.parse"><tt class="xref py py-meth docutils literal"><span class="pre">parse()</span></tt></a> is a method of the spider, which will
be called with the downloaded <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object of each
start URL. The response is passed to the method as the first and only
argument.</p>
<p>This method is responsible for parsing the response data and extracting
scraped data (as scraped items) and more URLs to follow.</p>
<p>The <a class="reference internal" href="index.html#scrapy.spider.Spider.parse" title="scrapy.spider.Spider.parse"><tt class="xref py py-meth docutils literal"><span class="pre">parse()</span></tt></a> method is in charge of processing
the response and returning scraped data (as <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a>
objects) and more URLs to follow (as <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> objects).</p>
</li>
</ul>
<p>This is the code for our first Spider; save it in a file named
<tt class="docutils literal"><span class="pre">dmoz_spider.py</span></tt> under the <tt class="docutils literal"><span class="pre">tutorial/spiders</span></tt> directory:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">DmozSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&quot;dmoz&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;dmoz.org&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;</span><span class="p">,</span>
        <span class="s">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="crawling">
<h5>Crawling<a class="headerlink" href="#crawling" title="Permalink to this headline">¶</a></h5>
<p>To put our spider to work, go to the project&#8217;s top level directory and run:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy crawl dmoz
</pre></div>
</div>
<p>The <tt class="docutils literal"><span class="pre">crawl</span> <span class="pre">dmoz</span></tt> command runs the spider for the <tt class="docutils literal"><span class="pre">dmoz.org</span></tt> domain. You
will get an output similar to this:</p>
<div class="highlight-python"><div class="highlight"><pre>2014-01-23 18:13:07-0400 [scrapy] INFO: Scrapy started (bot: tutorial)
2014-01-23 18:13:07-0400 [scrapy] INFO: Optional features available: ...
2014-01-23 18:13:07-0400 [scrapy] INFO: Overridden settings: {}
2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled extensions: ...
2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled downloader middlewares: ...
2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled spider middlewares: ...
2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled item pipelines: ...
2014-01-23 18:13:07-0400 [dmoz] INFO: Spider opened
2014-01-23 18:13:08-0400 [dmoz] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt; (referer: None)
2014-01-23 18:13:09-0400 [dmoz] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)
2014-01-23 18:13:09-0400 [dmoz] INFO: Closing spider (finished)
</pre></div>
</div>
<p>Pay attention to the lines containing <tt class="docutils literal"><span class="pre">[dmoz]</span></tt>, which corresponds to our
spider. You can see a log line for each URL defined in <tt class="docutils literal"><span class="pre">start_urls</span></tt>. Because
these URLs are the starting ones, they have no referrers, which is shown at the
end of the log line, where it says <tt class="docutils literal"><span class="pre">(referer:</span> <span class="pre">None)</span></tt>.</p>
<p>But more interesting, as our <tt class="docutils literal"><span class="pre">parse</span></tt> method instructs, two files have been
created: <em>Books</em> and <em>Resources</em>, with the content of both URLs.</p>
<div class="section" id="what-just-happened-under-the-hood">
<h6>What just happened under the hood?<a class="headerlink" href="#what-just-happened-under-the-hood" title="Permalink to this headline">¶</a></h6>
<p>Scrapy creates <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.Request</span></tt></a> objects
for each URL in the <tt class="docutils literal"><span class="pre">start_urls</span></tt> attribute of the Spider, and assigns
them the <tt class="docutils literal"><span class="pre">parse</span></tt> method of the spider as their callback function.</p>
<p>These Requests are scheduled, then executed, and <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.http.Response</span></tt></a>
objects are returned and then fed back to the spider, through the
<a class="reference internal" href="index.html#scrapy.spider.Spider.parse" title="scrapy.spider.Spider.parse"><tt class="xref py py-meth docutils literal"><span class="pre">parse()</span></tt></a> method.</p>
</div>
</div>
<div class="section" id="extracting-items">
<h5>Extracting Items<a class="headerlink" href="#extracting-items" title="Permalink to this headline">¶</a></h5>
<div class="section" id="introduction-to-selectors">
<h6>Introduction to Selectors<a class="headerlink" href="#introduction-to-selectors" title="Permalink to this headline">¶</a></h6>
<p>There are several ways to extract data from web pages. Scrapy uses a mechanism
based on <a class="reference external" href="http://www.w3.org/TR/xpath">XPath</a> or <a class="reference external" href="http://www.w3.org/TR/selectors">CSS</a> expressions called <a class="reference internal" href="index.html#topics-selectors"><em>Scrapy Selectors</em></a>.  For more information about selectors and other extraction
mechanisms see the <a class="reference internal" href="index.html#topics-selectors"><em>Selectors documentation</em></a>.</p>
<p>Here are some examples of XPath expressions and their meanings:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">/html/head/title</span></tt>: selects the <tt class="docutils literal"><span class="pre">&lt;title&gt;</span></tt> element, inside the <tt class="docutils literal"><span class="pre">&lt;head&gt;</span></tt>
element of a HTML document</li>
<li><tt class="docutils literal"><span class="pre">/html/head/title/text()</span></tt>: selects the text inside the aforementioned
<tt class="docutils literal"><span class="pre">&lt;title&gt;</span></tt> element.</li>
<li><tt class="docutils literal"><span class="pre">//td</span></tt>: selects all the <tt class="docutils literal"><span class="pre">&lt;td&gt;</span></tt> elements</li>
<li><tt class="docutils literal"><span class="pre">//div[&#64;class=&quot;mine&quot;]</span></tt>: selects all <tt class="docutils literal"><span class="pre">div</span></tt> elements which contain an
attribute <tt class="docutils literal"><span class="pre">class=&quot;mine&quot;</span></tt></li>
</ul>
<p>These are just a couple of simple examples of what you can do with XPath, but
XPath expressions are indeed much more powerful. To learn more about XPath we
recommend <a class="reference external" href="http://www.w3schools.com/XPath/default.asp">this XPath tutorial</a>.</p>
<p>For working with XPaths, Scrapy provides <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a>
class and convenient shortcuts to avoid instantiating selectors yourself
everytime you need to select something from a response.</p>
<p>You can see selectors as objects that represent nodes in the document
structure. So, the first instantiated selectors are associated with the root
node, or the entire document.</p>
<p>Selectors have four basic methods (click on the method to see the complete API
documentation):</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#scrapy.selector.Selector.xpath" title="scrapy.selector.Selector.xpath"><tt class="xref py py-meth docutils literal"><span class="pre">xpath()</span></tt></a>: returns a list of selectors, each of
them representing the nodes selected by the xpath expression given as
argument.</li>
<li><a class="reference internal" href="index.html#scrapy.selector.Selector.css" title="scrapy.selector.Selector.css"><tt class="xref py py-meth docutils literal"><span class="pre">css()</span></tt></a>: returns a list of selectors, each of
them representing the nodes selected by the CSS expression given as argument.</li>
<li><a class="reference internal" href="index.html#scrapy.selector.Selector.extract" title="scrapy.selector.Selector.extract"><tt class="xref py py-meth docutils literal"><span class="pre">extract()</span></tt></a>: returns a unicode string with the
selected data.</li>
<li><a class="reference internal" href="index.html#scrapy.selector.Selector.re" title="scrapy.selector.Selector.re"><tt class="xref py py-meth docutils literal"><span class="pre">re()</span></tt></a>: returns a list of unicode strings
extracted by applying the regular expression given as argument.</li>
</ul>
</div>
<div class="section" id="trying-selectors-in-the-shell">
<h6>Trying Selectors in the Shell<a class="headerlink" href="#trying-selectors-in-the-shell" title="Permalink to this headline">¶</a></h6>
<p>To illustrate the use of Selectors we&#8217;re going to use the built-in <a class="reference internal" href="index.html#topics-shell"><em>Scrapy
shell</em></a>, which also requires IPython (an extended Python console)
installed on your system.</p>
<p>To start a shell, you must go to the project&#8217;s top level directory and run:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy shell &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Remember to always enclose urls with quotes when running Scrapy shell from
command-line, otherwise urls containing arguments (ie. <tt class="docutils literal"><span class="pre">&amp;</span></tt> character)
will not work.</p>
</div>
<p>This is what the shell looks like:</p>
<div class="highlight-python"><div class="highlight"><pre>[ ... Scrapy log here ... ]

2014-01-23 17:11:42-0400 [default] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)
[s] Available Scrapy objects:
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x3636b50&gt;
[s]   item       {}
[s]   request    &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;
[s]   response   &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;
[s]   settings   &lt;scrapy.settings.Settings object at 0x3fadc50&gt;
[s]   spider     &lt;Spider &#39;default&#39; at 0x3cebf50&gt;
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser

In [1]:
</pre></div>
</div>
<p>After the shell loads, you will have the response fetched in a local
<tt class="docutils literal"><span class="pre">response</span></tt> variable, so if you type <tt class="docutils literal"><span class="pre">response.body</span></tt> you will see the body
of the response, or you can type <tt class="docutils literal"><span class="pre">response.headers</span></tt> to see its headers.</p>
<p>More important, if you type <tt class="docutils literal"><span class="pre">response.selector</span></tt> you will access a selector
object you can use to query the response, and convenient shortcuts like
<tt class="docutils literal"><span class="pre">response.xpath()</span></tt> and <tt class="docutils literal"><span class="pre">response.css()</span></tt> mapping to
<tt class="docutils literal"><span class="pre">response.selector.xpath()</span></tt> and <tt class="docutils literal"><span class="pre">response.selector.css()</span></tt></p>
<p>So let&#8217;s try it:</p>
<div class="highlight-python"><div class="highlight"><pre>In [1]: response.xpath(&#39;//title&#39;)
Out[1]: [&lt;Selector xpath=&#39;//title&#39; data=u&#39;&lt;title&gt;Open Directory - Computers: Progr&#39;&gt;]

In [2]: response.xpath(&#39;//title&#39;).extract()
Out[2]: [u&#39;&lt;title&gt;Open Directory - Computers: Programming: Languages: Python: Books&lt;/title&gt;&#39;]

In [3]: response.xpath(&#39;//title/text()&#39;)
Out[3]: [&lt;Selector xpath=&#39;//title/text()&#39; data=u&#39;Open Directory - Computers: Programming:&#39;&gt;]

In [4]: response.xpath(&#39;//title/text()&#39;).extract()
Out[4]: [u&#39;Open Directory - Computers: Programming: Languages: Python: Books&#39;]

In [5]: response.xpath(&#39;//title/text()&#39;).re(&#39;(\w+):&#39;)
Out[5]: [u&#39;Computers&#39;, u&#39;Programming&#39;, u&#39;Languages&#39;, u&#39;Python&#39;]
</pre></div>
</div>
</div>
<div class="section" id="extracting-the-data">
<h6>Extracting the data<a class="headerlink" href="#extracting-the-data" title="Permalink to this headline">¶</a></h6>
<p>Now, let&#8217;s try to extract some real information from those pages.</p>
<p>You could type <tt class="docutils literal"><span class="pre">response.body</span></tt> in the console, and inspect the source code to
figure out the XPaths you need to use. However, inspecting the raw HTML code
there could become a very tedious task. To make this an easier task, you can
use some Firefox extensions like Firebug. For more information see
<a class="reference internal" href="index.html#topics-firebug"><em>Using Firebug for scraping</em></a> and <a class="reference internal" href="index.html#topics-firefox"><em>Using Firefox for scraping</em></a>.</p>
<p>After inspecting the page source, you&#8217;ll find that the web sites information
is inside a <tt class="docutils literal"><span class="pre">&lt;ul&gt;</span></tt> element, in fact the <em>second</em> <tt class="docutils literal"><span class="pre">&lt;ul&gt;</span></tt> element.</p>
<p>So we can select each <tt class="docutils literal"><span class="pre">&lt;li&gt;</span></tt> element belonging to the sites list with this
code:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//ul/li&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>And from them, the sites descriptions:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//ul/li/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>The sites titles:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//ul/li/a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>And the sites links:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//ul/li/a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>As we&#8217;ve said before, each <tt class="docutils literal"><span class="pre">.xpath()</span></tt> call returns a list of selectors, so we can
concatenate further <tt class="docutils literal"><span class="pre">.xpath()</span></tt> calls to dig deeper into a node. We are going to use
that property here, so:</p>
<div class="highlight-python"><div class="highlight"><pre>for sel in response.xpath(&#39;//ul/li&#39;)
    title = sel.xpath(&#39;a/text()&#39;).extract()
    link = sel.xpath(&#39;a/@href&#39;).extract()
    desc = sel.xpath(&#39;text()&#39;).extract()
    print title, link, desc
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For a more detailed description of using nested selectors, see
<a class="reference internal" href="index.html#topics-selectors-nesting-selectors"><em>Nesting selectors</em></a> and
<a class="reference internal" href="index.html#topics-selectors-relative-xpaths"><em>Working with relative XPaths</em></a> in the <a class="reference internal" href="index.html#topics-selectors"><em>Selectors</em></a>
documentation</p>
</div>
<p>Let&#8217;s add this code to our spider:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">DmozSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&quot;dmoz&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;dmoz.org&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;</span><span class="p">,</span>
        <span class="s">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">sel</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//ul/li&#39;</span><span class="p">):</span>
            <span class="n">title</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">link</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">desc</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="k">print</span> <span class="n">title</span><span class="p">,</span> <span class="n">link</span><span class="p">,</span> <span class="n">desc</span>
</pre></div>
</div>
<p>Now try crawling the dmoz.org domain again and you&#8217;ll see sites being printed
in your output, run:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy crawl dmoz
</pre></div>
</div>
</div>
</div>
<div class="section" id="using-our-item">
<h5>Using our item<a class="headerlink" href="#using-our-item" title="Permalink to this headline">¶</a></h5>
<p><a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> objects are custom python dicts; you can access the
values of their fields (attributes of the class we defined earlier) using the
standard dict syntax like:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">item</span> <span class="o">=</span> <span class="n">DmozItem</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">item</span><span class="p">[</span><span class="s">&#39;title&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s">&#39;Example title&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">item</span><span class="p">[</span><span class="s">&#39;title&#39;</span><span class="p">]</span>
<span class="go">&#39;Example title&#39;</span>
</pre></div>
</div>
<p>Spiders are expected to return their scraped data inside
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> objects. So, in order to return the data we&#8217;ve
scraped so far, the final code for our Spider would be like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="kn">from</span> <span class="nn">tutorial.items</span> <span class="kn">import</span> <span class="n">DmozItem</span>

<span class="k">class</span> <span class="nc">DmozSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&quot;dmoz&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;dmoz.org&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;</span><span class="p">,</span>
        <span class="s">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">sel</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//ul/li&#39;</span><span class="p">):</span>
            <span class="n">item</span> <span class="o">=</span> <span class="n">DmozItem</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s">&#39;title&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s">&#39;link&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s">&#39;desc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="k">yield</span> <span class="n">item</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can find a fully-functional variant of this spider in the <a class="reference external" href="https://github.com/scrapy/dirbot">dirbot</a>
project available at <a class="reference external" href="https://github.com/scrapy/dirbot">https://github.com/scrapy/dirbot</a></p>
</div>
<p>Now doing a crawl on the dmoz.org domain yields <tt class="docutils literal"><span class="pre">DmozItem</span></tt> objects:</p>
<div class="highlight-python"><div class="highlight"><pre>[dmoz] DEBUG: Scraped from &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;
     {&#39;desc&#39;: [u&#39; - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author website, Gnosis Software, Inc.\n],
      &#39;link&#39;: [u&#39;http://gnosis.cx/TPiP/&#39;],
      &#39;title&#39;: [u&#39;Text Processing in Python&#39;]}
[dmoz] DEBUG: Scraped from &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;
     {&#39;desc&#39;: [u&#39; - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applications fast, Python tutorial, DOM and SAX, new Pyxie open source XML processing library. [Prentice Hall PTR]\n&#39;],
      &#39;link&#39;: [u&#39;http://www.informit.com/store/product.aspx?isbn=0130211192&#39;],
      &#39;title&#39;: [u&#39;XML Processing with Python&#39;]}
</pre></div>
</div>
</div>
</div>
<div class="section" id="storing-the-scraped-data">
<h4>Storing the scraped data<a class="headerlink" href="#storing-the-scraped-data" title="Permalink to this headline">¶</a></h4>
<p>The simplest way to store the scraped data is by using the <a class="reference internal" href="index.html#topics-feed-exports"><em>Feed exports</em></a>, with the following command:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy crawl dmoz -o items.json
</pre></div>
</div>
<p>That will generate a <tt class="docutils literal"><span class="pre">items.json</span></tt> file containing all scraped items,
serialized in <a class="reference external" href="http://en.wikipedia.org/wiki/JSON">JSON</a>.</p>
<p>In small projects (like the one in this tutorial), that should be enough.
However, if you want to perform more complex things with the scraped items, you
can write an <a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a>. As with Items, a
placeholder file for Item Pipelines has been set up for you when the project is
created, in <tt class="docutils literal"><span class="pre">tutorial/pipelines.py</span></tt>. Though you don&#8217;t need to implement any item
pipelines if you just want to store the scraped items.</p>
</div>
<div class="section" id="next-steps">
<h4>Next steps<a class="headerlink" href="#next-steps" title="Permalink to this headline">¶</a></h4>
<p>This tutorial covers only the basics of Scrapy, but there&#8217;s a lot of other
features not mentioned here. Check the <a class="reference internal" href="index.html#topics-whatelse"><em>What else?</em></a> section in
<a class="reference internal" href="index.html#intro-overview"><em>Scrapy at a glance</em></a> chapter for a quick overview of the most important ones.</p>
<p>Then, we recommend you continue by playing with an example project (see
<a class="reference internal" href="index.html#intro-examples"><em>Examples</em></a>), and then continue with the section
<a class="reference internal" href="#section-basics"><em>Basic concepts</em></a>.</p>
</div>
</div>
<span id="document-intro/examples"></span><div class="section" id="examples">
<span id="intro-examples"></span><h3>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h3>
<p>The best way to learn is with examples, and Scrapy is no exception. For this
reason, there is an example Scrapy project named <a class="reference external" href="https://github.com/scrapy/dirbot">dirbot</a>, that you can use to
play and learn more about Scrapy. It contains the dmoz spider described in the
tutorial.</p>
<p>This <a class="reference external" href="https://github.com/scrapy/dirbot">dirbot</a> project is available at: <a class="reference external" href="https://github.com/scrapy/dirbot">https://github.com/scrapy/dirbot</a></p>
<p>It contains a README file with a detailed description of the project contents.</p>
<p>If you&#8217;re familiar with git, you can checkout the code. Otherwise you can
download a tarball or zip file of the project by clicking on <a class="reference external" href="https://github.com/scrapy/dirbot/archives/master">Downloads</a>.</p>
<p>The <a class="reference external" href="http://snipplr.com/all/tags/scrapy/">scrapy tag on Snipplr</a> is used for sharing code snippets such as spiders,
middlewares, extensions, or scripts. Feel free (and encouraged!) to share any
code there.</p>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-intro/overview"><em>Scrapy at a glance</em></a></dt>
<dd>Understand what Scrapy is and how it can help you.</dd>
<dt><a class="reference internal" href="index.html#document-intro/install"><em>Installation guide</em></a></dt>
<dd>Get Scrapy installed on your computer.</dd>
<dt><a class="reference internal" href="index.html#document-intro/tutorial"><em>Scrapy Tutorial</em></a></dt>
<dd>Write your first Scrapy project.</dd>
<dt><a class="reference internal" href="index.html#document-intro/examples"><em>Examples</em></a></dt>
<dd>Learn more by playing with a pre-made Scrapy project.</dd>
</dl>
</div>
<div class="section" id="basic-concepts">
<span id="section-basics"></span><h2>Basic concepts<a class="headerlink" href="#basic-concepts" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-topics/commands"></span><div class="section" id="command-line-tool">
<span id="topics-commands"></span><h3>Command line tool<a class="headerlink" href="#command-line-tool" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p>Scrapy is controlled through the <tt class="docutils literal"><span class="pre">scrapy</span></tt> command-line tool, to be referred
here as the &#8220;Scrapy tool&#8221; to differentiate it from the sub-commands, which we
just call &#8220;commands&#8221; or &#8220;Scrapy commands&#8221;.</p>
<p>The Scrapy tool provides several commands, for multiple purposes, and each one
accepts a different set of arguments and options.</p>
<div class="section" id="default-structure-of-scrapy-projects">
<span id="topics-project-structure"></span><h4>Default structure of Scrapy projects<a class="headerlink" href="#default-structure-of-scrapy-projects" title="Permalink to this headline">¶</a></h4>
<p>Before delving into the command-line tool and its sub-commands, let&#8217;s first
understand the directory structure of a Scrapy project.</p>
<p>Even thought it can be modified, all Scrapy projects have the same file
structure by default, similar to this:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy.cfg
myproject/
    __init__.py
    items.py
    pipelines.py
    settings.py
    spiders/
        __init__.py
        spider1.py
        spider2.py
        ...
</pre></div>
</div>
<p>The directory where the <tt class="docutils literal"><span class="pre">scrapy.cfg</span></tt> file resides is known as the <em>project
root directory</em>. That file contains the name of the python module that defines
the project settings. Here is an example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="n">settings</span><span class="p">]</span>
<span class="n">default</span> <span class="o">=</span> <span class="n">myproject</span><span class="o">.</span><span class="n">settings</span>
</pre></div>
</div>
</div>
<div class="section" id="using-the-scrapy-tool">
<h4>Using the <tt class="docutils literal"><span class="pre">scrapy</span></tt> tool<a class="headerlink" href="#using-the-scrapy-tool" title="Permalink to this headline">¶</a></h4>
<p>You can start by running the Scrapy tool with no arguments and it will print
some usage help and the available commands:</p>
<div class="highlight-python"><div class="highlight"><pre>Scrapy X.Y - no active project

Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  crawl         Run a spider
  fetch         Fetch a URL using the Scrapy downloader
[...]
</pre></div>
</div>
<p>The first line will print the currently active project, if you&#8217;re inside a
Scrapy project. In this, it was run from outside a project. If run from inside
a project it would have printed something like this:</p>
<div class="highlight-python"><div class="highlight"><pre>Scrapy X.Y - project: myproject

Usage:
  scrapy &lt;command&gt; [options] [args]

[...]
</pre></div>
</div>
<div class="section" id="creating-projects">
<h5>Creating projects<a class="headerlink" href="#creating-projects" title="Permalink to this headline">¶</a></h5>
<p>The first thing you typically do with the <tt class="docutils literal"><span class="pre">scrapy</span></tt> tool is create your Scrapy
project:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy startproject myproject
</pre></div>
</div>
<p>That will create a Scrapy project under the <tt class="docutils literal"><span class="pre">myproject</span></tt> directory.</p>
<p>Next, you go inside the new project directory:</p>
<div class="highlight-python"><div class="highlight"><pre>cd myproject
</pre></div>
</div>
<p>And you&#8217;re ready to use the <tt class="docutils literal"><span class="pre">scrapy</span></tt> command to manage and control your
project from there.</p>
</div>
<div class="section" id="controlling-projects">
<h5>Controlling projects<a class="headerlink" href="#controlling-projects" title="Permalink to this headline">¶</a></h5>
<p>You use the <tt class="docutils literal"><span class="pre">scrapy</span></tt> tool from inside your projects to control and manage
them.</p>
<p>For example, to create a new spider:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy genspider mydomain mydomain.com
</pre></div>
</div>
<p>Some Scrapy commands (like <a class="reference internal" href="index.html#std:command-crawl"><tt class="xref std std-command docutils literal"><span class="pre">crawl</span></tt></a>) must be run from inside a Scrapy
project. See the <a class="reference internal" href="index.html#topics-commands-ref"><em>commands reference</em></a> below for more
information on which commands must be run from inside projects, and which not.</p>
<p>Also keep in mind that some commands may have slightly different behaviours
when running them from inside projects. For example, the fetch command will use
spider-overridden behaviours (such as the <tt class="docutils literal"><span class="pre">user_agent</span></tt> attribute to override
the user-agent) if the url being fetched is associated with some specific
spider. This is intentional, as the <tt class="docutils literal"><span class="pre">fetch</span></tt> command is meant to be used to
check how spiders are downloading pages.</p>
</div>
</div>
<div class="section" id="available-tool-commands">
<span id="topics-commands-ref"></span><h4>Available tool commands<a class="headerlink" href="#available-tool-commands" title="Permalink to this headline">¶</a></h4>
<p>This section contains a list of the available built-in commands with a
description and some usage examples. Remember you can always get more info
about each command by running:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">scrapy</span> <span class="o">&lt;</span><span class="n">command</span><span class="o">&gt;</span> <span class="o">-</span><span class="n">h</span>
</pre></div>
</div>
<p>And you can see all available commands with:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">scrapy</span> <span class="o">-</span><span class="n">h</span>
</pre></div>
</div>
<p>There are two kinds of commands, those that only work from inside a Scrapy
project (Project-specific commands) and those that also work without an active
Scrapy project (Global commands), though they may behave slightly different
when running from inside a project (as they would use the project overridden
settings).</p>
<p>Global commands:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:command-startproject"><tt class="xref std std-command docutils literal"><span class="pre">startproject</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-settings"><tt class="xref std std-command docutils literal"><span class="pre">settings</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-runspider"><tt class="xref std std-command docutils literal"><span class="pre">runspider</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-shell"><tt class="xref std std-command docutils literal"><span class="pre">shell</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-fetch"><tt class="xref std std-command docutils literal"><span class="pre">fetch</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-view"><tt class="xref std std-command docutils literal"><span class="pre">view</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-version"><tt class="xref std std-command docutils literal"><span class="pre">version</span></tt></a></li>
</ul>
<p>Project-only commands:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:command-crawl"><tt class="xref std std-command docutils literal"><span class="pre">crawl</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-check"><tt class="xref std std-command docutils literal"><span class="pre">check</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-list"><tt class="xref std std-command docutils literal"><span class="pre">list</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-edit"><tt class="xref std std-command docutils literal"><span class="pre">edit</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-parse"><tt class="xref std std-command docutils literal"><span class="pre">parse</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-genspider"><tt class="xref std std-command docutils literal"><span class="pre">genspider</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-deploy"><tt class="xref std std-command docutils literal"><span class="pre">deploy</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:command-bench"><tt class="xref std std-command docutils literal"><span class="pre">bench</span></tt></a></li>
</ul>
<div class="section" id="startproject">
<span id="std:command-startproject"></span><h5>startproject<a class="headerlink" href="#startproject" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">startproject</span> <span class="pre">&lt;project_name&gt;</span></tt></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Creates a new Scrapy project named <tt class="docutils literal"><span class="pre">project_name</span></tt>, under the <tt class="docutils literal"><span class="pre">project_name</span></tt>
directory.</p>
<p>Usage example:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy startproject myproject
</pre></div>
</div>
</div>
<div class="section" id="genspider">
<span id="std:command-genspider"></span><h5>genspider<a class="headerlink" href="#genspider" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">genspider</span> <span class="pre">[-t</span> <span class="pre">template]</span> <span class="pre">&lt;name&gt;</span> <span class="pre">&lt;domain&gt;</span></tt></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>Create a new spider in the current project.</p>
<p>This is just a convenient shortcut command for creating spiders based on
pre-defined templates, but certainly not the only way to create spiders. You
can just create the spider source code files yourself, instead of using this
command.</p>
<p>Usage example:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

$ scrapy genspider -d basic
import scrapy

class $classname(scrapy.Spider):
    name = &quot;$name&quot;
    allowed_domains = [&quot;$domain&quot;]
    start_urls = (
        &#39;http://www.$domain/&#39;,
        )

    def parse(self, response):
        pass

$ scrapy genspider -t basic example example.com
Created spider &#39;example&#39; using template &#39;basic&#39; in module:
  mybot.spiders.example
</pre></div>
</div>
</div>
<div class="section" id="crawl">
<span id="std:command-crawl"></span><h5>crawl<a class="headerlink" href="#crawl" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">crawl</span> <span class="pre">&lt;spider&gt;</span></tt></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>Start crawling using a spider.</p>
<p>Usage examples:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy crawl myspider
[ ... myspider starts crawling ... ]
</pre></div>
</div>
</div>
<div class="section" id="check">
<span id="std:command-check"></span><h5>check<a class="headerlink" href="#check" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">check</span> <span class="pre">[-l]</span> <span class="pre">&lt;spider&gt;</span></tt></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>Run contract checks.</p>
<p>Usage examples:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy check -l
first_spider
  * parse
  * parse_item
second_spider
  * parse
  * parse_item

$ scrapy check
[FAILED] first_spider:parse_item
&gt;&gt;&gt; &#39;RetailPricex&#39; field is missing

[FAILED] first_spider:parse
&gt;&gt;&gt; Returned 92 requests, expected 0..4
</pre></div>
</div>
</div>
<div class="section" id="list">
<span id="std:command-list"></span><h5>list<a class="headerlink" href="#list" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">list</span></tt></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>List all available spiders in the current project. The output is one spider per
line.</p>
<p>Usage example:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy list
spider1
spider2
</pre></div>
</div>
</div>
<div class="section" id="edit">
<span id="std:command-edit"></span><h5>edit<a class="headerlink" href="#edit" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">edit</span> <span class="pre">&lt;spider&gt;</span></tt></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>Edit the given spider using the editor defined in the <a class="reference internal" href="index.html#std:setting-EDITOR"><tt class="xref std std-setting docutils literal"><span class="pre">EDITOR</span></tt></a>
setting.</p>
<p>This command is provided only as a convenient shortcut for the most common
case, the developer is of course free to choose any tool or IDE to write and
debug his spiders.</p>
<p>Usage example:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy edit spider1
</pre></div>
</div>
</div>
<div class="section" id="fetch">
<span id="std:command-fetch"></span><h5>fetch<a class="headerlink" href="#fetch" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">fetch</span> <span class="pre">&lt;url&gt;</span></tt></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Downloads the given URL using the Scrapy downloader and writes the contents to
standard output.</p>
<p>The interesting thing about this command is that it fetches the page how the
spider would download it. For example, if the spider has an <tt class="docutils literal"><span class="pre">USER_AGENT</span></tt>
attribute which overrides the User Agent, it will use that one.</p>
<p>So this command can be used to &#8220;see&#8221; how your spider would fetch a certain page.</p>
<p>If used outside a project, no particular per-spider behaviour would be applied
and it will just use the default Scrapy downloader settings.</p>
<p>Usage examples:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy fetch --nolog http://www.example.com/some/page.html
[ ... html content here ... ]

$ scrapy fetch --nolog --headers http://www.example.com/
{&#39;Accept-Ranges&#39;: [&#39;bytes&#39;],
 &#39;Age&#39;: [&#39;1263   &#39;],
 &#39;Connection&#39;: [&#39;close     &#39;],
 &#39;Content-Length&#39;: [&#39;596&#39;],
 &#39;Content-Type&#39;: [&#39;text/html; charset=UTF-8&#39;],
 &#39;Date&#39;: [&#39;Wed, 18 Aug 2010 23:59:46 GMT&#39;],
 &#39;Etag&#39;: [&#39;&quot;573c1-254-48c9c87349680&quot;&#39;],
 &#39;Last-Modified&#39;: [&#39;Fri, 30 Jul 2010 15:30:18 GMT&#39;],
 &#39;Server&#39;: [&#39;Apache/2.2.3 (CentOS)&#39;]}
</pre></div>
</div>
</div>
<div class="section" id="view">
<span id="std:command-view"></span><h5>view<a class="headerlink" href="#view" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">view</span> <span class="pre">&lt;url&gt;</span></tt></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Opens the given URL in a browser, as your Scrapy spider would &#8220;see&#8221; it.
Sometimes spiders see pages differently from regular users, so this can be used
to check what the spider &#8220;sees&#8221; and confirm it&#8217;s what you expect.</p>
<p>Usage example:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy view http://www.example.com/some/page.html
[ ... browser starts ... ]
</pre></div>
</div>
</div>
<div class="section" id="shell">
<span id="std:command-shell"></span><h5>shell<a class="headerlink" href="#shell" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">[url]</span></tt></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Starts the Scrapy shell for the given URL (if given) or empty if no URL is
given. See <a class="reference internal" href="index.html#topics-shell"><em>Scrapy shell</em></a> for more info.</p>
<p>Usage example:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy shell http://www.example.com/some/page.html
[ ... scrapy shell starts ... ]
</pre></div>
</div>
</div>
<div class="section" id="parse">
<span id="std:command-parse"></span><h5>parse<a class="headerlink" href="#parse" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">parse</span> <span class="pre">&lt;url&gt;</span> <span class="pre">[options]</span></tt></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>Fetches the given URL and parses it with the spider that handles it, using the
method passed with the <tt class="docutils literal"><span class="pre">--callback</span></tt> option, or <tt class="docutils literal"><span class="pre">parse</span></tt> if not given.</p>
<p>Supported options:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">--spider=SPIDER</span></tt>: bypass spider autodetection and force use of specific spider</li>
<li><tt class="docutils literal"><span class="pre">--a</span> <span class="pre">NAME=VALUE</span></tt>: set spider argument (may be repeated)</li>
<li><tt class="docutils literal"><span class="pre">--callback</span></tt> or <tt class="docutils literal"><span class="pre">-c</span></tt>: spider method to use as callback for parsing the
response</li>
<li><tt class="docutils literal"><span class="pre">--pipelines</span></tt>: process items through pipelines</li>
<li><tt class="docutils literal"><span class="pre">--rules</span></tt> or <tt class="docutils literal"><span class="pre">-r</span></tt>: use <a class="reference internal" href="index.html#scrapy.contrib.spiders.CrawlSpider" title="scrapy.contrib.spiders.CrawlSpider"><tt class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></tt></a>
rules to discover the callback (i.e. spider method) to use for parsing the
response</li>
<li><tt class="docutils literal"><span class="pre">--noitems</span></tt>: don&#8217;t show scraped items</li>
<li><tt class="docutils literal"><span class="pre">--nolinks</span></tt>: don&#8217;t show extracted links</li>
<li><tt class="docutils literal"><span class="pre">--nocolour</span></tt>: avoid using pygments to colorize the output</li>
<li><tt class="docutils literal"><span class="pre">--depth</span></tt> or <tt class="docutils literal"><span class="pre">-d</span></tt>: depth level for which the requests should be followed
recursively (default: 1)</li>
<li><tt class="docutils literal"><span class="pre">--verbose</span></tt> or <tt class="docutils literal"><span class="pre">-v</span></tt>: display information for each depth level</li>
</ul>
<p>Usage example:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy parse http://www.example.com/ -c parse_item
[ ... scrapy log lines crawling example.com spider ... ]

&gt;&gt;&gt; STATUS DEPTH LEVEL 1 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{&#39;name&#39;: u&#39;Example item&#39;,
 &#39;category&#39;: u&#39;Furniture&#39;,
 &#39;length&#39;: u&#39;12 cm&#39;}]

# Requests  -----------------------------------------------------------------
[]
</pre></div>
</div>
</div>
<div class="section" id="settings">
<span id="std:command-settings"></span><h5>settings<a class="headerlink" href="#settings" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">settings</span> <span class="pre">[options]</span></tt></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Get the value of a Scrapy setting.</p>
<p>If used inside a project it&#8217;ll show the project setting value, otherwise it&#8217;ll
show the default Scrapy value for that setting.</p>
<p>Example usage:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy settings --get BOT_NAME
scrapybot
$ scrapy settings --get DOWNLOAD_DELAY
0
</pre></div>
</div>
</div>
<div class="section" id="runspider">
<span id="std:command-runspider"></span><h5>runspider<a class="headerlink" href="#runspider" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">runspider</span> <span class="pre">&lt;spider_file.py&gt;</span></tt></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Run a spider self-contained in a Python file, without having to create a
project.</p>
<p>Example usage:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy runspider myspider.py
[ ... spider starts crawling ... ]
</pre></div>
</div>
</div>
<div class="section" id="version">
<span id="std:command-version"></span><h5>version<a class="headerlink" href="#version" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">version</span> <span class="pre">[-v]</span></tt></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Prints the Scrapy version. If used with <tt class="docutils literal"><span class="pre">-v</span></tt> it also prints Python, Twisted
and Platform info, which is useful for bug reports.</p>
</div>
<div class="section" id="deploy">
<span id="std:command-deploy"></span><h5>deploy<a class="headerlink" href="#deploy" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.11.</span></p>
</div>
<ul class="simple">
<li>Syntax: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">deploy</span> <span class="pre">[</span> <span class="pre">&lt;target:project&gt;</span> <span class="pre">|</span> <span class="pre">-l</span> <span class="pre">&lt;target&gt;</span> <span class="pre">|</span> <span class="pre">-L</span> <span class="pre">]</span></tt></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>Deploy the project into a Scrapyd server. See <a class="reference external" href="http://scrapyd.readthedocs.org/en/latest/deploy.html">Deploying your project</a>.</p>
</div>
<div class="section" id="bench">
<span id="std:command-bench"></span><h5>bench<a class="headerlink" href="#bench" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<ul class="simple">
<li>Syntax: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">bench</span></tt></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Run a quick benchmark test. <a class="reference internal" href="index.html#benchmarking"><em>Benchmarking</em></a>.</p>
</div>
</div>
<div class="section" id="custom-project-commands">
<h4>Custom project commands<a class="headerlink" href="#custom-project-commands" title="Permalink to this headline">¶</a></h4>
<p>You can also add your custom project commands by using the
<a class="reference internal" href="index.html#std:setting-COMMANDS_MODULE"><tt class="xref std std-setting docutils literal"><span class="pre">COMMANDS_MODULE</span></tt></a> setting. See the Scrapy commands in
<a class="reference external" href="https://github.com/scrapy/scrapy/blob/master/scrapy/commands">scrapy/commands</a> for examples on how to implement your commands.</p>
<div class="section" id="commands-module">
<span id="std:setting-COMMANDS_MODULE"></span><h5>COMMANDS_MODULE<a class="headerlink" href="#commands-module" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">''</span></tt> (empty string)</p>
<p>A module to use for looking up custom Scrapy commands. This is used to add custom
commands for your Scrapy project.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">COMMANDS_MODULE</span> <span class="o">=</span> <span class="s">&#39;mybot.commands&#39;</span>
</pre></div>
</div>
</div>
</div>
</div>
<span id="document-topics/items"></span><div class="section" id="module-scrapy.item">
<span id="items"></span><span id="topics-items"></span><h3>Items<a class="headerlink" href="#module-scrapy.item" title="Permalink to this headline">¶</a></h3>
<p>The main goal in scraping is to extract structured data from unstructured
sources, typically, web pages. Scrapy provides the <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> class for this
purpose.</p>
<p><a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> objects are simple containers used to collect the scraped data.
They provide a <a class="reference external" href="http://docs.python.org/library/stdtypes.html#dict">dictionary-like</a> API with a convenient syntax for declaring
their available fields.</p>
<div class="section" id="declaring-items">
<span id="topics-items-declaring"></span><h4>Declaring Items<a class="headerlink" href="#declaring-items" title="Permalink to this headline">¶</a></h4>
<p>Items are declared using a simple class definition syntax and <a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a>
objects. Here is an example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">Product</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">stock</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">last_updated</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">serializer</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Those familiar with <a class="reference external" href="http://www.djangoproject.com/">Django</a> will notice that Scrapy Items are
declared similar to <a class="reference external" href="http://docs.djangoproject.com/en/dev/topics/db/models/">Django Models</a>, except that Scrapy Items are much
simpler as there is no concept of different field types.</p>
</div>
</div>
<div class="section" id="item-fields">
<span id="topics-items-fields"></span><h4>Item Fields<a class="headerlink" href="#item-fields" title="Permalink to this headline">¶</a></h4>
<p><a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> objects are used to specify metadata for each field. For
example, the serializer function for the <tt class="docutils literal"><span class="pre">last_updated</span></tt> field illustrated in
the example above.</p>
<p>You can specify any kind of metadata for each field. There is no restriction on
the values accepted by <a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> objects. For this same
reason, there is no reference list of all available metadata keys. Each key
defined in <a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> objects could be used by a different components, and
only those components know about it. You can also define and use any other
<a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> key in your project too, for your own needs. The main goal of
<a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> objects is to provide a way to define all field metadata in one
place. Typically, those components whose behaviour depends on each field use
certain field keys to configure that behaviour. You must refer to their
documentation to see which metadata keys are used by each component.</p>
<p>It&#8217;s important to note that the <a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> objects used to declare the item
do not stay assigned as class attributes. Instead, they can be accessed through
the <a class="reference internal" href="index.html#scrapy.item.Item.fields" title="scrapy.item.Item.fields"><tt class="xref py py-attr docutils literal"><span class="pre">Item.fields</span></tt></a> attribute.</p>
<p>And that&#8217;s all you need to know about declaring items.</p>
</div>
<div class="section" id="working-with-items">
<h4>Working with Items<a class="headerlink" href="#working-with-items" title="Permalink to this headline">¶</a></h4>
<p>Here are some examples of common tasks performed with items, using the
<tt class="docutils literal"><span class="pre">Product</span></tt> item <a class="reference internal" href="index.html#topics-items-declaring"><em>declared above</em></a>. You will
notice the API is very similar to the <a class="reference external" href="http://docs.python.org/library/stdtypes.html#dict">dict API</a>.</p>
<div class="section" id="creating-items">
<h5>Creating items<a class="headerlink" href="#creating-items" title="Permalink to this headline">¶</a></h5>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span> <span class="o">=</span> <span class="n">Product</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;Desktop PC&#39;</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">product</span>
<span class="go">Product(name=&#39;Desktop PC&#39;, price=1000)</span>
</pre></div>
</div>
</div>
<div class="section" id="getting-field-values">
<h5>Getting field values<a class="headerlink" href="#getting-field-values" title="Permalink to this headline">¶</a></h5>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">]</span>
<span class="go">Desktop PC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">)</span>
<span class="go">Desktop PC</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s">&#39;price&#39;</span><span class="p">]</span>
<span class="go">1000</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s">&#39;last_updated&#39;</span><span class="p">]</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;last_updated&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;last_updated&#39;</span><span class="p">,</span> <span class="s">&#39;not set&#39;</span><span class="p">)</span>
<span class="go">not set</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s">&#39;lala&#39;</span><span class="p">]</span> <span class="c"># getting unknown field</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;lala&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;lala&#39;</span><span class="p">,</span> <span class="s">&#39;unknown field&#39;</span><span class="p">)</span>
<span class="go">&#39;unknown field&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s">&#39;name&#39;</span> <span class="ow">in</span> <span class="n">product</span>  <span class="c"># is name field populated?</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s">&#39;last_updated&#39;</span> <span class="ow">in</span> <span class="n">product</span>  <span class="c"># is last_updated populated?</span>
<span class="go">False</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s">&#39;last_updated&#39;</span> <span class="ow">in</span> <span class="n">product</span><span class="o">.</span><span class="n">fields</span>  <span class="c"># is last_updated a declared field?</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s">&#39;lala&#39;</span> <span class="ow">in</span> <span class="n">product</span><span class="o">.</span><span class="n">fields</span>  <span class="c"># is lala a declared field?</span>
<span class="go">False</span>
</pre></div>
</div>
</div>
<div class="section" id="setting-field-values">
<h5>Setting field values<a class="headerlink" href="#setting-field-values" title="Permalink to this headline">¶</a></h5>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s">&#39;last_updated&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s">&#39;today&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s">&#39;last_updated&#39;</span><span class="p">]</span>
<span class="go">today</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s">&#39;lala&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s">&#39;test&#39;</span> <span class="c"># setting unknown field</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;Product does not support field: lala&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="accessing-all-populated-values">
<h5>Accessing all populated values<a class="headerlink" href="#accessing-all-populated-values" title="Permalink to this headline">¶</a></h5>
<p>To access all populated values, just use the typical <a class="reference external" href="http://docs.python.org/library/stdtypes.html#dict">dict API</a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;price&#39;, &#39;name&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
<span class="go">[(&#39;price&#39;, 1000), (&#39;name&#39;, &#39;Desktop PC&#39;)]</span>
</pre></div>
</div>
</div>
<div class="section" id="other-common-tasks">
<h5>Other common tasks<a class="headerlink" href="#other-common-tasks" title="Permalink to this headline">¶</a></h5>
<p>Copying items:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">product2</span> <span class="o">=</span> <span class="n">Product</span><span class="p">(</span><span class="n">product</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">product2</span>
<span class="go">Product(name=&#39;Desktop PC&#39;, price=1000)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product3</span> <span class="o">=</span> <span class="n">product2</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">product3</span>
<span class="go">Product(name=&#39;Desktop PC&#39;, price=1000)</span>
</pre></div>
</div>
<p>Creating dicts from items:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="nb">dict</span><span class="p">(</span><span class="n">product</span><span class="p">)</span> <span class="c"># create a dict from all populated values</span>
<span class="go">{&#39;price&#39;: 1000, &#39;name&#39;: &#39;Desktop PC&#39;}</span>
</pre></div>
</div>
<p>Creating items from dicts:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Product</span><span class="p">({</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;Laptop PC&#39;</span><span class="p">,</span> <span class="s">&#39;price&#39;</span><span class="p">:</span> <span class="mi">1500</span><span class="p">})</span>
<span class="go">Product(price=1500, name=&#39;Laptop PC&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">Product</span><span class="p">({</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;Laptop PC&#39;</span><span class="p">,</span> <span class="s">&#39;lala&#39;</span><span class="p">:</span> <span class="mi">1500</span><span class="p">})</span> <span class="c"># warning: unknown field in dict</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;Product does not support field: lala&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="extending-items">
<h4>Extending Items<a class="headerlink" href="#extending-items" title="Permalink to this headline">¶</a></h4>
<p>You can extend Items (to add more fields or to change some metadata for some
fields) by declaring a subclass of your original Item.</p>
<p>For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">DiscountedProduct</span><span class="p">(</span><span class="n">Product</span><span class="p">):</span>
    <span class="n">discount_percent</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">serializer</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
    <span class="n">discount_expiration_date</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<p>You can also extend field metadata by using the previous field metadata and
appending more values, or changing existing values, like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">SpecificProduct</span><span class="p">(</span><span class="n">Product</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">Product</span><span class="o">.</span><span class="n">fields</span><span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">],</span> <span class="n">serializer</span><span class="o">=</span><span class="n">my_serializer</span><span class="p">)</span>
</pre></div>
</div>
<p>That adds (or replaces) the <tt class="docutils literal"><span class="pre">serializer</span></tt> metadata key for the <tt class="docutils literal"><span class="pre">name</span></tt> field,
keeping all the previously existing metadata values.</p>
</div>
<div class="section" id="item-objects">
<h4>Item objects<a class="headerlink" href="#item-objects" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.item.Item">
<em class="property">class </em><tt class="descclassname">scrapy.item.</tt><tt class="descname">Item</tt><big>(</big><span class="optional">[</span><em>arg</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.item.Item" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new Item optionally initialized from the given argument.</p>
<p>Items replicate the standard <a class="reference external" href="http://docs.python.org/library/stdtypes.html#dict">dict API</a>, including its constructor. The
only additional attribute provided by Items is:</p>
<dl class="attribute">
<dt id="scrapy.item.Item.fields">
<tt class="descname">fields</tt><a class="headerlink" href="#scrapy.item.Item.fields" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary containing <em>all declared fields</em> for this Item, not only
those populated. The keys are the field names and the values are the
<a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> objects used in the <a class="reference internal" href="index.html#topics-items-declaring"><em>Item declaration</em></a>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="field-objects">
<h4>Field objects<a class="headerlink" href="#field-objects" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.item.Field">
<em class="property">class </em><tt class="descclassname">scrapy.item.</tt><tt class="descname">Field</tt><big>(</big><span class="optional">[</span><em>arg</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.item.Field" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> class is just an alias to the built-in <a class="reference external" href="http://docs.python.org/library/stdtypes.html#dict">dict</a> class and
doesn&#8217;t provide any extra functionality or attributes. In other words,
<a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> objects are plain-old Python dicts. A separate class is used
to support the <a class="reference internal" href="index.html#topics-items-declaring"><em>item declaration syntax</em></a>
based on class attributes.</p>
</dd></dl>

</div>
</div>
<span id="document-topics/spiders"></span><div class="section" id="spiders">
<span id="topics-spiders"></span><h3>Spiders<a class="headerlink" href="#spiders" title="Permalink to this headline">¶</a></h3>
<p>Spiders are classes which define how a certain site (or a group of sites) will be
scraped, including how to perform the crawl (i.e. follow links) and how to
extract structured data from their pages (i.e. scraping items). In other words,
Spiders are the place where you define the custom behaviour for crawling and
parsing pages for a particular site (or, in some cases, a group of sites).</p>
<p>For spiders, the scraping cycle goes through something like this:</p>
<ol class="arabic">
<li><p class="first">You start by generating the initial Requests to crawl the first URLs, and
specify a callback function to be called with the response downloaded from
those requests.</p>
<p>The first requests to perform are obtained by calling the
<a class="reference internal" href="index.html#scrapy.spider.Spider.start_requests" title="scrapy.spider.Spider.start_requests"><tt class="xref py py-meth docutils literal"><span class="pre">start_requests()</span></tt></a> method which (by default)
generates <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> for the URLs specified in the
<a class="reference internal" href="index.html#scrapy.spider.Spider.start_urls" title="scrapy.spider.Spider.start_urls"><tt class="xref py py-attr docutils literal"><span class="pre">start_urls</span></tt></a> and the
<a class="reference internal" href="index.html#scrapy.spider.Spider.parse" title="scrapy.spider.Spider.parse"><tt class="xref py py-attr docutils literal"><span class="pre">parse</span></tt></a> method as callback function for the
Requests.</p>
</li>
<li><p class="first">In the callback function, you parse the response (web page) and return either
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> objects, <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> objects,
or an iterable of both. Those Requests will also contain a callback (maybe
the same) and will then be downloaded by Scrapy and then their
response handled by the specified callback.</p>
</li>
<li><p class="first">In callback functions, you parse the page contents, typically using
<a class="reference internal" href="index.html#topics-selectors"><em>Selectors</em></a> (but you can also use BeautifulSoup, lxml or whatever
mechanism you prefer) and generate items with the parsed data.</p>
</li>
<li><p class="first">Finally, the items returned from the spider will be typically persisted to a
database (in some <a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a>) or written to
a file using <a class="reference internal" href="index.html#topics-feed-exports"><em>Feed exports</em></a>.</p>
</li>
</ol>
<p>Even though this cycle applies (more or less) to any kind of spider, there are
different kinds of default spiders bundled into Scrapy for different purposes.
We will talk about those types here.</p>
<div class="section" id="spider-arguments">
<span id="spiderargs"></span><h4>Spider arguments<a class="headerlink" href="#spider-arguments" title="Permalink to this headline">¶</a></h4>
<p>Spiders can receive arguments that modify their behaviour. Some common uses for
spider arguments are to define the start URLs or to restrict the crawl to
certain sections of the site, but they can be used to configure any
functionality of the spider.</p>
<p>Spider arguments are passed through the <a class="reference internal" href="index.html#std:command-crawl"><tt class="xref std std-command docutils literal"><span class="pre">crawl</span></tt></a> command using the
<tt class="docutils literal"><span class="pre">-a</span></tt> option. For example:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy crawl myspider -a category=electronics
</pre></div>
</div>
<p>Spiders receive arguments in their constructors:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;myspider&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MySpider</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/categories/</span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">category</span><span class="p">]</span>
        <span class="c"># ...</span>
</pre></div>
</div>
<p>Spider arguments can also be passed through the Scrapyd <tt class="docutils literal"><span class="pre">schedule.json</span></tt> API.
See <a class="reference external" href="http://scrapyd.readthedocs.org/">Scrapyd documentation</a>.</p>
</div>
<div class="section" id="built-in-spiders-reference">
<span id="topics-spiders-ref"></span><h4>Built-in spiders reference<a class="headerlink" href="#built-in-spiders-reference" title="Permalink to this headline">¶</a></h4>
<p>Scrapy comes with some useful generic spiders that you can use, to subclass
your spiders from. Their aim is to provide convenient functionality for a few
common scraping cases, like following all links on a site based on certain
rules, crawling from <a class="reference external" href="http://www.sitemaps.org">Sitemaps</a>, or parsing a XML/CSV feed.</p>
<p>For the examples used in the following spiders, we&#8217;ll assume you have a project
with a <tt class="docutils literal"><span class="pre">TestItem</span></tt> declared in a <tt class="docutils literal"><span class="pre">myproject.items</span></tt> module:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">TestItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="nb">id</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">description</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<span class="target" id="module-scrapy.spider"></span><div class="section" id="spider">
<h5>Spider<a class="headerlink" href="#spider" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spider.Spider">
<em class="property">class </em><tt class="descclassname">scrapy.spider.</tt><tt class="descname">Spider</tt><a class="headerlink" href="#scrapy.spider.Spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the simplest spider, and the one from which every other spider
must inherit from (either the ones that come bundled with Scrapy, or the ones
that you write yourself). It doesn&#8217;t provide any special functionality. It just
requests the given <tt class="docutils literal"><span class="pre">start_urls</span></tt>/<tt class="docutils literal"><span class="pre">start_requests</span></tt>, and calls the spider&#8217;s
method <tt class="docutils literal"><span class="pre">parse</span></tt> for each of the resulting responses.</p>
<dl class="attribute">
<dt id="scrapy.spider.Spider.name">
<tt class="descname">name</tt><a class="headerlink" href="#scrapy.spider.Spider.name" title="Permalink to this definition">¶</a></dt>
<dd><p>A string which defines the name for this spider. The spider name is how
the spider is located (and instantiated) by Scrapy, so it must be
unique. However, nothing prevents you from instantiating more than one
instance of the same spider. This is the most important spider attribute
and it&#8217;s required.</p>
<p>If the spider scrapes a single domain, a common practice is to name the
spider after the domain, with or without the <a class="reference external" href="http://en.wikipedia.org/wiki/Top-level_domain">TLD</a>. So, for example, a
spider that crawls <tt class="docutils literal"><span class="pre">mywebsite.com</span></tt> would often be called
<tt class="docutils literal"><span class="pre">mywebsite</span></tt>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spider.Spider.allowed_domains">
<tt class="descname">allowed_domains</tt><a class="headerlink" href="#scrapy.spider.Spider.allowed_domains" title="Permalink to this definition">¶</a></dt>
<dd><p>An optional list of strings containing domains that this spider is
allowed to crawl. Requests for URLs not belonging to the domain names
specified in this list won&#8217;t be followed if
<a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware" title="scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">OffsiteMiddleware</span></tt></a> is enabled.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spider.Spider.start_urls">
<tt class="descname">start_urls</tt><a class="headerlink" href="#scrapy.spider.Spider.start_urls" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of URLs where the spider will begin to crawl from, when no
particular URLs are specified. So, the first pages downloaded will be those
listed here. The subsequent URLs will be generated successively from data
contained in the start URLs.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spider.Spider.start_requests">
<tt class="descname">start_requests</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.spider.Spider.start_requests" title="Permalink to this definition">¶</a></dt>
<dd><p>This method must return an iterable with the first Requests to crawl for
this spider.</p>
<p>This is the method called by Scrapy when the spider is opened for
scraping when no particular URLs are specified. If particular URLs are
specified, the <a class="reference internal" href="index.html#scrapy.spider.Spider.make_requests_from_url" title="scrapy.spider.Spider.make_requests_from_url"><tt class="xref py py-meth docutils literal"><span class="pre">make_requests_from_url()</span></tt></a> is used instead to create
the Requests. This method is also called only once from Scrapy, so it&#8217;s
safe to implement it as a generator.</p>
<p>The default implementation uses <a class="reference internal" href="index.html#scrapy.spider.Spider.make_requests_from_url" title="scrapy.spider.Spider.make_requests_from_url"><tt class="xref py py-meth docutils literal"><span class="pre">make_requests_from_url()</span></tt></a> to
generate Requests for each url in <a class="reference internal" href="index.html#scrapy.spider.Spider.start_urls" title="scrapy.spider.Spider.start_urls"><tt class="xref py py-attr docutils literal"><span class="pre">start_urls</span></tt></a>.</p>
<p>If you want to change the Requests used to start scraping a domain, this is
the method to override. For example, if you need to start by logging in using
a POST request, you could do:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">FormRequest</span><span class="p">(</span><span class="s">&quot;http://www.example.com/login&quot;</span><span class="p">,</span>
                               <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;user&#39;</span><span class="p">:</span> <span class="s">&#39;john&#39;</span><span class="p">,</span> <span class="s">&#39;pass&#39;</span><span class="p">:</span> <span class="s">&#39;secret&#39;</span><span class="p">},</span>
                               <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logged_in</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">logged_in</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="c"># here you would extract links to follow and return Requests for</span>
    <span class="c"># each of them, with another callback</span>
    <span class="k">pass</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.spider.Spider.make_requests_from_url">
<tt class="descname">make_requests_from_url</tt><big>(</big><em>url</em><big>)</big><a class="headerlink" href="#scrapy.spider.Spider.make_requests_from_url" title="Permalink to this definition">¶</a></dt>
<dd><p>A method that receives a URL and returns a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a>
object (or a list of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> objects) to scrape. This
method is used to construct the initial requests in the
<a class="reference internal" href="index.html#scrapy.spider.Spider.start_requests" title="scrapy.spider.Spider.start_requests"><tt class="xref py py-meth docutils literal"><span class="pre">start_requests()</span></tt></a> method, and is typically used to convert urls to
requests.</p>
<p>Unless overridden, this method returns Requests with the <a class="reference internal" href="index.html#scrapy.spider.Spider.parse" title="scrapy.spider.Spider.parse"><tt class="xref py py-meth docutils literal"><span class="pre">parse()</span></tt></a>
method as their callback function, and with dont_filter parameter enabled
(see <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> class for more info).</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spider.Spider.parse">
<tt class="descname">parse</tt><big>(</big><em>response</em><big>)</big><a class="headerlink" href="#scrapy.spider.Spider.parse" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the default callback used by Scrapy to process downloaded
responses, when their requests don&#8217;t specify a callback.</p>
<p>The <tt class="docutils literal"><span class="pre">parse</span></tt> method is in charge of processing the response and returning
scraped data and/or more URLs to follow. Other Requests callbacks have
the same requirements as the <a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> class.</p>
<p>This method, as well as any other Request callback, must return an
iterable of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> and/or
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> objects.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>response</strong> (<em>:class:~scrapy.http.Response`</em>) &#8211; the response to parse</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.spider.Spider.log">
<tt class="descname">log</tt><big>(</big><em>message</em><span class="optional">[</span>, <em>level</em>, <em>component</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.spider.Spider.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a message using the <a class="reference internal" href="index.html#scrapy.log.msg" title="scrapy.log.msg"><tt class="xref py py-func docutils literal"><span class="pre">scrapy.log.msg()</span></tt></a> function, automatically
populating the spider argument with the <a class="reference internal" href="index.html#scrapy.spider.Spider.name" title="scrapy.spider.Spider.name"><tt class="xref py py-attr docutils literal"><span class="pre">name</span></tt></a> of this
spider. For more information see <a class="reference internal" href="index.html#topics-logging"><em>Logging</em></a>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spider.Spider.closed">
<tt class="descname">closed</tt><big>(</big><em>reason</em><big>)</big><a class="headerlink" href="#scrapy.spider.Spider.closed" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when the spider closes. This method provides a shortcut to
signals.connect() for the <a class="reference internal" href="index.html#std:signal-spider_closed"><tt class="xref std std-signal docutils literal"><span class="pre">spider_closed</span></tt></a> signal.</p>
</dd></dl>

</dd></dl>

<div class="section" id="spider-example">
<h6>Spider example<a class="headerlink" href="#spider-example" title="Permalink to this headline">¶</a></h6>
<p>Let&#8217;s see an example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">&#39;http://www.example.com/1.html&#39;</span><span class="p">,</span>
        <span class="s">&#39;http://www.example.com/2.html&#39;</span><span class="p">,</span>
        <span class="s">&#39;http://www.example.com/3.html&#39;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s">&#39;A response from </span><span class="si">%s</span><span class="s"> just arrived!&#39;</span> <span class="o">%</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p>Another example returning multiple Requests and Items from a single callback:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">MyItem</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">&#39;http://www.example.com/1.html&#39;</span><span class="p">,</span>
        <span class="s">&#39;http://www.example.com/2.html&#39;</span><span class="p">,</span>
        <span class="s">&#39;http://www.example.com/3.html&#39;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">h3</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//h3&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">MyItem</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">h3</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<span class="target" id="module-scrapy.contrib.spiders"></span></div>
</div>
<div class="section" id="crawlspider">
<h5>CrawlSpider<a class="headerlink" href="#crawlspider" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spiders.CrawlSpider">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spiders.</tt><tt class="descname">CrawlSpider</tt><a class="headerlink" href="#scrapy.contrib.spiders.CrawlSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the most commonly used spider for crawling regular websites, as it
provides a convenient mechanism for following links by defining a set of rules.
It may not be the best suited for your particular web sites or project, but
it&#8217;s generic enough for several cases, so you can start from it and override it
as needed for more custom functionality, or just implement your own spider.</p>
<p>Apart from the attributes inherited from Spider (that you must
specify), this class supports a new attribute:</p>
<dl class="attribute">
<dt id="scrapy.contrib.spiders.CrawlSpider.rules">
<tt class="descname">rules</tt><a class="headerlink" href="#scrapy.contrib.spiders.CrawlSpider.rules" title="Permalink to this definition">¶</a></dt>
<dd><p>Which is a list of one (or more) <a class="reference internal" href="index.html#scrapy.contrib.spiders.Rule" title="scrapy.contrib.spiders.Rule"><tt class="xref py py-class docutils literal"><span class="pre">Rule</span></tt></a> objects.  Each <a class="reference internal" href="index.html#scrapy.contrib.spiders.Rule" title="scrapy.contrib.spiders.Rule"><tt class="xref py py-class docutils literal"><span class="pre">Rule</span></tt></a>
defines a certain behaviour for crawling the site. Rules objects are
described below. If multiple rules match the same link, the first one
will be used, according to the order they&#8217;re defined in this attribute.</p>
</dd></dl>

<p>This spider also exposes an overrideable method:</p>
<dl class="method">
<dt id="scrapy.contrib.spiders.CrawlSpider.parse_start_url">
<tt class="descname">parse_start_url</tt><big>(</big><em>response</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spiders.CrawlSpider.parse_start_url" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for the start_urls responses. It allows to parse
the initial responses and must return either a
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> object, a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a>
object, or an iterable containing any of them.</p>
</dd></dl>

</dd></dl>

<div class="section" id="crawling-rules">
<h6>Crawling rules<a class="headerlink" href="#crawling-rules" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.spiders.Rule">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spiders.</tt><tt class="descname">Rule</tt><big>(</big><em>link_extractor</em>, <em>callback=None</em>, <em>cb_kwargs=None</em>, <em>follow=None</em>, <em>process_links=None</em>, <em>process_request=None</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spiders.Rule" title="Permalink to this definition">¶</a></dt>
<dd><p><tt class="docutils literal"><span class="pre">link_extractor</span></tt> is a <a class="reference internal" href="index.html#topics-link-extractors"><em>Link Extractor</em></a> object which
defines how links will be extracted from each crawled page.</p>
<p><tt class="docutils literal"><span class="pre">callback</span></tt> is a callable or a string (in which case a method from the spider
object with that name will be used) to be called for each link extracted with
the specified link_extractor. This callback receives a response as its first
argument and must return a list containing <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> and/or
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> objects (or any subclass of them).</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">When writing crawl spider rules, avoid using <tt class="docutils literal"><span class="pre">parse</span></tt> as
callback, since the <a class="reference internal" href="index.html#scrapy.contrib.spiders.CrawlSpider" title="scrapy.contrib.spiders.CrawlSpider"><tt class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></tt></a> uses the <tt class="docutils literal"><span class="pre">parse</span></tt> method
itself to implement its logic. So if you override the <tt class="docutils literal"><span class="pre">parse</span></tt> method,
the crawl spider will no longer work.</p>
</div>
<p><tt class="docutils literal"><span class="pre">cb_kwargs</span></tt> is a dict containing the keyword arguments to be passed to the
callback function.</p>
<p><tt class="docutils literal"><span class="pre">follow</span></tt> is a boolean which specifies if links should be followed from each
response extracted with this rule. If <tt class="docutils literal"><span class="pre">callback</span></tt> is None <tt class="docutils literal"><span class="pre">follow</span></tt> defaults
to <tt class="docutils literal"><span class="pre">True</span></tt>, otherwise it default to <tt class="docutils literal"><span class="pre">False</span></tt>.</p>
<p><tt class="docutils literal"><span class="pre">process_links</span></tt> is a callable, or a string (in which case a method from the
spider object with that name will be used) which will be called for each list
of links extracted from each response using the specified <tt class="docutils literal"><span class="pre">link_extractor</span></tt>.
This is mainly used for filtering purposes.</p>
<p><tt class="docutils literal"><span class="pre">process_request</span></tt> is a callable, or a string (in which case a method from
the spider object with that name will be used) which will be called with
every request extracted by this rule, and must return a request or None (to
filter out the request).</p>
</dd></dl>

</div>
<div class="section" id="crawlspider-example">
<h6>CrawlSpider example<a class="headerlink" href="#crawlspider-example" title="Permalink to this headline">¶</a></h6>
<p>Let&#8217;s now take a look at an example CrawlSpider with rules:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span><span class="p">,</span> <span class="n">Rule</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.linkextractors</span> <span class="kn">import</span> <span class="n">LinkExtractor</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com&#39;</span><span class="p">]</span>

    <span class="n">rules</span> <span class="o">=</span> <span class="p">(</span>
        <span class="c"># Extract links matching &#39;category.php&#39; (but not matching &#39;subsection.php&#39;)</span>
        <span class="c"># and follow links from them (since no callback means follow=True by default).</span>
        <span class="n">Rule</span><span class="p">(</span><span class="n">LinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="p">(</span><span class="s">&#39;category\.php&#39;</span><span class="p">,</span> <span class="p">),</span> <span class="n">deny</span><span class="o">=</span><span class="p">(</span><span class="s">&#39;subsection\.php&#39;</span><span class="p">,</span> <span class="p">))),</span>

        <span class="c"># Extract links matching &#39;item.php&#39; and parse them with the spider&#39;s method parse_item</span>
        <span class="n">Rule</span><span class="p">(</span><span class="n">LinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="p">(</span><span class="s">&#39;item\.php&#39;</span><span class="p">,</span> <span class="p">)),</span> <span class="n">callback</span><span class="o">=</span><span class="s">&#39;parse_item&#39;</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s">&#39;Hi, this is an item page! </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//td[@id=&quot;item_id&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="s">r&#39;ID: (\d+)&#39;</span><span class="p">)</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//td[@id=&quot;item_name&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;description&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//td[@id=&quot;item_description&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>This spider would start crawling example.com&#8217;s home page, collecting category
links, and item links, parsing the latter with the <tt class="docutils literal"><span class="pre">parse_item</span></tt> method. For
each item response, some data will be extracted from the HTML using XPath, and
a <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> will be filled with it.</p>
</div>
</div>
<div class="section" id="xmlfeedspider">
<h5>XMLFeedSpider<a class="headerlink" href="#xmlfeedspider" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spiders.XMLFeedSpider">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spiders.</tt><tt class="descname">XMLFeedSpider</tt><a class="headerlink" href="#scrapy.contrib.spiders.XMLFeedSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>XMLFeedSpider is designed for parsing XML feeds by iterating through them by a
certain node name.  The iterator can be chosen from: <tt class="docutils literal"><span class="pre">iternodes</span></tt>, <tt class="docutils literal"><span class="pre">xml</span></tt>,
and <tt class="docutils literal"><span class="pre">html</span></tt>.  It&#8217;s recommended to use the <tt class="docutils literal"><span class="pre">iternodes</span></tt> iterator for
performance reasons, since the <tt class="docutils literal"><span class="pre">xml</span></tt> and <tt class="docutils literal"><span class="pre">html</span></tt> iterators generate the
whole DOM at once in order to parse it.  However, using <tt class="docutils literal"><span class="pre">html</span></tt> as the
iterator may be useful when parsing XML with bad markup.</p>
<p>To set the iterator and the tag name, you must define the following class
attributes:</p>
<dl class="attribute">
<dt id="scrapy.contrib.spiders.XMLFeedSpider.iterator">
<tt class="descname">iterator</tt><a class="headerlink" href="#scrapy.contrib.spiders.XMLFeedSpider.iterator" title="Permalink to this definition">¶</a></dt>
<dd><p>A string which defines the iterator to use. It can be either:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">'iternodes'</span></tt> - a fast iterator based on regular expressions</li>
<li><tt class="docutils literal"><span class="pre">'html'</span></tt> - an iterator which uses <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a>.
Keep in mind this uses DOM parsing and must load all DOM in memory
which could be a problem for big feeds</li>
<li><tt class="docutils literal"><span class="pre">'xml'</span></tt> - an iterator which uses <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a>.
Keep in mind this uses DOM parsing and must load all DOM in memory
which could be a problem for big feeds</li>
</ul>
</div></blockquote>
<p>It defaults to: <tt class="docutils literal"><span class="pre">'iternodes'</span></tt>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.spiders.XMLFeedSpider.itertag">
<tt class="descname">itertag</tt><a class="headerlink" href="#scrapy.contrib.spiders.XMLFeedSpider.itertag" title="Permalink to this definition">¶</a></dt>
<dd><p>A string with the name of the node (or element) to iterate in. Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">itertag</span> <span class="o">=</span> <span class="s">&#39;product&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.spiders.XMLFeedSpider.namespaces">
<tt class="descname">namespaces</tt><a class="headerlink" href="#scrapy.contrib.spiders.XMLFeedSpider.namespaces" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of <tt class="docutils literal"><span class="pre">(prefix,</span> <span class="pre">uri)</span></tt> tuples which define the namespaces
available in that document that will be processed with this spider. The
<tt class="docutils literal"><span class="pre">prefix</span></tt> and <tt class="docutils literal"><span class="pre">uri</span></tt> will be used to automatically register
namespaces using the
<a class="reference internal" href="index.html#scrapy.selector.Selector.register_namespace" title="scrapy.selector.Selector.register_namespace"><tt class="xref py py-meth docutils literal"><span class="pre">register_namespace()</span></tt></a> method.</p>
<p>You can then specify nodes with namespaces in the <a class="reference internal" href="index.html#scrapy.contrib.spiders.XMLFeedSpider.itertag" title="scrapy.contrib.spiders.XMLFeedSpider.itertag"><tt class="xref py py-attr docutils literal"><span class="pre">itertag</span></tt></a>
attribute.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">YourSpider</span><span class="p">(</span><span class="n">XMLFeedSpider</span><span class="p">):</span>

    <span class="n">namespaces</span> <span class="o">=</span> <span class="p">[(</span><span class="s">&#39;n&#39;</span><span class="p">,</span> <span class="s">&#39;http://www.sitemaps.org/schemas/sitemap/0.9&#39;</span><span class="p">)]</span>
    <span class="n">itertag</span> <span class="o">=</span> <span class="s">&#39;n:url&#39;</span>
    <span class="c"># ...</span>
</pre></div>
</div>
</dd></dl>

<p>Apart from these new attributes, this spider has the following overrideable
methods too:</p>
<dl class="method">
<dt id="scrapy.contrib.spiders.XMLFeedSpider.adapt_response">
<tt class="descname">adapt_response</tt><big>(</big><em>response</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spiders.XMLFeedSpider.adapt_response" title="Permalink to this definition">¶</a></dt>
<dd><p>A method that receives the response as soon as it arrives from the spider
middleware, before the spider starts parsing it. It can be used to modify
the response body before parsing it. This method receives a response and
also returns a response (it could be the same or another one).</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.spiders.XMLFeedSpider.parse_node">
<tt class="descname">parse_node</tt><big>(</big><em>response</em>, <em>selector</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spiders.XMLFeedSpider.parse_node" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for the nodes matching the provided tag name
(<tt class="docutils literal"><span class="pre">itertag</span></tt>).  Receives the response and an
<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> for each node.  Overriding this
method is mandatory. Otherwise, you spider won&#8217;t work.  This method
must return either a <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> object, a
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object, or an iterable containing any of
them.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.spiders.XMLFeedSpider.process_results">
<tt class="descname">process_results</tt><big>(</big><em>response</em>, <em>results</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spiders.XMLFeedSpider.process_results" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for each result (item or request) returned by the
spider, and it&#8217;s intended to perform any last time processing required
before returning the results to the framework core, for example setting the
item IDs. It receives a list of results and the response which originated
those results. It must return a list of results (Items or Requests).</p>
</dd></dl>

</dd></dl>

<div class="section" id="xmlfeedspider-example">
<h6>XMLFeedSpider example<a class="headerlink" href="#xmlfeedspider-example" title="Permalink to this headline">¶</a></h6>
<p>These spiders are pretty easy to use, let&#8217;s have a look at one example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">log</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">XMLFeedSpider</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">TestItem</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">XMLFeedSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/feed.xml&#39;</span><span class="p">]</span>
    <span class="n">iterator</span> <span class="o">=</span> <span class="s">&#39;iternodes&#39;</span>  <span class="c"># This is actually unnecessary, since it&#39;s the default value</span>
    <span class="n">itertag</span> <span class="o">=</span> <span class="s">&#39;item&#39;</span>

    <span class="k">def</span> <span class="nf">parse_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="n">log</span><span class="o">.</span><span class="n">msg</span><span class="p">(</span><span class="s">&#39;Hi, this is a &lt;</span><span class="si">%s</span><span class="s">&gt; node!: </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">itertag</span><span class="p">,</span> <span class="s">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">extract</span><span class="p">())))</span>

        <span class="n">item</span> <span class="o">=</span> <span class="n">TestItem</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;@id&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;description&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;description&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>Basically what we did up there was to create a spider that downloads a feed from
the given <tt class="docutils literal"><span class="pre">start_urls</span></tt>, and then iterates through each of its <tt class="docutils literal"><span class="pre">item</span></tt> tags,
prints them out, and stores some random data in an <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a>.</p>
</div>
</div>
<div class="section" id="csvfeedspider">
<h5>CSVFeedSpider<a class="headerlink" href="#csvfeedspider" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spiders.CSVFeedSpider">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spiders.</tt><tt class="descname">CSVFeedSpider</tt><a class="headerlink" href="#scrapy.contrib.spiders.CSVFeedSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>This spider is very similar to the XMLFeedSpider, except that it iterates
over rows, instead of nodes. The method that gets called in each iteration
is <a class="reference internal" href="index.html#scrapy.contrib.spiders.CSVFeedSpider.parse_row" title="scrapy.contrib.spiders.CSVFeedSpider.parse_row"><tt class="xref py py-meth docutils literal"><span class="pre">parse_row()</span></tt></a>.</p>
<dl class="attribute">
<dt id="scrapy.contrib.spiders.CSVFeedSpider.delimiter">
<tt class="descname">delimiter</tt><a class="headerlink" href="#scrapy.contrib.spiders.CSVFeedSpider.delimiter" title="Permalink to this definition">¶</a></dt>
<dd><p>A string with the separator character for each field in the CSV file
Defaults to <tt class="docutils literal"><span class="pre">','</span></tt> (comma).</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.spiders.CSVFeedSpider.headers">
<tt class="descname">headers</tt><a class="headerlink" href="#scrapy.contrib.spiders.CSVFeedSpider.headers" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of the rows contained in the file CSV feed which will be used to
extract fields from it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.spiders.CSVFeedSpider.parse_row">
<tt class="descname">parse_row</tt><big>(</big><em>response</em>, <em>row</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spiders.CSVFeedSpider.parse_row" title="Permalink to this definition">¶</a></dt>
<dd><p>Receives a response and a dict (representing each row) with a key for each
provided (or detected) header of the CSV file.  This spider also gives the
opportunity to override <tt class="docutils literal"><span class="pre">adapt_response</span></tt> and <tt class="docutils literal"><span class="pre">process_results</span></tt> methods
for pre- and post-processing purposes.</p>
</dd></dl>

</dd></dl>

<div class="section" id="csvfeedspider-example">
<h6>CSVFeedSpider example<a class="headerlink" href="#csvfeedspider-example" title="Permalink to this headline">¶</a></h6>
<p>Let&#8217;s see an example similar to the previous one, but using a
<a class="reference internal" href="index.html#scrapy.contrib.spiders.CSVFeedSpider" title="scrapy.contrib.spiders.CSVFeedSpider"><tt class="xref py py-class docutils literal"><span class="pre">CSVFeedSpider</span></tt></a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">log</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">CSVFeedSpider</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">TestItem</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CSVFeedSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/feed.csv&#39;</span><span class="p">]</span>
    <span class="n">delimiter</span> <span class="o">=</span> <span class="s">&#39;;&#39;</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;id&#39;</span><span class="p">,</span> <span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;description&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse_row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">row</span><span class="p">):</span>
        <span class="n">log</span><span class="o">.</span><span class="n">msg</span><span class="p">(</span><span class="s">&#39;Hi, this is a row!: </span><span class="si">%r</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">row</span><span class="p">)</span>

        <span class="n">item</span> <span class="o">=</span> <span class="n">TestItem</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s">&#39;id&#39;</span><span class="p">]</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">]</span>
        <span class="n">item</span><span class="p">[</span><span class="s">&#39;description&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s">&#39;description&#39;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="sitemapspider">
<h5>SitemapSpider<a class="headerlink" href="#sitemapspider" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spiders.SitemapSpider">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spiders.</tt><tt class="descname">SitemapSpider</tt><a class="headerlink" href="#scrapy.contrib.spiders.SitemapSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>SitemapSpider allows you to crawl a site by discovering the URLs using
<a class="reference external" href="http://www.sitemaps.org">Sitemaps</a>.</p>
<p>It supports nested sitemaps and discovering sitemap urls from
<a class="reference external" href="http://www.robotstxt.org/">robots.txt</a>.</p>
<dl class="attribute">
<dt id="scrapy.contrib.spiders.SitemapSpider.sitemap_urls">
<tt class="descname">sitemap_urls</tt><a class="headerlink" href="#scrapy.contrib.spiders.SitemapSpider.sitemap_urls" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of urls pointing to the sitemaps whose urls you want to crawl.</p>
<p>You can also point to a <a class="reference external" href="http://www.robotstxt.org/">robots.txt</a> and it will be parsed to extract
sitemap urls from it.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.spiders.SitemapSpider.sitemap_rules">
<tt class="descname">sitemap_rules</tt><a class="headerlink" href="#scrapy.contrib.spiders.SitemapSpider.sitemap_rules" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of tuples <tt class="docutils literal"><span class="pre">(regex,</span> <span class="pre">callback)</span></tt> where:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">regex</span></tt> is a regular expression to match urls extracted from sitemaps.
<tt class="docutils literal"><span class="pre">regex</span></tt> can be either a str or a compiled regex object.</li>
<li>callback is the callback to use for processing the urls that match
the regular expression. <tt class="docutils literal"><span class="pre">callback</span></tt> can be a string (indicating the
name of a spider method) or a callable.</li>
</ul>
<p>For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[(</span><span class="s">&#39;/product/&#39;</span><span class="p">,</span> <span class="s">&#39;parse_product&#39;</span><span class="p">)]</span>
</pre></div>
</div>
<p>Rules are applied in order, and only the first one that matches will be
used.</p>
<p>If you omit this attribute, all urls found in sitemaps will be
processed with the <tt class="docutils literal"><span class="pre">parse</span></tt> callback.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.spiders.SitemapSpider.sitemap_follow">
<tt class="descname">sitemap_follow</tt><a class="headerlink" href="#scrapy.contrib.spiders.SitemapSpider.sitemap_follow" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of regexes of sitemap that should be followed. This is is only
for sites that use <a class="reference external" href="http://www.sitemaps.org/protocol.php#index">Sitemap index files</a> that point to other sitemap
files.</p>
<p>By default, all sitemaps are followed.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.spiders.SitemapSpider.sitemap_alternate_links">
<tt class="descname">sitemap_alternate_links</tt><a class="headerlink" href="#scrapy.contrib.spiders.SitemapSpider.sitemap_alternate_links" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies if alternate links for one <tt class="docutils literal"><span class="pre">url</span></tt> should be followed. These
are links for the same website in another language passed within
the same <tt class="docutils literal"><span class="pre">url</span></tt> block.</p>
<p>For example:</p>
<div class="highlight-python"><div class="highlight"><pre>&lt;url&gt;
    &lt;loc&gt;http://example.com/&lt;/loc&gt;
    &lt;xhtml:link rel=&quot;alternate&quot; hreflang=&quot;de&quot; href=&quot;http://example.com/de&quot;/&gt;
&lt;/url&gt;
</pre></div>
</div>
<p>With <tt class="docutils literal"><span class="pre">sitemap_alternate_links</span></tt> set, this would retrieve both URLs. With
<tt class="docutils literal"><span class="pre">sitemap_alternate_links</span></tt> disabled, only <tt class="docutils literal"><span class="pre">http://example.com/</span></tt> would be
retrieved.</p>
<p>Default is <tt class="docutils literal"><span class="pre">sitemap_alternate_links</span></tt> disabled.</p>
</dd></dl>

</dd></dl>

<div class="section" id="sitemapspider-examples">
<h6>SitemapSpider examples<a class="headerlink" href="#sitemapspider-examples" title="Permalink to this headline">¶</a></h6>
<p>Simplest example: process all urls discovered through sitemaps using the
<tt class="docutils literal"><span class="pre">parse</span></tt> callback:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/sitemap.xml&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c"># ... scrape item here ...</span>
</pre></div>
</div>
<p>Process some urls with certain callback and other urls with a different
callback:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/sitemap.xml&#39;</span><span class="p">]</span>
    <span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s">&#39;/product/&#39;</span><span class="p">,</span> <span class="s">&#39;parse_product&#39;</span><span class="p">),</span>
        <span class="p">(</span><span class="s">&#39;/category/&#39;</span><span class="p">,</span> <span class="s">&#39;parse_category&#39;</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse_product</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c"># ... scrape product ...</span>

    <span class="k">def</span> <span class="nf">parse_category</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c"># ... scrape category ...</span>
</pre></div>
</div>
<p>Follow sitemaps defined in the <a class="reference external" href="http://www.robotstxt.org/">robots.txt</a> file and only follow sitemaps
whose url contains <tt class="docutils literal"><span class="pre">/sitemap_shop</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/robots.txt&#39;</span><span class="p">]</span>
    <span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s">&#39;/shop/&#39;</span><span class="p">,</span> <span class="s">&#39;parse_shop&#39;</span><span class="p">),</span>
    <span class="p">]</span>
    <span class="n">sitemap_follow</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;/sitemap_shops&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse_shop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c"># ... scrape shop here ...</span>
</pre></div>
</div>
<p>Combine SitemapSpider with other sources of urls:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/robots.txt&#39;</span><span class="p">]</span>
    <span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s">&#39;/shop/&#39;</span><span class="p">,</span> <span class="s">&#39;parse_shop&#39;</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="n">other_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/about&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">requests</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">super</span><span class="p">(</span><span class="n">MySpider</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">start_requests</span><span class="p">())</span>
        <span class="n">requests</span> <span class="o">+=</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_other</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">other_urls</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">requests</span>

    <span class="k">def</span> <span class="nf">parse_shop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c"># ... scrape shop here ...</span>

    <span class="k">def</span> <span class="nf">parse_other</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c"># ... scrape other here ...</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<span id="document-topics/selectors"></span><div class="section" id="selectors">
<span id="topics-selectors"></span><h3>Selectors<a class="headerlink" href="#selectors" title="Permalink to this headline">¶</a></h3>
<p>When you&#8217;re scraping web pages, the most common task you need to perform is
to extract data from the HTML source. There are several libraries available to
achieve this:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> is a very popular screen scraping library among Python
programmers which constructs a Python object based on the structure of the
HTML code and also deals with bad markup reasonably well, but it has one
drawback: it&#8217;s slow.</li>
<li><a class="reference external" href="http://lxml.de/">lxml</a> is a XML parsing library (which also parses HTML) with a pythonic
API based on <a class="reference external" href="http://docs.python.org/library/xml.etree.elementtree.html">ElementTree</a> (which is not part of the Python standard
library).</li>
</ul>
</div></blockquote>
<p>Scrapy comes with its own mechanism for extracting data. They&#8217;re called
selectors because they &#8220;select&#8221; certain parts of the HTML document specified
either by <a class="reference external" href="http://www.w3.org/TR/xpath">XPath</a> or <a class="reference external" href="http://www.w3.org/TR/selectors">CSS</a> expressions.</p>
<p><a class="reference external" href="http://www.w3.org/TR/xpath">XPath</a> is a language for selecting nodes in XML documents, which can also be
used with HTML. <a class="reference external" href="http://www.w3.org/TR/selectors">CSS</a> is a language for applying styles to HTML documents. It
defines selectors to associate those styles with specific HTML elements.</p>
<p>Scrapy selectors are built over the <a class="reference external" href="http://lxml.de/">lxml</a> library, which means they&#8217;re very
similar in speed and parsing accuracy.</p>
<p>This page explains how selectors work and describes their API which is very
small and simple, unlike the <a class="reference external" href="http://lxml.de/">lxml</a> API which is much bigger because the
<a class="reference external" href="http://lxml.de/">lxml</a> library can be used for many other tasks, besides selecting markup
documents.</p>
<p>For a complete reference of the selectors API see
<a class="reference internal" href="index.html#topics-selectors-ref"><em>Selector reference</em></a></p>
<div class="section" id="using-selectors">
<h4>Using selectors<a class="headerlink" href="#using-selectors" title="Permalink to this headline">¶</a></h4>
<div class="section" id="constructing-selectors">
<h5>Constructing selectors<a class="headerlink" href="#constructing-selectors" title="Permalink to this headline">¶</a></h5>
<p>Scrapy selectors are instances of <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> class
constructed by passing <strong>text</strong> or <a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a>
object. It automatically chooses the best parsing rules (XML vs HTML) based on
input type:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.http</span> <span class="kn">import</span> <span class="n">HtmlResponse</span>
</pre></div>
</div>
<p>Constructing from text:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">body</span> <span class="o">=</span> <span class="s">&#39;&lt;html&gt;&lt;body&gt;&lt;span&gt;good&lt;/span&gt;&lt;/body&gt;&lt;/html&gt;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">body</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;good&#39;]</span>
</pre></div>
</div>
<p>Constructing from response:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span> <span class="o">=</span> <span class="n">HtmlResponse</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">&#39;http://example.com&#39;</span><span class="p">,</span> <span class="n">body</span><span class="o">=</span><span class="n">body</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;good&#39;]</span>
</pre></div>
</div>
<p>For convenience, response objects exposes a selector on <cite>.selector</cite> attribute,
it&#8217;s totally OK to use this shortcut when possible:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;good&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="id1">
<h5>Using selectors<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<p>To explain how to use the selectors we&#8217;ll use the <cite>Scrapy shell</cite> (which
provides interactive testing) and an example page located in the Scrapy
documentation server:</p>
<blockquote>
<div><a class="reference external" href="http://doc.scrapy.org/en/latest/_static/selectors-sample1.html">http://doc.scrapy.org/en/latest/_static/selectors-sample1.html</a></div></blockquote>
<p id="topics-selectors-htmlcode">Here&#8217;s its HTML code:</p>
<div class="highlight-html"><div class="highlight"><pre><span class="nt">&lt;html&gt;</span>
 <span class="nt">&lt;head&gt;</span>
  <span class="nt">&lt;base</span> <span class="na">href=</span><span class="s">&#39;http://example.com/&#39;</span> <span class="nt">/&gt;</span>
  <span class="nt">&lt;title&gt;</span>Example website<span class="nt">&lt;/title&gt;</span>
 <span class="nt">&lt;/head&gt;</span>
 <span class="nt">&lt;body&gt;</span>
  <span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&#39;images&#39;</span><span class="nt">&gt;</span>
   <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">&#39;image1.html&#39;</span><span class="nt">&gt;</span>Name: My image 1 <span class="nt">&lt;br</span> <span class="nt">/&gt;&lt;img</span> <span class="na">src=</span><span class="s">&#39;image1_thumb.jpg&#39;</span> <span class="nt">/&gt;&lt;/a&gt;</span>
   <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">&#39;image2.html&#39;</span><span class="nt">&gt;</span>Name: My image 2 <span class="nt">&lt;br</span> <span class="nt">/&gt;&lt;img</span> <span class="na">src=</span><span class="s">&#39;image2_thumb.jpg&#39;</span> <span class="nt">/&gt;&lt;/a&gt;</span>
   <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">&#39;image3.html&#39;</span><span class="nt">&gt;</span>Name: My image 3 <span class="nt">&lt;br</span> <span class="nt">/&gt;&lt;img</span> <span class="na">src=</span><span class="s">&#39;image3_thumb.jpg&#39;</span> <span class="nt">/&gt;&lt;/a&gt;</span>
   <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">&#39;image4.html&#39;</span><span class="nt">&gt;</span>Name: My image 4 <span class="nt">&lt;br</span> <span class="nt">/&gt;&lt;img</span> <span class="na">src=</span><span class="s">&#39;image4_thumb.jpg&#39;</span> <span class="nt">/&gt;&lt;/a&gt;</span>
   <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">&#39;image5.html&#39;</span><span class="nt">&gt;</span>Name: My image 5 <span class="nt">&lt;br</span> <span class="nt">/&gt;&lt;img</span> <span class="na">src=</span><span class="s">&#39;image5_thumb.jpg&#39;</span> <span class="nt">/&gt;&lt;/a&gt;</span>
  <span class="nt">&lt;/div&gt;</span>
 <span class="nt">&lt;/body&gt;</span>
<span class="nt">&lt;/html&gt;</span>
</pre></div>
</div>
<p>First, let&#8217;s open the shell:</p>
<div class="highlight-sh"><div class="highlight"><pre>scrapy shell http://doc.scrapy.org/en/latest/_static/selectors-sample1.html
</pre></div>
</div>
<p>Then, after the shell loads, you&#8217;ll have the response available as <tt class="docutils literal"><span class="pre">response</span></tt>
shell variable, and its attached selector in <tt class="docutils literal"><span class="pre">response.selector</span></tt> attribute.</p>
<p>Since we&#8217;re dealing with HTML, the selector will automatically use an HTML parser.</p>
<p>So, by looking at the <a class="reference internal" href="index.html#topics-selectors-htmlcode"><em>HTML code</em></a> of that
page, let&#8217;s construct an XPath for selecting the text inside the title tag:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//title/text()&#39;</span><span class="p">)</span>
<span class="go">[&lt;Selector (text) xpath=//title/text()&gt;]</span>
</pre></div>
</div>
<p>Querying responses using XPath and CSS is so common that responses includes two
convenient shortcuts: <tt class="docutils literal"><span class="pre">response.xpath()</span></tt> and <tt class="docutils literal"><span class="pre">response.css()</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//title/text()&#39;</span><span class="p">)</span>
<span class="go">[&lt;Selector (text) xpath=//title/text()&gt;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">&#39;title::text&#39;</span><span class="p">)</span>
<span class="go">[&lt;Selector (text) xpath=//title/text()&gt;]</span>
</pre></div>
</div>
<p>As you can see, <tt class="docutils literal"><span class="pre">.xpath()</span></tt> and <tt class="docutils literal"><span class="pre">.css()</span></tt> methods returns an
<a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><tt class="xref py py-class docutils literal"><span class="pre">SelectorList</span></tt></a> instance, which is a list of new
selectors. This API can be used quickly for selecting nested data:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">&#39;img&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;@src&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;image1_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image2_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image3_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image4_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image5_thumb.jpg&#39;]</span>
</pre></div>
</div>
<p>To actually extract the textual data, you must call the selector <tt class="docutils literal"><span class="pre">.extract()</span></tt>
method, as follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//title/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;Example website&#39;]</span>
</pre></div>
</div>
<p>Notice that CSS selectors can select text or attribute nodes using CSS3
pseudo-elements:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">&#39;title::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;Example website&#39;]</span>
</pre></div>
</div>
<p>Now we&#8217;re going to get the base URL and some image links:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//base/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;http://example.com/&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">&#39;base::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;http://example.com/&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//a[contains(@href, &quot;image&quot;)]/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;image1.html&#39;,</span>
<span class="go"> u&#39;image2.html&#39;,</span>
<span class="go"> u&#39;image3.html&#39;,</span>
<span class="go"> u&#39;image4.html&#39;,</span>
<span class="go"> u&#39;image5.html&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">&#39;a[href*=image]::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;image1.html&#39;,</span>
<span class="go"> u&#39;image2.html&#39;,</span>
<span class="go"> u&#39;image3.html&#39;,</span>
<span class="go"> u&#39;image4.html&#39;,</span>
<span class="go"> u&#39;image5.html&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//a[contains(@href, &quot;image&quot;)]/img/@src&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;image1_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image2_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image3_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image4_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image5_thumb.jpg&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">&#39;a[href*=image] img::attr(src)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;image1_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image2_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image3_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image4_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image5_thumb.jpg&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="nesting-selectors">
<span id="topics-selectors-nesting-selectors"></span><h5>Nesting selectors<a class="headerlink" href="#nesting-selectors" title="Permalink to this headline">¶</a></h5>
<p>The selection methods (<tt class="docutils literal"><span class="pre">.xpath()</span></tt> or <tt class="docutils literal"><span class="pre">.css()</span></tt>) returns a list of selectors
of the same type, so you can call the selection methods for those selectors
too. Here&#8217;s an example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">links</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//a[contains(@href, &quot;image&quot;)]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">links</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;&lt;a href=&quot;image1.html&quot;&gt;Name: My image 1 &lt;br&gt;&lt;img src=&quot;image1_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go"> u&#39;&lt;a href=&quot;image2.html&quot;&gt;Name: My image 2 &lt;br&gt;&lt;img src=&quot;image2_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go"> u&#39;&lt;a href=&quot;image3.html&quot;&gt;Name: My image 3 &lt;br&gt;&lt;img src=&quot;image3_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go"> u&#39;&lt;a href=&quot;image4.html&quot;&gt;Name: My image 4 &lt;br&gt;&lt;img src=&quot;image4_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go"> u&#39;&lt;a href=&quot;image5.html&quot;&gt;Name: My image 5 &lt;br&gt;&lt;img src=&quot;image5_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">link</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">links</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">link</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">(),</span> <span class="n">link</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;img/@src&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">())</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="s">&#39;Link number </span><span class="si">%d</span><span class="s"> points to url </span><span class="si">%s</span><span class="s"> and image </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">args</span>

<span class="go">Link number 0 points to url [u&#39;image1.html&#39;] and image [u&#39;image1_thumb.jpg&#39;]</span>
<span class="go">Link number 1 points to url [u&#39;image2.html&#39;] and image [u&#39;image2_thumb.jpg&#39;]</span>
<span class="go">Link number 2 points to url [u&#39;image3.html&#39;] and image [u&#39;image3_thumb.jpg&#39;]</span>
<span class="go">Link number 3 points to url [u&#39;image4.html&#39;] and image [u&#39;image4_thumb.jpg&#39;]</span>
<span class="go">Link number 4 points to url [u&#39;image5.html&#39;] and image [u&#39;image5_thumb.jpg&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="using-selectors-with-regular-expressions">
<h5>Using selectors with regular expressions<a class="headerlink" href="#using-selectors-with-regular-expressions" title="Permalink to this headline">¶</a></h5>
<p><a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> also have a <tt class="docutils literal"><span class="pre">.re()</span></tt> method for extracting
data using regular expressions. However, unlike using <tt class="docutils literal"><span class="pre">.xpath()</span></tt> or
<tt class="docutils literal"><span class="pre">.css()</span></tt> methods, <tt class="docutils literal"><span class="pre">.re()</span></tt> method returns a list of unicode strings. So you
can&#8217;t construct nested <tt class="docutils literal"><span class="pre">.re()</span></tt> calls.</p>
<p>Here&#8217;s an example used to extract images names from the <a class="reference internal" href="index.html#topics-selectors-htmlcode"><em>HTML code</em></a> above:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//a[contains(@href, &quot;image&quot;)]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="s">r&#39;Name:\s*(.*)&#39;</span><span class="p">)</span>
<span class="go">[u&#39;My image 1&#39;,</span>
<span class="go"> u&#39;My image 2&#39;,</span>
<span class="go"> u&#39;My image 3&#39;,</span>
<span class="go"> u&#39;My image 4&#39;,</span>
<span class="go"> u&#39;My image 5&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="working-with-relative-xpaths">
<span id="topics-selectors-relative-xpaths"></span><h5>Working with relative XPaths<a class="headerlink" href="#working-with-relative-xpaths" title="Permalink to this headline">¶</a></h5>
<p>Keep in mind that if you are nesting selectors and use an XPath that starts
with <tt class="docutils literal"><span class="pre">/</span></tt>, that XPath will be absolute to the document and not relative to the
<tt class="docutils literal"><span class="pre">Selector</span></tt> you&#8217;re calling it from.</p>
<p>For example, suppose you want to extract all <tt class="docutils literal"><span class="pre">&lt;p&gt;</span></tt> elements inside <tt class="docutils literal"><span class="pre">&lt;div&gt;</span></tt>
elements. First, you would get all <tt class="docutils literal"><span class="pre">&lt;div&gt;</span></tt> elements:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">divs</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//div&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>At first, you may be tempted to use the following approach, which is wrong, as
it actually extracts all <tt class="docutils literal"><span class="pre">&lt;p&gt;</span></tt> elements from the document, not only those
inside <tt class="docutils literal"><span class="pre">&lt;div&gt;</span></tt> elements:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//p&#39;</span><span class="p">):</span>  <span class="c"># this is wrong - gets all &lt;p&gt; from the whole document</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>This is the proper way to do it (note the dot prefixing the <tt class="docutils literal"><span class="pre">.//p</span></tt> XPath):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;.//p&#39;</span><span class="p">):</span>  <span class="c"># extracts all &lt;p&gt; inside</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>Another common case would be to extract all direct <tt class="docutils literal"><span class="pre">&lt;p&gt;</span></tt> children:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;p&#39;</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>For more details about relative XPaths see the <a class="reference external" href="http://www.w3.org/TR/xpath#location-paths">Location Paths</a> section in the
XPath specification.</p>
</div>
<div class="section" id="using-exslt-extensions">
<h5>Using EXSLT extensions<a class="headerlink" href="#using-exslt-extensions" title="Permalink to this headline">¶</a></h5>
<p>Being built atop <a class="reference external" href="http://lxml.de/">lxml</a>, Scrapy selectors also support some <a class="reference external" href="http://www.exslt.org/">EXSLT</a> extensions
and come with these pre-registered namespaces to use in XPath expressions:</p>
<table border="1" class="docutils">
<colgroup>
<col width="9%" />
<col width="56%" />
<col width="35%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">prefix</th>
<th class="head">namespace</th>
<th class="head">usage</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>re</td>
<td>http://exslt.org/regular-expressions</td>
<td><a class="reference external" href="http://www.exslt.org/regexp/index.html">regular expressions</a></td>
</tr>
<tr class="row-odd"><td>set</td>
<td>http://exslt.org/sets</td>
<td><a class="reference external" href="http://www.exslt.org/set/index.html">set manipulation</a></td>
</tr>
</tbody>
</table>
<div class="section" id="regular-expressions">
<h6>Regular expressions<a class="headerlink" href="#regular-expressions" title="Permalink to this headline">¶</a></h6>
<p>The <tt class="docutils literal"><span class="pre">test()</span></tt> function for example can prove quite useful when XPath&#8217;s
<tt class="docutils literal"><span class="pre">starts-with()</span></tt> or <tt class="docutils literal"><span class="pre">contains()</span></tt> are not sufficient.</p>
<p>Example selecting links in list item with a &#8220;class&#8221; attribute ending with a digit:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">doc</span> <span class="o">=</span> <span class="s">&quot;&quot;&quot;</span>
<span class="gp">... </span><span class="s">&lt;div&gt;</span>
<span class="gp">... </span><span class="s">    &lt;ul&gt;</span>
<span class="gp">... </span><span class="s">        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s">        &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s">        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s">    &lt;/ul&gt;</span>
<span class="gp">... </span><span class="s">&lt;/div&gt;</span>
<span class="gp">... </span><span class="s">&quot;&quot;&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">doc</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s">&quot;html&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//li//@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;link1.html&#39;, u&#39;link2.html&#39;, u&#39;link3.html&#39;, u&#39;link4.html&#39;, u&#39;link5.html&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//li[re:test(@class, &quot;item-\d$&quot;)]//@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;link1.html&#39;, u&#39;link2.html&#39;, u&#39;link4.html&#39;, u&#39;link5.html&#39;]</span>
<span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">C library <tt class="docutils literal"><span class="pre">libxslt</span></tt> doesn&#8217;t natively support EXSLT regular
expressions so <a class="reference external" href="http://lxml.de/">lxml</a>&#8216;s implementation uses hooks to Python&#8217;s <tt class="docutils literal"><span class="pre">re</span></tt> module.
Thus, using regexp functions in your XPath expressions may add a small
performance penalty.</p>
</div>
</div>
<div class="section" id="set-operations">
<h6>Set operations<a class="headerlink" href="#set-operations" title="Permalink to this headline">¶</a></h6>
<p>These can be handy for excluding parts of a document tree before
extracting text elements for example.</p>
<p>Example extracting microdata (sample content taken from <a class="reference external" href="http://schema.org/Product">http://schema.org/Product</a>)
with groups of itemscopes and corresponding itemprops:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">doc</span> <span class="o">=</span> <span class="s">&quot;&quot;&quot;</span>
<span class="gp">... </span><span class="s">&lt;div itemscope itemtype=&quot;http://schema.org/Product&quot;&gt;</span>
<span class="gp">... </span><span class="s">  &lt;span itemprop=&quot;name&quot;&gt;Kenmore White 17&quot; Microwave&lt;/span&gt;</span>
<span class="gp">... </span><span class="s">  &lt;img src=&quot;kenmore-microwave-17in.jpg&quot; alt=&#39;Kenmore 17&quot; Microwave&#39; /&gt;</span>
<span class="gp">... </span><span class="s">  &lt;div itemprop=&quot;aggregateRating&quot;</span>
<span class="gp">... </span><span class="s">    itemscope itemtype=&quot;http://schema.org/AggregateRating&quot;&gt;</span>
<span class="gp">... </span><span class="s">   Rated &lt;span itemprop=&quot;ratingValue&quot;&gt;3.5&lt;/span&gt;/5</span>
<span class="gp">... </span><span class="s">   based on &lt;span itemprop=&quot;reviewCount&quot;&gt;11&lt;/span&gt; customer reviews</span>
<span class="gp">... </span><span class="s">  &lt;/div&gt;</span>
<span class="gp">...</span><span class="s"></span>
<span class="gp">... </span><span class="s">  &lt;div itemprop=&quot;offers&quot; itemscope itemtype=&quot;http://schema.org/Offer&quot;&gt;</span>
<span class="gp">... </span><span class="s">    &lt;span itemprop=&quot;price&quot;&gt;$55.00&lt;/span&gt;</span>
<span class="gp">... </span><span class="s">    &lt;link itemprop=&quot;availability&quot; href=&quot;http://schema.org/InStock&quot; /&gt;In stock</span>
<span class="gp">... </span><span class="s">  &lt;/div&gt;</span>
<span class="gp">...</span><span class="s"></span>
<span class="gp">... </span><span class="s">  Product description:</span>
<span class="gp">... </span><span class="s">  &lt;span itemprop=&quot;description&quot;&gt;0.7 cubic feet countertop microwave.</span>
<span class="gp">... </span><span class="s">  Has six preset cooking categories and convenience features like</span>
<span class="gp">... </span><span class="s">  Add-A-Minute and Child Lock.&lt;/span&gt;</span>
<span class="gp">...</span><span class="s"></span>
<span class="gp">... </span><span class="s">  Customer reviews:</span>
<span class="gp">...</span><span class="s"></span>
<span class="gp">... </span><span class="s">  &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;</span>
<span class="gp">... </span><span class="s">    &lt;span itemprop=&quot;name&quot;&gt;Not a happy camper&lt;/span&gt; -</span>
<span class="gp">... </span><span class="s">    by &lt;span itemprop=&quot;author&quot;&gt;Ellie&lt;/span&gt;,</span>
<span class="gp">... </span><span class="s">    &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-04-01&quot;&gt;April 1, 2011</span>
<span class="gp">... </span><span class="s">    &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;</span>
<span class="gp">... </span><span class="s">      &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;&gt;</span>
<span class="gp">... </span><span class="s">      &lt;span itemprop=&quot;ratingValue&quot;&gt;1&lt;/span&gt;/</span>
<span class="gp">... </span><span class="s">      &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars</span>
<span class="gp">... </span><span class="s">    &lt;/div&gt;</span>
<span class="gp">... </span><span class="s">    &lt;span itemprop=&quot;description&quot;&gt;The lamp burned out and now I have to replace</span>
<span class="gp">... </span><span class="s">    it. &lt;/span&gt;</span>
<span class="gp">... </span><span class="s">  &lt;/div&gt;</span>
<span class="gp">...</span><span class="s"></span>
<span class="gp">... </span><span class="s">  &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;</span>
<span class="gp">... </span><span class="s">    &lt;span itemprop=&quot;name&quot;&gt;Value purchase&lt;/span&gt; -</span>
<span class="gp">... </span><span class="s">    by &lt;span itemprop=&quot;author&quot;&gt;Lucas&lt;/span&gt;,</span>
<span class="gp">... </span><span class="s">    &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-03-25&quot;&gt;March 25, 2011</span>
<span class="gp">... </span><span class="s">    &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;</span>
<span class="gp">... </span><span class="s">      &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;/&gt;</span>
<span class="gp">... </span><span class="s">      &lt;span itemprop=&quot;ratingValue&quot;&gt;4&lt;/span&gt;/</span>
<span class="gp">... </span><span class="s">      &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars</span>
<span class="gp">... </span><span class="s">    &lt;/div&gt;</span>
<span class="gp">... </span><span class="s">    &lt;span itemprop=&quot;description&quot;&gt;Great microwave for the price. It is small and</span>
<span class="gp">... </span><span class="s">    fits in my apartment.&lt;/span&gt;</span>
<span class="gp">... </span><span class="s">  &lt;/div&gt;</span>
<span class="gp">... </span><span class="s">  ...</span>
<span class="gp">... </span><span class="s">&lt;/div&gt;</span>
<span class="gp">... </span><span class="s">&quot;&quot;&quot;</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">scope</span> <span class="ow">in</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//div[@itemscope]&#39;</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="s">&quot;current scope:&quot;</span><span class="p">,</span> <span class="n">scope</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;@itemtype&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">props</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;&#39;&#39;</span>
<span class="gp">... </span><span class="s">                set:difference(./descendant::*/@itemprop,</span>
<span class="gp">... </span><span class="s">                               .//*[@itemscope]/*/@itemprop)&#39;&#39;&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="s">&quot;    properties:&quot;</span><span class="p">,</span> <span class="n">props</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">print</span>

<span class="go">current scope: [u&#39;http://schema.org/Product&#39;]</span>
<span class="go">    properties: [u&#39;name&#39;, u&#39;aggregateRating&#39;, u&#39;offers&#39;, u&#39;description&#39;, u&#39;review&#39;, u&#39;review&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/AggregateRating&#39;]</span>
<span class="go">    properties: [u&#39;ratingValue&#39;, u&#39;reviewCount&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/Offer&#39;]</span>
<span class="go">    properties: [u&#39;price&#39;, u&#39;availability&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/Review&#39;]</span>
<span class="go">    properties: [u&#39;name&#39;, u&#39;author&#39;, u&#39;datePublished&#39;, u&#39;reviewRating&#39;, u&#39;description&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/Rating&#39;]</span>
<span class="go">    properties: [u&#39;worstRating&#39;, u&#39;ratingValue&#39;, u&#39;bestRating&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/Review&#39;]</span>
<span class="go">    properties: [u&#39;name&#39;, u&#39;author&#39;, u&#39;datePublished&#39;, u&#39;reviewRating&#39;, u&#39;description&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/Rating&#39;]</span>
<span class="go">    properties: [u&#39;worstRating&#39;, u&#39;ratingValue&#39;, u&#39;bestRating&#39;]</span>

<span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
<p>Here we first iterate over <tt class="docutils literal"><span class="pre">itemscope</span></tt> elements, and for each one,
we look for all <tt class="docutils literal"><span class="pre">itemprops</span></tt> elements and exclude those that are themselves
inside another <tt class="docutils literal"><span class="pre">itemscope</span></tt>.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.selector">
<span id="built-in-selectors-reference"></span><span id="topics-selectors-ref"></span><h4>Built-in Selectors reference<a class="headerlink" href="#module-scrapy.selector" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.selector.Selector">
<em class="property">class </em><tt class="descclassname">scrapy.selector.</tt><tt class="descname">Selector</tt><big>(</big><em>response=None</em>, <em>text=None</em>, <em>type=None</em><big>)</big><a class="headerlink" href="#scrapy.selector.Selector" title="Permalink to this definition">¶</a></dt>
<dd><p>An instance of <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> is a wrapper over response to select
certain parts of its content.</p>
<p><tt class="docutils literal"><span class="pre">response</span></tt> is a <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></tt></a> or
<a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></tt></a> object that will be used for selecting and
extracting data.</p>
<p><tt class="docutils literal"><span class="pre">text</span></tt> is a unicode string or utf-8 encoded text for cases when a
<tt class="docutils literal"><span class="pre">response</span></tt> isn&#8217;t available. Using <tt class="docutils literal"><span class="pre">text</span></tt> and <tt class="docutils literal"><span class="pre">response</span></tt> together is
undefined behavior.</p>
<p><tt class="docutils literal"><span class="pre">type</span></tt> defines the selector type, it can be <tt class="docutils literal"><span class="pre">&quot;html&quot;</span></tt>, <tt class="docutils literal"><span class="pre">&quot;xml&quot;</span></tt> or <tt class="docutils literal"><span class="pre">None</span></tt> (default).</p>
<blockquote>
<div><blockquote>
<div><p>If <tt class="docutils literal"><span class="pre">type</span></tt> is <tt class="docutils literal"><span class="pre">None</span></tt>, the selector automatically chooses the best type
based on <tt class="docutils literal"><span class="pre">response</span></tt> type (see below), or defaults to <tt class="docutils literal"><span class="pre">&quot;html&quot;</span></tt> in case it
is used together with <tt class="docutils literal"><span class="pre">text</span></tt>.</p>
<p>If <tt class="docutils literal"><span class="pre">type</span></tt> is <tt class="docutils literal"><span class="pre">None</span></tt> and a <tt class="docutils literal"><span class="pre">response</span></tt> is passed, the selector type is
inferred from the response type as follow:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">&quot;html&quot;</span></tt> for <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></tt></a> type</li>
<li><tt class="docutils literal"><span class="pre">&quot;xml&quot;</span></tt> for <a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></tt></a> type</li>
<li><tt class="docutils literal"><span class="pre">&quot;html&quot;</span></tt> for anything else</li>
</ul>
</div></blockquote>
</div></blockquote>
<p>Otherwise, if <tt class="docutils literal"><span class="pre">type</span></tt> is set, the selector type will be forced and no
detection will occur.</p>
</div></blockquote>
<dl class="method">
<dt id="scrapy.selector.Selector.xpath">
<tt class="descname">xpath</tt><big>(</big><em>query</em><big>)</big><a class="headerlink" href="#scrapy.selector.Selector.xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Find nodes matching the xpath <tt class="docutils literal"><span class="pre">query</span></tt> and return the result as a
<a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><tt class="xref py py-class docutils literal"><span class="pre">SelectorList</span></tt></a> instance with all elements flattened. List
elements implement <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> interface too.</p>
<p><tt class="docutils literal"><span class="pre">query</span></tt> is a string containing the XPATH query to apply.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For convenience this method can be called as <tt class="docutils literal"><span class="pre">response.xpath()</span></tt></p>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.css">
<tt class="descname">css</tt><big>(</big><em>query</em><big>)</big><a class="headerlink" href="#scrapy.selector.Selector.css" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply the given CSS selector and return a <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><tt class="xref py py-class docutils literal"><span class="pre">SelectorList</span></tt></a> instance.</p>
<p><tt class="docutils literal"><span class="pre">query</span></tt> is a string containing the CSS selector to apply.</p>
<p>In the background, CSS queries are translated into XPath queries using
<a class="reference external" href="https://pypi.python.org/pypi/cssselect/">cssselect</a> library and run <tt class="docutils literal"><span class="pre">.xpath()</span></tt> method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For convenience this method can be called as <tt class="docutils literal"><span class="pre">response.css()</span></tt></p>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.extract">
<tt class="descname">extract</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.selector.Selector.extract" title="Permalink to this definition">¶</a></dt>
<dd><p>Serialize and return the matched nodes as a list of unicode strings.
Percent encoded content is unquoted.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.re">
<tt class="descname">re</tt><big>(</big><em>regex</em><big>)</big><a class="headerlink" href="#scrapy.selector.Selector.re" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply the given regex and return a list of unicode strings with the
matches.</p>
<p><tt class="docutils literal"><span class="pre">regex</span></tt> can be either a compiled regular expression or a string which
will be compiled to a regular expression using <tt class="docutils literal"><span class="pre">re.compile(regex)</span></tt></p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.register_namespace">
<tt class="descname">register_namespace</tt><big>(</big><em>prefix</em>, <em>uri</em><big>)</big><a class="headerlink" href="#scrapy.selector.Selector.register_namespace" title="Permalink to this definition">¶</a></dt>
<dd><p>Register the given namespace to be used in this <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a>.
Without registering namespaces you can&#8217;t select or extract data from
non-standard namespaces. See examples below.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.remove_namespaces">
<tt class="descname">remove_namespaces</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.selector.Selector.remove_namespaces" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove all namespaces, allowing to traverse the document using
namespace-less xpaths. See example below.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.__nonzero__">
<tt class="descname">__nonzero__</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.selector.Selector.__nonzero__" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <tt class="docutils literal"><span class="pre">True</span></tt> if there is any real content selected or <tt class="docutils literal"><span class="pre">False</span></tt>
otherwise.  In other words, the boolean value of a <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> is
given by the contents it selects.</p>
</dd></dl>

</dd></dl>

<div class="section" id="selectorlist-objects">
<h5>SelectorList objects<a class="headerlink" href="#selectorlist-objects" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.selector.SelectorList">
<em class="property">class </em><tt class="descclassname">scrapy.selector.</tt><tt class="descname">SelectorList</tt><a class="headerlink" href="#scrapy.selector.SelectorList" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><tt class="xref py py-class docutils literal"><span class="pre">SelectorList</span></tt></a> class is subclass of the builtin <tt class="docutils literal"><span class="pre">list</span></tt>
class, which provides a few additional methods.</p>
<dl class="method">
<dt id="scrapy.selector.SelectorList.xpath">
<tt class="descname">xpath</tt><big>(</big><em>query</em><big>)</big><a class="headerlink" href="#scrapy.selector.SelectorList.xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <tt class="docutils literal"><span class="pre">.xpath()</span></tt> method for each element in this list and return
their results flattened as another <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><tt class="xref py py-class docutils literal"><span class="pre">SelectorList</span></tt></a>.</p>
<p><tt class="docutils literal"><span class="pre">query</span></tt> is the same argument as the one in <a class="reference internal" href="index.html#scrapy.selector.Selector.xpath" title="scrapy.selector.Selector.xpath"><tt class="xref py py-meth docutils literal"><span class="pre">Selector.xpath()</span></tt></a></p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.css">
<tt class="descname">css</tt><big>(</big><em>query</em><big>)</big><a class="headerlink" href="#scrapy.selector.SelectorList.css" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <tt class="docutils literal"><span class="pre">.css()</span></tt> method for each element in this list and return
their results flattened as another <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><tt class="xref py py-class docutils literal"><span class="pre">SelectorList</span></tt></a>.</p>
<p><tt class="docutils literal"><span class="pre">query</span></tt> is the same argument as the one in <a class="reference internal" href="index.html#scrapy.selector.Selector.css" title="scrapy.selector.Selector.css"><tt class="xref py py-meth docutils literal"><span class="pre">Selector.css()</span></tt></a></p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.extract">
<tt class="descname">extract</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.selector.SelectorList.extract" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <tt class="docutils literal"><span class="pre">.extract()</span></tt> method for each element is this list and return
their results flattened, as a list of unicode strings.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.re">
<tt class="descname">re</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.selector.SelectorList.re" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <tt class="docutils literal"><span class="pre">.re()</span></tt> method for each element is this list and return
their results flattened, as a list of unicode strings.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.__nonzero__">
<tt class="descname">__nonzero__</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.selector.SelectorList.__nonzero__" title="Permalink to this definition">¶</a></dt>
<dd><p>returns True if the list is not empty, False otherwise.</p>
</dd></dl>

</dd></dl>

<div class="section" id="selector-examples-on-html-response">
<h6>Selector examples on HTML response<a class="headerlink" href="#selector-examples-on-html-response" title="Permalink to this headline">¶</a></h6>
<p>Here&#8217;s a couple of <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> examples to illustrate several concepts.
In all cases, we assume there is already an <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> instantiated with
a <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></tt></a> object like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">html_response</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic">
<li><p class="first">Select all <tt class="docutils literal"><span class="pre">&lt;h1&gt;</span></tt> elements from a HTML response body, returning a list of
<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> objects (ie. a <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><tt class="xref py py-class docutils literal"><span class="pre">SelectorList</span></tt></a> object):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//h1&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="first">Extract the text of all <tt class="docutils literal"><span class="pre">&lt;h1&gt;</span></tt> elements from a HTML response body,
returning a list of unicode strings:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//h1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>         <span class="c"># this includes the h1 tag</span>
<span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//h1/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>  <span class="c"># this excludes the h1 tag</span>
</pre></div>
</div>
</li>
<li><p class="first">Iterate over all <tt class="docutils literal"><span class="pre">&lt;p&gt;</span></tt> tags and print their class attribute:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//p&quot;</span><span class="p">):</span>
    <span class="k">print</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;@class&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="selector-examples-on-xml-response">
<h6>Selector examples on XML response<a class="headerlink" href="#selector-examples-on-xml-response" title="Permalink to this headline">¶</a></h6>
<p>Here&#8217;s a couple of examples to illustrate several concepts. In both cases we
assume there is already an <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> instantiated with a
<a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></tt></a> object like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">xml_response</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic">
<li><p class="first">Select all <tt class="docutils literal"><span class="pre">&lt;product&gt;</span></tt> elements from a XML response body, returning a list
of <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> objects (ie. a <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><tt class="xref py py-class docutils literal"><span class="pre">SelectorList</span></tt></a> object):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//product&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="first">Extract all prices from a <a class="reference external" href="https://support.google.com/merchants/answer/160589?hl=en&amp;ref_topic=2473799">Google Base XML feed</a> which requires registering
a namespace:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">sel</span><span class="o">.</span><span class="n">register_namespace</span><span class="p">(</span><span class="s">&quot;g&quot;</span><span class="p">,</span> <span class="s">&quot;http://base.google.com/ns/1.0&quot;</span><span class="p">)</span>
<span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//g:price&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="removing-namespaces">
<span id="id3"></span><h6>Removing namespaces<a class="headerlink" href="#removing-namespaces" title="Permalink to this headline">¶</a></h6>
<p>When dealing with scraping projects, it is often quite convenient to get rid of
namespaces altogether and just work with element names, to write more
simple/convenient XPaths. You can use the
<a class="reference internal" href="index.html#scrapy.selector.Selector.remove_namespaces" title="scrapy.selector.Selector.remove_namespaces"><tt class="xref py py-meth docutils literal"><span class="pre">Selector.remove_namespaces()</span></tt></a> method for that.</p>
<p>Let&#8217;s show an example that illustrates this with Github blog atom feed.</p>
<p>First, we open the shell with the url we want to scrape:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scrapy shell https://github.com/blog.atom
</pre></div>
</div>
<p>Once in the shell we can try selecting all <tt class="docutils literal"><span class="pre">&lt;link&gt;</span></tt> objects and see that it
doesn&#8217;t work (because the Atom XML namespace is obfuscating those nodes):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//link&quot;</span><span class="p">)</span>
<span class="go">[]</span>
</pre></div>
</div>
<p>But once we call the <a class="reference internal" href="index.html#scrapy.selector.Selector.remove_namespaces" title="scrapy.selector.Selector.remove_namespaces"><tt class="xref py py-meth docutils literal"><span class="pre">Selector.remove_namespaces()</span></tt></a> method, all
nodes can be accessed directly by their names:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">selector</span><span class="o">.</span><span class="n">remove_namespaces</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//link&quot;</span><span class="p">)</span>
<span class="go">[&lt;Selector xpath=&#39;//link&#39; data=u&#39;&lt;link xmlns=&quot;http://www.w3.org/2005/Atom&#39;&gt;,</span>
<span class="go"> &lt;Selector xpath=&#39;//link&#39; data=u&#39;&lt;link xmlns=&quot;http://www.w3.org/2005/Atom&#39;&gt;,</span>
<span class="go"> ...</span>
</pre></div>
</div>
<p>If you wonder why the namespace removal procedure is not always called, instead
of having to call it manually. This is because of two reasons which, in order
of relevance, are:</p>
<ol class="arabic simple">
<li>Removing namespaces requires to iterate and modify all nodes in the
document, which is a reasonably expensive operation to performs for all
documents crawled by Scrapy</li>
<li>There could be some cases where using namespaces is actually required, in
case some element names clash between namespaces. These cases are very rare
though.</li>
</ol>
</div>
</div>
</div>
</div>
<span id="document-topics/loaders"></span><div class="section" id="module-scrapy.contrib.loader">
<span id="item-loaders"></span><span id="topics-loaders"></span><h3>Item Loaders<a class="headerlink" href="#module-scrapy.contrib.loader" title="Permalink to this headline">¶</a></h3>
<p>Item Loaders provide a convenient mechanism for populating scraped <a class="reference internal" href="index.html#topics-items"><em>Items</em></a>. Even though Items can be populated using their own
dictionary-like API, the Item Loaders provide a much more convenient API for
populating them from a scraping process, by automating some common tasks like
parsing the raw extracted data before assigning it.</p>
<p>In other words, <a class="reference internal" href="index.html#topics-items"><em>Items</em></a> provide the <em>container</em> of
scraped data, while Item Loaders provide the mechanism for <em>populating</em> that
container.</p>
<p>Item Loaders are designed to provide a flexible, efficient and easy mechanism
for extending and overriding different field parsing rules, either by spider,
or by source format (HTML, XML, etc) without becoming a nightmare to maintain.</p>
<div class="section" id="using-item-loaders-to-populate-items">
<h4>Using Item Loaders to populate items<a class="headerlink" href="#using-item-loaders-to-populate-items" title="Permalink to this headline">¶</a></h4>
<p>To use an Item Loader, you must first instantiate it. You can either
instantiate it with an dict-like object (e.g. Item or dict) or without one, in
which case an Item is automatically instantiated in the Item Loader constructor
using the Item class specified in the <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.default_item_class" title="scrapy.contrib.loader.ItemLoader.default_item_class"><tt class="xref py py-attr docutils literal"><span class="pre">ItemLoader.default_item_class</span></tt></a>
attribute.</p>
<p>Then, you start collecting values into the Item Loader, typically using
<a class="reference internal" href="index.html#topics-selectors"><em>Selectors</em></a>. You can add more than one value to
the same item field; the Item Loader will know how to &#8220;join&#8221; those values later
using a proper processing function.</p>
<p>Here is a typical Item Loader usage in a <a class="reference internal" href="index.html#topics-spiders"><em>Spider</em></a>, using
the <a class="reference internal" href="index.html#topics-items-declaring"><em>Product item</em></a> declared in the <a class="reference internal" href="index.html#topics-items"><em>Items
chapter</em></a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.loader</span> <span class="kn">import</span> <span class="n">ItemLoader</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">Product</span>

<span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">Product</span><span class="p">(),</span> <span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;//div[@class=&quot;product_name&quot;]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;//div[@class=&quot;product_title&quot;]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">&#39;price&#39;</span><span class="p">,</span> <span class="s">&#39;//p[@id=&quot;price&quot;]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s">&#39;stock&#39;</span><span class="p">,</span> <span class="s">&#39;p#stock]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">&#39;last_updated&#39;</span><span class="p">,</span> <span class="s">&#39;today&#39;</span><span class="p">)</span> <span class="c"># you can also use literal values</span>
    <span class="k">return</span> <span class="n">l</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span>
</pre></div>
</div>
<p>By quickly looking at that code, we can see the <tt class="docutils literal"><span class="pre">name</span></tt> field is being
extracted from two different XPath locations in the page:</p>
<ol class="arabic simple">
<li><tt class="docutils literal"><span class="pre">//div[&#64;class=&quot;product_name&quot;]</span></tt></li>
<li><tt class="docutils literal"><span class="pre">//div[&#64;class=&quot;product_title&quot;]</span></tt></li>
</ol>
<p>In other words, data is being collected by extracting it from two XPath
locations, using the <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_xpath" title="scrapy.contrib.loader.ItemLoader.add_xpath"><tt class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></tt></a> method. This is the
data that will be assigned to the <tt class="docutils literal"><span class="pre">name</span></tt> field later.</p>
<p>Afterwords, similar calls are used for <tt class="docutils literal"><span class="pre">price</span></tt> and <tt class="docutils literal"><span class="pre">stock</span></tt> fields
(the later using a CSS selector with the <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_css" title="scrapy.contrib.loader.ItemLoader.add_css"><tt class="xref py py-meth docutils literal"><span class="pre">add_css()</span></tt></a> method),
and finally the <tt class="docutils literal"><span class="pre">last_update</span></tt> field is populated directly with a literal value
(<tt class="docutils literal"><span class="pre">today</span></tt>) using a different method: <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_value" title="scrapy.contrib.loader.ItemLoader.add_value"><tt class="xref py py-meth docutils literal"><span class="pre">add_value()</span></tt></a>.</p>
<p>Finally, when all data is collected, the <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.load_item" title="scrapy.contrib.loader.ItemLoader.load_item"><tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.load_item()</span></tt></a> method is
called which actually populates and returns the item populated with the data
previously extracted and collected with the <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_xpath" title="scrapy.contrib.loader.ItemLoader.add_xpath"><tt class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></tt></a>,
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_css" title="scrapy.contrib.loader.ItemLoader.add_css"><tt class="xref py py-meth docutils literal"><span class="pre">add_css()</span></tt></a>, and <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_value" title="scrapy.contrib.loader.ItemLoader.add_value"><tt class="xref py py-meth docutils literal"><span class="pre">add_value()</span></tt></a> calls.</p>
</div>
<div class="section" id="input-and-output-processors">
<span id="topics-loaders-processors"></span><h4>Input and Output processors<a class="headerlink" href="#input-and-output-processors" title="Permalink to this headline">¶</a></h4>
<p>An Item Loader contains one input processor and one output processor for each
(item) field. The input processor processes the extracted data as soon as it&#8217;s
received (through the <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_xpath" title="scrapy.contrib.loader.ItemLoader.add_xpath"><tt class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></tt></a>, <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_css" title="scrapy.contrib.loader.ItemLoader.add_css"><tt class="xref py py-meth docutils literal"><span class="pre">add_css()</span></tt></a> or
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_value" title="scrapy.contrib.loader.ItemLoader.add_value"><tt class="xref py py-meth docutils literal"><span class="pre">add_value()</span></tt></a> methods) and the result of the input processor is
collected and kept inside the ItemLoader. After collecting all data, the
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.load_item" title="scrapy.contrib.loader.ItemLoader.load_item"><tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.load_item()</span></tt></a> method is called to populate and get the populated
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> object.  That&#8217;s when the output processor is
called with the data previously collected (and processed using the input
processor). The result of the output processor is the final value that gets
assigned to the item.</p>
<p>Let&#8217;s see an example to illustrate how the input and output processors are
called for a particular field (the same applies for any other field):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">l</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">Product</span><span class="p">(),</span> <span class="n">some_selector</span><span class="p">)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="n">xpath1</span><span class="p">)</span> <span class="c"># (1)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="n">xpath2</span><span class="p">)</span> <span class="c"># (2)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="n">css</span><span class="p">)</span> <span class="c"># (3)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;test&#39;</span><span class="p">)</span> <span class="c"># (4)</span>
<span class="k">return</span> <span class="n">l</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span> <span class="c"># (5)</span>
</pre></div>
</div>
<p>So what happens is:</p>
<ol class="arabic simple">
<li>Data from <tt class="docutils literal"><span class="pre">xpath1</span></tt> is extracted, and passed through the <em>input processor</em> of
the <tt class="docutils literal"><span class="pre">name</span></tt> field. The result of the input processor is collected and kept in
the Item Loader (but not yet assigned to the item).</li>
<li>Data from <tt class="docutils literal"><span class="pre">xpath2</span></tt> is extracted, and passed through the same <em>input
processor</em> used in (1). The result of the input processor is appended to the
data collected in (1) (if any).</li>
<li>This case is similar to the previous ones, except that the data is extracted
from the <tt class="docutils literal"><span class="pre">css</span></tt> CSS selector, and passed through the same <em>input
processor</em> used in (1) and (2). The result of the input processor is appended to the
data collected in (1) and (2) (if any).</li>
<li>This case is also similar to the previous ones, except that the value to be
collected is assigned directly, instead of being extracted from a XPath
expression or a CSS selector.
However, the value is still passed through the input processors. In this
case, since the value is not iterable it is converted to an iterable of a
single element before passing it to the input processor, because input
processor always receive iterables.</li>
<li>The data collected in steps (1), (2), (3) and (4) is passed through
the <em>output processor</em> of the <tt class="docutils literal"><span class="pre">name</span></tt> field.
The result of the output processor is the value assigned to the <tt class="docutils literal"><span class="pre">name</span></tt>
field in the item.</li>
</ol>
<p>It&#8217;s worth noticing that processors are just callable objects, which are called
with the data to be parsed, and return a parsed value. So you can use any
function as input or output processor. The only requirement is that they must
accept one (and only one) positional argument, which will be an iterator.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Both input and output processors must receive an iterator as their
first argument. The output of those functions can be anything. The result of
input processors will be appended to an internal list (in the Loader)
containing the collected values (for that field). The result of the output
processors is the value that will be finally assigned to the item.</p>
</div>
<p>The other thing you need to keep in mind is that the values returned by input
processors are collected internally (in lists) and then passed to output
processors to populate the fields.</p>
<p>Last, but not least, Scrapy comes with some <a class="reference internal" href="index.html#topics-loaders-available-processors"><em>commonly used processors</em></a> built-in for convenience.</p>
</div>
<div class="section" id="declaring-item-loaders">
<h4>Declaring Item Loaders<a class="headerlink" href="#declaring-item-loaders" title="Permalink to this headline">¶</a></h4>
<p>Item Loaders are declared like Items, by using a class definition syntax. Here
is an example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.loader</span> <span class="kn">import</span> <span class="n">ItemLoader</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">TakeFirst</span><span class="p">,</span> <span class="n">MapCompose</span><span class="p">,</span> <span class="n">Join</span>

<span class="k">class</span> <span class="nc">ProductLoader</span><span class="p">(</span><span class="n">ItemLoader</span><span class="p">):</span>

    <span class="n">default_output_processor</span> <span class="o">=</span> <span class="n">TakeFirst</span><span class="p">()</span>

    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="nb">unicode</span><span class="o">.</span><span class="n">title</span><span class="p">)</span>
    <span class="n">name_out</span> <span class="o">=</span> <span class="n">Join</span><span class="p">()</span>

    <span class="n">price_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="nb">unicode</span><span class="o">.</span><span class="n">strip</span><span class="p">)</span>

    <span class="c"># ...</span>
</pre></div>
</div>
<p>As you can see, input processors are declared using the <tt class="docutils literal"><span class="pre">_in</span></tt> suffix while
output processors are declared using the <tt class="docutils literal"><span class="pre">_out</span></tt> suffix. And you can also
declare a default input/output processors using the
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.default_input_processor" title="scrapy.contrib.loader.ItemLoader.default_input_processor"><tt class="xref py py-attr docutils literal"><span class="pre">ItemLoader.default_input_processor</span></tt></a> and
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.default_output_processor" title="scrapy.contrib.loader.ItemLoader.default_output_processor"><tt class="xref py py-attr docutils literal"><span class="pre">ItemLoader.default_output_processor</span></tt></a> attributes.</p>
</div>
<div class="section" id="declaring-input-and-output-processors">
<span id="topics-loaders-processors-declaring"></span><h4>Declaring Input and Output Processors<a class="headerlink" href="#declaring-input-and-output-processors" title="Permalink to this headline">¶</a></h4>
<p>As seen in the previous section, input and output processors can be declared in
the Item Loader definition, and it&#8217;s very common to declare input processors
this way. However, there is one more place where you can specify the input and
output processors to use: in the <a class="reference internal" href="index.html#topics-items-fields"><em>Item Field</em></a>
metadata. Here is an example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">MapCompose</span><span class="p">,</span> <span class="n">Join</span><span class="p">,</span> <span class="n">TakeFirst</span>
<span class="kn">from</span> <span class="nn">w3lib.html</span> <span class="kn">import</span> <span class="n">remove_entities</span>
<span class="kn">from</span> <span class="nn">myproject.utils</span> <span class="kn">import</span> <span class="n">filter_prices</span>

<span class="k">class</span> <span class="nc">Product</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span>
        <span class="n">input_processor</span><span class="o">=</span><span class="n">MapCompose</span><span class="p">(</span><span class="n">remove_entities</span><span class="p">),</span>
        <span class="n">output_processor</span><span class="o">=</span><span class="n">Join</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">input_processor</span><span class="o">=</span><span class="n">MapCompose</span><span class="p">(</span><span class="n">remove_entities</span><span class="p">,</span> <span class="n">filter_prices</span><span class="p">),</span>
        <span class="n">output_processor</span><span class="o">=</span><span class="n">TakeFirst</span><span class="p">(),</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>The precedence order, for both input and output processors, is as follows:</p>
<ol class="arabic simple">
<li>Item Loader field-specific attributes: <tt class="docutils literal"><span class="pre">field_in</span></tt> and <tt class="docutils literal"><span class="pre">field_out</span></tt> (most
precedence)</li>
<li>Field metadata (<tt class="docutils literal"><span class="pre">input_processor</span></tt> and <tt class="docutils literal"><span class="pre">output_processor</span></tt> key)</li>
<li>Item Loader defaults: <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.default_input_processor" title="scrapy.contrib.loader.ItemLoader.default_input_processor"><tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.default_input_processor()</span></tt></a> and
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.default_output_processor" title="scrapy.contrib.loader.ItemLoader.default_output_processor"><tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.default_output_processor()</span></tt></a> (least precedence)</li>
</ol>
<p>See also: <a class="reference internal" href="index.html#topics-loaders-extending"><em>Reusing and extending Item Loaders</em></a>.</p>
</div>
<div class="section" id="item-loader-context">
<span id="topics-loaders-context"></span><h4>Item Loader Context<a class="headerlink" href="#item-loader-context" title="Permalink to this headline">¶</a></h4>
<p>The Item Loader Context is a dict of arbitrary key/values which is shared among
all input and output processors in the Item Loader. It can be passed when
declaring, instantiating or using Item Loader. They are used to modify the
behaviour of the input/output processors.</p>
<p>For example, suppose you have a function <tt class="docutils literal"><span class="pre">parse_length</span></tt> which receives a text
value and extracts a length from it:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">parse_length</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">loader_context</span><span class="p">):</span>
    <span class="n">unit</span> <span class="o">=</span> <span class="n">loader_context</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;unit&#39;</span><span class="p">,</span> <span class="s">&#39;m&#39;</span><span class="p">)</span>
    <span class="c"># ... length parsing code goes here ...</span>
    <span class="k">return</span> <span class="n">parsed_length</span>
</pre></div>
</div>
<p>By accepting a <tt class="docutils literal"><span class="pre">loader_context</span></tt> argument the function is explicitly telling
the Item Loader that it&#8217;s able to receive an Item Loader context, so the Item
Loader passes the currently active context when calling it, and the processor
function (<tt class="docutils literal"><span class="pre">parse_length</span></tt> in this case) can thus use them.</p>
<p>There are several ways to modify Item Loader context values:</p>
<ol class="arabic">
<li><p class="first">By modifying the currently active Item Loader context
(<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.context" title="scrapy.contrib.loader.ItemLoader.context"><tt class="xref py py-attr docutils literal"><span class="pre">context</span></tt></a> attribute):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">product</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">context</span><span class="p">[</span><span class="s">&#39;unit&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s">&#39;cm&#39;</span>
</pre></div>
</div>
</li>
<li><p class="first">On Item Loader instantiation (the keyword arguments of Item Loader
constructor are stored in the Item Loader context):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">product</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s">&#39;cm&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="first">On Item Loader declaration, for those input/output processors that support
instantiating them with an Item Loader context. <a class="reference internal" href="index.html#scrapy.contrib.loader.processor.MapCompose" title="scrapy.contrib.loader.processor.MapCompose"><tt class="xref py py-class docutils literal"><span class="pre">MapCompose</span></tt></a> is one of
them:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">ProductLoader</span><span class="p">(</span><span class="n">ItemLoader</span><span class="p">):</span>
    <span class="n">length_out</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">parse_length</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s">&#39;cm&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="itemloader-objects">
<h4>ItemLoader objects<a class="headerlink" href="#itemloader-objects" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.contrib.loader.ItemLoader">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.loader.</tt><tt class="descname">ItemLoader</tt><big>(</big><span class="optional">[</span><em>item</em>, <em>selector</em>, <em>response</em>, <span class="optional">]</span><em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new Item Loader for populating the given Item. If no item is
given, one is instantiated automatically using the class in
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.default_item_class" title="scrapy.contrib.loader.ItemLoader.default_item_class"><tt class="xref py py-attr docutils literal"><span class="pre">default_item_class</span></tt></a>.</p>
<p>When instantiated with a <cite>selector</cite> or a <cite>response</cite> parameters
the <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader" title="scrapy.contrib.loader.ItemLoader"><tt class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></tt></a> class provides convenient mechanisms for extracting
data from web pages using <a class="reference internal" href="index.html#topics-selectors"><em>selectors</em></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> object) &#8211; The item instance to populate using subsequent calls to
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_xpath" title="scrapy.contrib.loader.ItemLoader.add_xpath"><tt class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></tt></a>, <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_css" title="scrapy.contrib.loader.ItemLoader.add_css"><tt class="xref py py-meth docutils literal"><span class="pre">add_css()</span></tt></a>,
or <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_value" title="scrapy.contrib.loader.ItemLoader.add_value"><tt class="xref py py-meth docutils literal"><span class="pre">add_value()</span></tt></a>.</li>
<li><strong>selector</strong> (<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> object) &#8211; The selector to extract data from, when using the
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_xpath" title="scrapy.contrib.loader.ItemLoader.add_xpath"><tt class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></tt></a> (resp. <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_css" title="scrapy.contrib.loader.ItemLoader.add_css"><tt class="xref py py-meth docutils literal"><span class="pre">add_css()</span></tt></a>) or <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.replace_xpath" title="scrapy.contrib.loader.ItemLoader.replace_xpath"><tt class="xref py py-meth docutils literal"><span class="pre">replace_xpath()</span></tt></a>
(resp. <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.replace_css" title="scrapy.contrib.loader.ItemLoader.replace_css"><tt class="xref py py-meth docutils literal"><span class="pre">replace_css()</span></tt></a>) method.</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object) &#8211; The response used to construct the selector using the
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.default_selector_class" title="scrapy.contrib.loader.ItemLoader.default_selector_class"><tt class="xref py py-attr docutils literal"><span class="pre">default_selector_class</span></tt></a>, unless the selector argument is given,
in which case this argument is ignored.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The item, selector, response and the remaining keyword arguments are
assigned to the Loader context (accessible through the <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.context" title="scrapy.contrib.loader.ItemLoader.context"><tt class="xref py py-attr docutils literal"><span class="pre">context</span></tt></a> attribute).</p>
<p><a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader" title="scrapy.contrib.loader.ItemLoader"><tt class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></tt></a> instances have the following methods:</p>
<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.get_value">
<tt class="descname">get_value</tt><big>(</big><em>value</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.get_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Process the given <tt class="docutils literal"><span class="pre">value</span></tt> by the given <tt class="docutils literal"><span class="pre">processors</span></tt> and keyword
arguments.</p>
<p>Available keyword arguments:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>re</strong> (<em>str or compiled regex</em>) &#8211; a regular expression to use for extracting data from the
given value using <tt class="xref py py-meth docutils literal"><span class="pre">extract_regex()</span></tt> method,
applied before processors</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">TakeFirst</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loader</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="s">u&#39;name: foo&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="nb">unicode</span><span class="o">.</span><span class="n">upper</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s">&#39;name: (.+)&#39;</span><span class="p">)</span>
<span class="go">&#39;FOO`</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.add_value">
<tt class="descname">add_value</tt><big>(</big><em>field_name</em>, <em>value</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.add_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Process and then add the given <tt class="docutils literal"><span class="pre">value</span></tt> for the given field.</p>
<p>The value is first passed through <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.get_value" title="scrapy.contrib.loader.ItemLoader.get_value"><tt class="xref py py-meth docutils literal"><span class="pre">get_value()</span></tt></a> by giving the
<tt class="docutils literal"><span class="pre">processors</span></tt> and <tt class="docutils literal"><span class="pre">kwargs</span></tt>, and then passed through the
<a class="reference internal" href="index.html#topics-loaders-processors"><em>field input processor</em></a> and its result
appended to the data collected for that field. If the field already
contains collected data, the new data is added.</p>
<p>The given <tt class="docutils literal"><span class="pre">field_name</span></tt> can be <tt class="docutils literal"><span class="pre">None</span></tt>, in which case values for
multiple fields may be added. And the processed value should be a dict
with field_name mapped to values.</p>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">u&#39;Color TV&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">&#39;colours&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s">u&#39;white&#39;</span><span class="p">,</span> <span class="s">u&#39;blue&#39;</span><span class="p">])</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">&#39;length&#39;</span><span class="p">,</span> <span class="s">u&#39;100&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">u&#39;name: foo&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s">&#39;name: (.+)&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="p">{</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">u&#39;foo&#39;</span><span class="p">,</span> <span class="s">&#39;sex&#39;</span><span class="p">:</span> <span class="s">u&#39;male&#39;</span><span class="p">})</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.replace_value">
<tt class="descname">replace_value</tt><big>(</big><em>field_name</em>, <em>value</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.replace_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_value" title="scrapy.contrib.loader.ItemLoader.add_value"><tt class="xref py py-meth docutils literal"><span class="pre">add_value()</span></tt></a> but replaces the collected data with the
new value instead of adding it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.get_xpath">
<tt class="descname">get_xpath</tt><big>(</big><em>xpath</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.get_xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.get_value" title="scrapy.contrib.loader.ItemLoader.get_value"><tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.get_value()</span></tt></a> but receives an XPath instead of a
value, which is used to extract a list of unicode strings from the
selector associated with this <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader" title="scrapy.contrib.loader.ItemLoader"><tt class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>xpath</strong> (<em>str</em>) &#8211; the XPath to extract data from</li>
<li><strong>re</strong> (<em>str or compiled regex</em>) &#8211; a regular expression to use for extracting data from the
selected XPath region</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_xpath</span><span class="p">(</span><span class="s">&#39;//p[@class=&quot;product-name&quot;]&#39;</span><span class="p">)</span>
<span class="c"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_xpath</span><span class="p">(</span><span class="s">&#39;//p[@id=&quot;price&quot;]&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.add_xpath">
<tt class="descname">add_xpath</tt><big>(</big><em>field_name</em>, <em>xpath</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.add_xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_value" title="scrapy.contrib.loader.ItemLoader.add_value"><tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.add_value()</span></tt></a> but receives an XPath instead of a
value, which is used to extract a list of unicode strings from the
selector associated with this <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader" title="scrapy.contrib.loader.ItemLoader"><tt class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></tt></a>.</p>
<p>See <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.get_xpath" title="scrapy.contrib.loader.ItemLoader.get_xpath"><tt class="xref py py-meth docutils literal"><span class="pre">get_xpath()</span></tt></a> for <tt class="docutils literal"><span class="pre">kwargs</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>xpath</strong> (<em>str</em>) &#8211; the XPath to extract data from</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;//p[@class=&quot;product-name&quot;]&#39;</span><span class="p">)</span>
<span class="c"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">&#39;price&#39;</span><span class="p">,</span> <span class="s">&#39;//p[@id=&quot;price&quot;]&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.replace_xpath">
<tt class="descname">replace_xpath</tt><big>(</big><em>field_name</em>, <em>xpath</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.replace_xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_xpath" title="scrapy.contrib.loader.ItemLoader.add_xpath"><tt class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></tt></a> but replaces collected data instead of
adding it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.get_css">
<tt class="descname">get_css</tt><big>(</big><em>css</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.get_css" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.get_value" title="scrapy.contrib.loader.ItemLoader.get_value"><tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.get_value()</span></tt></a> but receives a CSS selector
instead of a value, which is used to extract a list of unicode strings
from the selector associated with this <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader" title="scrapy.contrib.loader.ItemLoader"><tt class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>css</strong> (<em>str</em>) &#8211; the CSS selector to extract data from</li>
<li><strong>re</strong> (<em>str or compiled regex</em>) &#8211; a regular expression to use for extracting data from the
selected CSS region</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_css</span><span class="p">(</span><span class="s">&#39;p.product-name&#39;</span><span class="p">)</span>
<span class="c"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_css</span><span class="p">(</span><span class="s">&#39;p#price&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.add_css">
<tt class="descname">add_css</tt><big>(</big><em>field_name</em>, <em>css</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.add_css" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_value" title="scrapy.contrib.loader.ItemLoader.add_value"><tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.add_value()</span></tt></a> but receives a CSS selector
instead of a value, which is used to extract a list of unicode strings
from the selector associated with this <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader" title="scrapy.contrib.loader.ItemLoader"><tt class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></tt></a>.</p>
<p>See <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.get_css" title="scrapy.contrib.loader.ItemLoader.get_css"><tt class="xref py py-meth docutils literal"><span class="pre">get_css()</span></tt></a> for <tt class="docutils literal"><span class="pre">kwargs</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>css</strong> (<em>str</em>) &#8211; the CSS selector to extract data from</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;p.product-name&#39;</span><span class="p">)</span>
<span class="c"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s">&#39;price&#39;</span><span class="p">,</span> <span class="s">&#39;p#price&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.replace_css">
<tt class="descname">replace_css</tt><big>(</big><em>field_name</em>, <em>css</em>, <em>*processors</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.replace_css" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.add_css" title="scrapy.contrib.loader.ItemLoader.add_css"><tt class="xref py py-meth docutils literal"><span class="pre">add_css()</span></tt></a> but replaces collected data instead of
adding it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.load_item">
<tt class="descname">load_item</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.load_item" title="Permalink to this definition">¶</a></dt>
<dd><p>Populate the item with the data collected so far, and return it. The
data collected is first passed through the <a class="reference internal" href="index.html#topics-loaders-processors"><em>output processors</em></a> to get the final value to assign to each
item field.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.get_collected_values">
<tt class="descname">get_collected_values</tt><big>(</big><em>field_name</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.get_collected_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the collected values for the given field.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.get_output_value">
<tt class="descname">get_output_value</tt><big>(</big><em>field_name</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.get_output_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the collected values parsed using the output processor, for the
given field. This method doesn&#8217;t populate or modify the item at all.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.get_input_processor">
<tt class="descname">get_input_processor</tt><big>(</big><em>field_name</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.get_input_processor" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the input processor for the given field.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.loader.ItemLoader.get_output_processor">
<tt class="descname">get_output_processor</tt><big>(</big><em>field_name</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.get_output_processor" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the output processor for the given field.</p>
</dd></dl>

<p><a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader" title="scrapy.contrib.loader.ItemLoader"><tt class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></tt></a> instances have the following attributes:</p>
<dl class="attribute">
<dt id="scrapy.contrib.loader.ItemLoader.item">
<tt class="descname">item</tt><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.item" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> object being parsed by this Item Loader.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.loader.ItemLoader.context">
<tt class="descname">context</tt><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.context" title="Permalink to this definition">¶</a></dt>
<dd><p>The currently active <a class="reference internal" href="index.html#topics-loaders-context"><em>Context</em></a> of this
Item Loader.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.loader.ItemLoader.default_item_class">
<tt class="descname">default_item_class</tt><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.default_item_class" title="Permalink to this definition">¶</a></dt>
<dd><p>An Item class (or factory), used to instantiate items when not given in
the constructor.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.loader.ItemLoader.default_input_processor">
<tt class="descname">default_input_processor</tt><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.default_input_processor" title="Permalink to this definition">¶</a></dt>
<dd><p>The default input processor to use for those fields which don&#8217;t specify
one.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.loader.ItemLoader.default_output_processor">
<tt class="descname">default_output_processor</tt><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.default_output_processor" title="Permalink to this definition">¶</a></dt>
<dd><p>The default output processor to use for those fields which don&#8217;t specify
one.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.loader.ItemLoader.default_selector_class">
<tt class="descname">default_selector_class</tt><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.default_selector_class" title="Permalink to this definition">¶</a></dt>
<dd><p>The class used to construct the <a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.selector" title="scrapy.contrib.loader.ItemLoader.selector"><tt class="xref py py-attr docutils literal"><span class="pre">selector</span></tt></a> of this
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader" title="scrapy.contrib.loader.ItemLoader"><tt class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></tt></a>, if only a response is given in the constructor.
If a selector is given in the constructor this attribute is ignored.
This attribute is sometimes overridden in subclasses.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.loader.ItemLoader.selector">
<tt class="descname">selector</tt><a class="headerlink" href="#scrapy.contrib.loader.ItemLoader.selector" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> object to extract data from.
It&#8217;s either the selector given in the constructor or one created from
the response given in the constructor using the
<a class="reference internal" href="index.html#scrapy.contrib.loader.ItemLoader.default_selector_class" title="scrapy.contrib.loader.ItemLoader.default_selector_class"><tt class="xref py py-attr docutils literal"><span class="pre">default_selector_class</span></tt></a>. This attribute is meant to be
read-only.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="reusing-and-extending-item-loaders">
<span id="topics-loaders-extending"></span><h4>Reusing and extending Item Loaders<a class="headerlink" href="#reusing-and-extending-item-loaders" title="Permalink to this headline">¶</a></h4>
<p>As your project grows bigger and acquires more and more spiders, maintenance
becomes a fundamental problem, especially when you have to deal with many
different parsing rules for each spider, having a lot of exceptions, but also
wanting to reuse the common processors.</p>
<p>Item Loaders are designed to ease the maintenance burden of parsing rules,
without losing flexibility and, at the same time, providing a convenient
mechanism for extending and overriding them. For this reason Item Loaders
support traditional Python class inheritance for dealing with differences of
specific spiders (or groups of spiders).</p>
<p>Suppose, for example, that some particular site encloses their product names in
three dashes (e.g. <tt class="docutils literal"><span class="pre">---Plasma</span> <span class="pre">TV---</span></tt>) and you don&#8217;t want to end up scraping
those dashes in the final product names.</p>
<p>Here&#8217;s how you can remove those dashes by reusing and extending the default
Product Item Loader (<tt class="docutils literal"><span class="pre">ProductLoader</span></tt>):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">MapCompose</span>
<span class="kn">from</span> <span class="nn">myproject.ItemLoaders</span> <span class="kn">import</span> <span class="n">ProductLoader</span>

<span class="k">def</span> <span class="nf">strip_dashes</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s">&#39;-&#39;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">SiteSpecificLoader</span><span class="p">(</span><span class="n">ProductLoader</span><span class="p">):</span>
    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">strip_dashes</span><span class="p">,</span> <span class="n">ProductLoader</span><span class="o">.</span><span class="n">name_in</span><span class="p">)</span>
</pre></div>
</div>
<p>Another case where extending Item Loaders can be very helpful is when you have
multiple source formats, for example XML and HTML. In the XML version you may
want to remove <tt class="docutils literal"><span class="pre">CDATA</span></tt> occurrences. Here&#8217;s an example of how to do it:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">MapCompose</span>
<span class="kn">from</span> <span class="nn">myproject.ItemLoaders</span> <span class="kn">import</span> <span class="n">ProductLoader</span>
<span class="kn">from</span> <span class="nn">myproject.utils.xml</span> <span class="kn">import</span> <span class="n">remove_cdata</span>

<span class="k">class</span> <span class="nc">XmlProductLoader</span><span class="p">(</span><span class="n">ProductLoader</span><span class="p">):</span>
    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">remove_cdata</span><span class="p">,</span> <span class="n">ProductLoader</span><span class="o">.</span><span class="n">name_in</span><span class="p">)</span>
</pre></div>
</div>
<p>And that&#8217;s how you typically extend input processors.</p>
<p>As for output processors, it is more common to declare them in the field metadata,
as they usually depend only on the field and not on each specific site parsing
rule (as input processors do). See also:
<a class="reference internal" href="index.html#topics-loaders-processors-declaring"><em>Declaring Input and Output Processors</em></a>.</p>
<p>There are many other possible ways to extend, inherit and override your Item
Loaders, and different Item Loaders hierarchies may fit better for different
projects. Scrapy only provides the mechanism; it doesn&#8217;t impose any specific
organization of your Loaders collection - that&#8217;s up to you and your project&#8217;s
needs.</p>
</div>
<div class="section" id="module-scrapy.contrib.loader.processor">
<span id="available-built-in-processors"></span><span id="topics-loaders-available-processors"></span><h4>Available built-in processors<a class="headerlink" href="#module-scrapy.contrib.loader.processor" title="Permalink to this headline">¶</a></h4>
<p>Even though you can use any callable function as input and output processors,
Scrapy provides some commonly used processors, which are described below. Some
of them, like the <a class="reference internal" href="index.html#scrapy.contrib.loader.processor.MapCompose" title="scrapy.contrib.loader.processor.MapCompose"><tt class="xref py py-class docutils literal"><span class="pre">MapCompose</span></tt></a> (which is typically used as input
processor) compose the output of several functions executed in order, to
produce the final parsed value.</p>
<p>Here is a list of all built-in processors:</p>
<dl class="class">
<dt id="scrapy.contrib.loader.processor.Identity">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.loader.processor.</tt><tt class="descname">Identity</tt><a class="headerlink" href="#scrapy.contrib.loader.processor.Identity" title="Permalink to this definition">¶</a></dt>
<dd><p>The simplest processor, which doesn&#8217;t do anything. It returns the original
values unchanged. It doesn&#8217;t receive any constructor arguments nor accepts
Loader contexts.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">Identity</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s">&#39;one&#39;</span><span class="p">,</span> <span class="s">&#39;two&#39;</span><span class="p">,</span> <span class="s">&#39;three&#39;</span><span class="p">])</span>
<span class="go">[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.contrib.loader.processor.TakeFirst">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.loader.processor.</tt><tt class="descname">TakeFirst</tt><a class="headerlink" href="#scrapy.contrib.loader.processor.TakeFirst" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first non-null/non-empty value from the values received,
so it&#8217;s typically used as an output processor to single-valued fields.
It doesn&#8217;t receive any constructor arguments, nor accept Loader contexts.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">TakeFirst</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">TakeFirst</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s">&#39;&#39;</span><span class="p">,</span> <span class="s">&#39;one&#39;</span><span class="p">,</span> <span class="s">&#39;two&#39;</span><span class="p">,</span> <span class="s">&#39;three&#39;</span><span class="p">])</span>
<span class="go">&#39;one&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.contrib.loader.processor.Join">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.loader.processor.</tt><tt class="descname">Join</tt><big>(</big><em>separator=u' '</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.processor.Join" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the values joined with the separator given in the constructor, which
defaults to <tt class="docutils literal"><span class="pre">u'</span> <span class="pre">'</span></tt>. It doesn&#8217;t accept Loader contexts.</p>
<p>When using the default separator, this processor is equivalent to the
function: <tt class="docutils literal"><span class="pre">u'</span> <span class="pre">'.join</span></tt></p>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">Join</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">Join</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s">&#39;one&#39;</span><span class="p">,</span> <span class="s">&#39;two&#39;</span><span class="p">,</span> <span class="s">&#39;three&#39;</span><span class="p">])</span>
<span class="go">u&#39;one two three&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">Join</span><span class="p">(</span><span class="s">&#39;&lt;br&gt;&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s">&#39;one&#39;</span><span class="p">,</span> <span class="s">&#39;two&#39;</span><span class="p">,</span> <span class="s">&#39;three&#39;</span><span class="p">])</span>
<span class="go">u&#39;one&lt;br&gt;two&lt;br&gt;three&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.contrib.loader.processor.Compose">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.loader.processor.</tt><tt class="descname">Compose</tt><big>(</big><em>*functions</em>, <em>**default_loader_context</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.processor.Compose" title="Permalink to this definition">¶</a></dt>
<dd><p>A processor which is constructed from the composition of the given
functions. This means that each input value of this processor is passed to
the first function, and the result of that function is passed to the second
function, and so on, until the last function returns the output value of
this processor.</p>
<p>By default, stop process on <tt class="docutils literal"><span class="pre">None</span></tt> value. This behaviour can be changed by
passing keyword argument <tt class="docutils literal"><span class="pre">stop_on_none=False</span></tt>.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">Compose</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="o">.</span><span class="n">upper</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s">&#39;hello&#39;</span><span class="p">,</span> <span class="s">&#39;world&#39;</span><span class="p">])</span>
<span class="go">&#39;HELLO&#39;</span>
</pre></div>
</div>
<p>Each function can optionally receive a <tt class="docutils literal"><span class="pre">loader_context</span></tt> parameter. For
those which do, this processor will pass the currently active <a class="reference internal" href="index.html#topics-loaders-context"><em>Loader
context</em></a> through that parameter.</p>
<p>The keyword arguments passed in the constructor are used as the default
Loader context values passed to each function call. However, the final
Loader context values passed to functions are overridden with the currently
active Loader context accessible through the <tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.context()</span></tt>
attribute.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.contrib.loader.processor.MapCompose">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.loader.processor.</tt><tt class="descname">MapCompose</tt><big>(</big><em>*functions</em>, <em>**default_loader_context</em><big>)</big><a class="headerlink" href="#scrapy.contrib.loader.processor.MapCompose" title="Permalink to this definition">¶</a></dt>
<dd><p>A processor which is constructed from the composition of the given
functions, similar to the <a class="reference internal" href="index.html#scrapy.contrib.loader.processor.Compose" title="scrapy.contrib.loader.processor.Compose"><tt class="xref py py-class docutils literal"><span class="pre">Compose</span></tt></a> processor. The difference with
this processor is the way internal results are passed among functions,
which is as follows:</p>
<p>The input value of this processor is <em>iterated</em> and the first function is
applied to each element. The results of these function calls (one for each element)
are concatenated to construct a new iterable, which is then used to apply the
second function, and so on, until the last function is applied to each
value of the list of values collected so far. The output values of the last
function are concatenated together to produce the output of this processor.</p>
<p>Each particular function can return a value or a list of values, which is
flattened with the list of values returned by the same function applied to
the other input values. The functions can also return <tt class="docutils literal"><span class="pre">None</span></tt> in which
case the output of that function is ignored for further processing over the
chain.</p>
<p>This processor provides a convenient way to compose functions that only
work with single values (instead of iterables). For this reason the
<a class="reference internal" href="index.html#scrapy.contrib.loader.processor.MapCompose" title="scrapy.contrib.loader.processor.MapCompose"><tt class="xref py py-class docutils literal"><span class="pre">MapCompose</span></tt></a> processor is typically used as input processor, since
data is often extracted using the
<a class="reference internal" href="index.html#scrapy.selector.Selector.extract" title="scrapy.selector.Selector.extract"><tt class="xref py py-meth docutils literal"><span class="pre">extract()</span></tt></a> method of <a class="reference internal" href="index.html#topics-selectors"><em>selectors</em></a>, which returns a list of unicode strings.</p>
<p>The example below should clarify how it works:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">filter_world</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="bp">None</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">&#39;world&#39;</span> <span class="k">else</span> <span class="n">x</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.contrib.loader.processor</span> <span class="kn">import</span> <span class="n">MapCompose</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">filter_world</span><span class="p">,</span> <span class="nb">unicode</span><span class="o">.</span><span class="n">upper</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s">u&#39;hello&#39;</span><span class="p">,</span> <span class="s">u&#39;world&#39;</span><span class="p">,</span> <span class="s">u&#39;this&#39;</span><span class="p">,</span> <span class="s">u&#39;is&#39;</span><span class="p">,</span> <span class="s">u&#39;scrapy&#39;</span><span class="p">])</span>
<span class="go">[u&#39;HELLO, u&#39;THIS&#39;, u&#39;IS&#39;, u&#39;SCRAPY&#39;]</span>
</pre></div>
</div>
<p>As with the Compose processor, functions can receive Loader contexts, and
constructor keyword arguments are used as default context values. See
<a class="reference internal" href="index.html#scrapy.contrib.loader.processor.Compose" title="scrapy.contrib.loader.processor.Compose"><tt class="xref py py-class docutils literal"><span class="pre">Compose</span></tt></a> processor for more info.</p>
</dd></dl>

</div>
</div>
<span id="document-topics/shell"></span><div class="section" id="scrapy-shell">
<span id="topics-shell"></span><h3>Scrapy shell<a class="headerlink" href="#scrapy-shell" title="Permalink to this headline">¶</a></h3>
<p>The Scrapy shell is an interactive shell where you can try and debug your
scraping code very quickly, without having to run the spider. It&#8217;s meant to be
used for testing data extraction code, but you can actually use it for testing
any kind of code as it is also a regular Python shell.</p>
<p>The shell is used for testing XPath or CSS expressions and see how they work
and what data they extract from the web pages you&#8217;re trying to scrape. It
allows you to interactively test your expressions while you&#8217;re writing your
spider, without having to run the spider to test every change.</p>
<p>Once you get familiarized with the Scrapy shell, you&#8217;ll see that it&#8217;s an
invaluable tool for developing and debugging your spiders.</p>
<p>If you have <a class="reference external" href="http://ipython.org/">IPython</a> installed, the Scrapy shell will use it (instead of the
standard Python console). The <a class="reference external" href="http://ipython.org/">IPython</a> console is much more powerful and
provides smart auto-completion and colorized output, among other things.</p>
<p>We highly recommend you install <a class="reference external" href="http://ipython.org/">IPython</a>, specially if you&#8217;re working on
Unix systems (where <a class="reference external" href="http://ipython.org/">IPython</a> excels). See the <a class="reference external" href="http://ipython.org/install.html">IPython installation guide</a>
for more info.</p>
<div class="section" id="launch-the-shell">
<h4>Launch the shell<a class="headerlink" href="#launch-the-shell" title="Permalink to this headline">¶</a></h4>
<p>To launch the Scrapy shell you can use the <a class="reference internal" href="index.html#std:command-shell"><tt class="xref std std-command docutils literal"><span class="pre">shell</span></tt></a> command like
this:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy shell &lt;url&gt;
</pre></div>
</div>
<p>Where the <tt class="docutils literal"><span class="pre">&lt;url&gt;</span></tt> is the URL you want to scrape.</p>
</div>
<div class="section" id="using-the-shell">
<h4>Using the shell<a class="headerlink" href="#using-the-shell" title="Permalink to this headline">¶</a></h4>
<p>The Scrapy shell is just a regular Python console (or <a class="reference external" href="http://ipython.org/">IPython</a> console if you
have it available) which provides some additional shortcut functions for
convenience.</p>
<div class="section" id="available-shortcuts">
<h5>Available Shortcuts<a class="headerlink" href="#available-shortcuts" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">shelp()</span></tt> - print a help with the list of available objects and shortcuts</li>
<li><tt class="docutils literal"><span class="pre">fetch(request_or_url)</span></tt> - fetch a new response from the given request or
URL and update all related objects accordingly.</li>
<li><tt class="docutils literal"><span class="pre">view(response)</span></tt> - open the given response in your local web browser, for
inspection. This will add a <a class="reference external" href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base">&lt;base&gt; tag</a> to the response body in order
for external links (such as images and style sheets) to display properly.
Note, however,that this will create a temporary file in your computer,
which won&#8217;t be removed automatically.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="available-scrapy-objects">
<h5>Available Scrapy objects<a class="headerlink" href="#available-scrapy-objects" title="Permalink to this headline">¶</a></h5>
<p>The Scrapy shell automatically creates some convenient objects from the
downloaded page, like the <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object and the
<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> objects (for both HTML and XML
content).</p>
<p>Those objects are:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">crawler</span></tt> - the current <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><tt class="xref py py-class docutils literal"><span class="pre">Crawler</span></tt></a> object.</li>
<li><tt class="docutils literal"><span class="pre">spider</span></tt> - the Spider which is known to handle the URL, or a
<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object if there is no spider found for
the current URL</li>
<li><tt class="docutils literal"><span class="pre">request</span></tt> - a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object of the last fetched
page. You can modify this request using <a class="reference internal" href="index.html#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><tt class="xref py py-meth docutils literal"><span class="pre">replace()</span></tt></a>
or fetch a new request (without leaving the shell) using the <tt class="docutils literal"><span class="pre">fetch</span></tt>
shortcut.</li>
<li><tt class="docutils literal"><span class="pre">response</span></tt> - a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object containing the last
fetched page</li>
<li><tt class="docutils literal"><span class="pre">sel</span></tt> - a <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> object constructed
with the last response fetched</li>
<li><tt class="docutils literal"><span class="pre">settings</span></tt> - the current <a class="reference internal" href="index.html#topics-settings"><em>Scrapy settings</em></a></li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="example-of-shell-session">
<h4>Example of shell session<a class="headerlink" href="#example-of-shell-session" title="Permalink to this headline">¶</a></h4>
<p>Here&#8217;s an example of a typical shell session where we start by scraping the
<a class="reference external" href="http://scrapy.org">http://scrapy.org</a> page, and then proceed to scrape the <a class="reference external" href="http://slashdot.org">http://slashdot.org</a>
page. Finally, we modify the (Slashdot) request method to POST and re-fetch it
getting a HTTP 405 (method not allowed) error. We end the session by typing
Ctrl-D (in Unix systems) or Ctrl-Z in Windows.</p>
<p>Keep in mind that the data extracted here may not be the same when you try it,
as those pages are not static and could have changed by the time you test this.
The only purpose of this example is to get you familiarized with how the Scrapy
shell works.</p>
<p>First, we launch the shell:</p>
<div class="highlight-python"><div class="highlight"><pre>scrapy shell &#39;http://scrapy.org&#39; --nolog
</pre></div>
</div>
<p>Then, the shell fetches the URL (using the Scrapy downloader) and prints the
list of available objects and useful shortcuts (you&#8217;ll notice that these lines
all start with the <tt class="docutils literal"><span class="pre">[s]</span></tt> prefix):</p>
<div class="highlight-python"><div class="highlight"><pre>[s] Available Scrapy objects:
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x1e16b50&gt;
[s]   item       {}
[s]   request    &lt;GET http://scrapy.org&gt;
[s]   response   &lt;200 http://scrapy.org&gt;
[s]   sel        &lt;Selector xpath=None data=u&#39;&lt;html&gt;\n  &lt;head&gt;\n    &lt;meta charset=&quot;utf-8&#39;&gt;
[s]   settings   &lt;scrapy.settings.Settings object at 0x2bfd650&gt;
[s]   spider     &lt;Spider &#39;default&#39; at 0x20c6f50&gt;
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser

&gt;&gt;&gt;
</pre></div>
</div>
<p>After that, we can star playing with the objects:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&quot;//h2/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">u&#39;Welcome to Scrapy&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">fetch</span><span class="p">(</span><span class="s">&quot;http://slashdot.org&quot;</span><span class="p">)</span>
<span class="go">[s] Available Scrapy objects:</span>
<span class="go">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x1a13b50&gt;</span>
<span class="go">[s]   item       {}</span>
<span class="go">[s]   request    &lt;GET http://slashdot.org&gt;</span>
<span class="go">[s]   response   &lt;200 http://slashdot.org&gt;</span>
<span class="go">[s]   sel        &lt;Selector xpath=None data=u&#39;&lt;html lang=&quot;en&quot;&gt;\n&lt;head&gt;\n\n\n\n\n&lt;script id=&quot;&#39;&gt;</span>
<span class="go">[s]   settings   &lt;scrapy.settings.Settings object at 0x2bfd650&gt;</span>
<span class="go">[s]   spider     &lt;Spider &#39;default&#39; at 0x20c6f50&gt;</span>
<span class="go">[s] Useful shortcuts:</span>
<span class="go">[s]   shelp()           Shell help (print this help)</span>
<span class="go">[s]   fetch(req_or_url) Fetch request (or URL) and update local objects</span>
<span class="go">[s]   view(response)    View response in a browser</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//title/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;Slashdot: News for nerds, stuff that matters&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">request</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s">&quot;POST&quot;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">fetch</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
<span class="go">[s] Available Scrapy objects:</span>
<span class="go">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x1e16b50&gt;</span>
<span class="gp">...</span>

<span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="invoking-the-shell-from-spiders-to-inspect-responses">
<span id="topics-shell-inspect-response"></span><h4>Invoking the shell from spiders to inspect responses<a class="headerlink" href="#invoking-the-shell-from-spiders-to-inspect-responses" title="Permalink to this headline">¶</a></h4>
<p>Sometimes you want to inspect the responses that are being processed in a
certain point of your spider, if only to check that response you expect is
getting there.</p>
<p>This can be achieved by using the <tt class="docutils literal"><span class="pre">scrapy.shell.inspect_response</span></tt> function.</p>
<p>Here&#8217;s an example of how you would call it from your spider:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&quot;myspider&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">&quot;http://example.com&quot;</span><span class="p">,</span>
        <span class="s">&quot;http://example.org&quot;</span><span class="p">,</span>
        <span class="s">&quot;http://example.net&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c"># We want to inspect one specific response.</span>
        <span class="k">if</span> <span class="s">&quot;.org&quot;</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">scrapy.shell</span> <span class="kn">import</span> <span class="n">inspect_response</span>
            <span class="n">inspect_response</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

        <span class="c"># Rest of parsing code.</span>
</pre></div>
</div>
<p>When you run the spider, you will get something similar to this:</p>
<div class="highlight-python"><div class="highlight"><pre>2014-01-23 17:48:31-0400 [myspider] DEBUG: Crawled (200) &lt;GET http://example.com&gt; (referer: None)
2014-01-23 17:48:31-0400 [myspider] DEBUG: Crawled (200) &lt;GET http://example.org&gt; (referer: None)
[s] Available Scrapy objects:
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x1e16b50&gt;
...

&gt;&gt;&gt; response.url
&#39;http://example.org&#39;
</pre></div>
</div>
<p>Then, you can check if the extraction code is working:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//h1[@class=&quot;fn&quot;]&#39;</span><span class="p">)</span>
<span class="go">[]</span>
</pre></div>
</div>
<p>Nope, it doesn&#8217;t. So you can open the response in your web browser and see if
it&#8217;s the response you were expecting:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">view</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Finally you hit Ctrl-D (or Ctrl-Z in Windows) to exit the shell and resume the
crawling:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="o">^</span><span class="n">D</span>
<span class="go">2014-01-23 17:50:03-0400 [myspider] DEBUG: Crawled (200) &lt;GET http://example.net&gt; (referer: None)</span>
<span class="gp">...</span>
</pre></div>
</div>
<p>Note that you can&#8217;t use the <tt class="docutils literal"><span class="pre">fetch</span></tt> shortcut here since the Scrapy engine is
blocked by the shell. However, after you leave the shell, the spider will
continue crawling where it stopped, as shown above.</p>
</div>
</div>
<span id="document-topics/item-pipeline"></span><div class="section" id="item-pipeline">
<span id="topics-item-pipeline"></span><h3>Item Pipeline<a class="headerlink" href="#item-pipeline" title="Permalink to this headline">¶</a></h3>
<p>After an item has been scraped by a spider, it is sent to the Item Pipeline
which process it through several components that are executed sequentially.</p>
<p>Each item pipeline component (sometimes referred as just &#8220;Item Pipeline&#8221;) is a
Python class that implements a simple method. They receive an Item and perform
an action over it, also deciding if the Item should continue through the
pipeline or be dropped and no longer processed.</p>
<p>Typical use for item pipelines are:</p>
<ul class="simple">
<li>cleansing HTML data</li>
<li>validating scraped data (checking that the items contain certain fields)</li>
<li>checking for duplicates (and dropping them)</li>
<li>storing the scraped item in a database</li>
</ul>
<div class="section" id="writing-your-own-item-pipeline">
<h4>Writing your own item pipeline<a class="headerlink" href="#writing-your-own-item-pipeline" title="Permalink to this headline">¶</a></h4>
<p>Writing your own item pipeline is easy. Each item pipeline component is a
single Python class that must implement the following method:</p>
<dl class="method">
<dt id="process_item">
<tt class="descname">process_item</tt><big>(</big><em>item</em>, <em>spider</em><big>)</big><a class="headerlink" href="#process_item" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for every item pipeline component and must either return
a <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> (or any descendant class) object or raise a
<a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><tt class="xref py py-exc docutils literal"><span class="pre">DropItem</span></tt></a> exception. Dropped items are no longer
processed by further pipeline components.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> object) &#8211; the item scraped</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider which scraped the item</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<p>Additionally, they may also implement the following methods:</p>
<dl class="method">
<dt id="open_spider">
<tt class="descname">open_spider</tt><big>(</big><em>spider</em><big>)</big><a class="headerlink" href="#open_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called when the spider is opened.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider which was opened</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="close_spider">
<tt class="descname">close_spider</tt><big>(</big><em>spider</em><big>)</big><a class="headerlink" href="#close_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called when the spider is closed.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider which was closed</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="item-pipeline-example">
<h4>Item pipeline example<a class="headerlink" href="#item-pipeline-example" title="Permalink to this headline">¶</a></h4>
<div class="section" id="price-validation-and-dropping-items-with-no-prices">
<h5>Price validation and dropping items with no prices<a class="headerlink" href="#price-validation-and-dropping-items-with-no-prices" title="Permalink to this headline">¶</a></h5>
<p>Let&#8217;s take a look at the following hypothetical pipeline that adjusts the <tt class="docutils literal"><span class="pre">price</span></tt>
attribute for those items that do not include VAT (<tt class="docutils literal"><span class="pre">price_excludes_vat</span></tt>
attribute), and drops those items which don&#8217;t contain a price:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>

<span class="k">class</span> <span class="nc">PricePipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="n">vat_factor</span> <span class="o">=</span> <span class="mf">1.15</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;price&#39;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;price_excludes_vat&#39;</span><span class="p">]:</span>
                <span class="n">item</span><span class="p">[</span><span class="s">&#39;price&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;price&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vat_factor</span>
            <span class="k">return</span> <span class="n">item</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s">&quot;Missing price in </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">item</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="write-items-to-a-json-file">
<h5>Write items to a JSON file<a class="headerlink" href="#write-items-to-a-json-file" title="Permalink to this headline">¶</a></h5>
<p>The following pipeline stores all scraped items (from all spiders) into a a
single <tt class="docutils literal"><span class="pre">items.jl</span></tt> file, containing one item per line serialized in JSON
format:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">json</span>

<span class="k">class</span> <span class="nc">JsonWriterPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">&#39;items.jl&#39;</span><span class="p">,</span> <span class="s">&#39;wb&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">))</span> <span class="o">+</span> <span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The purpose of JsonWriterPipeline is just to introduce how to write
item pipelines. If you really want to store all scraped items into a JSON
file you should use the <a class="reference internal" href="index.html#topics-feed-exports"><em>Feed exports</em></a>.</p>
</div>
</div>
<div class="section" id="duplicates-filter">
<h5>Duplicates filter<a class="headerlink" href="#duplicates-filter" title="Permalink to this headline">¶</a></h5>
<p>A filter that looks for duplicate items, and drops those items that were
already processed. Let say that our items have an unique id, but our spider
returns multiples items with the same id:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>

<span class="k">class</span> <span class="nc">DuplicatesPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s">&#39;id&#39;</span><span class="p">]</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s">&quot;Duplicate item found: </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">item</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s">&#39;id&#39;</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="activating-an-item-pipeline-component">
<h4>Activating an Item Pipeline component<a class="headerlink" href="#activating-an-item-pipeline-component" title="Permalink to this headline">¶</a></h4>
<p>To activate an Item Pipeline component you must add its class to the
<a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><tt class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></tt></a> setting, like in the following example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">&#39;myproject.pipelines.PricePipeline&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s">&#39;myproject.pipelines.JsonWriterPipeline&#39;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The integer values you assign to classes in this setting determine the
order they run in- items go through pipelines from order number low to
high. It&#8217;s customary to define these numbers in the 0-1000 range.</p>
</div>
</div>
<span id="document-topics/feed-exports"></span><div class="section" id="feed-exports">
<span id="topics-feed-exports"></span><h3>Feed exports<a class="headerlink" href="#feed-exports" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p>One of the most frequently required features when implementing scrapers is
being able to store the scraped data properly and, quite often, that means
generating a &#8220;export file&#8221; with the scraped data (commonly called &#8220;export
feed&#8221;) to be consumed by other systems.</p>
<p>Scrapy provides this functionality out of the box with the Feed Exports, which
allows you to generate a feed with the scraped items, using multiple
serialization formats and storage backends.</p>
<div class="section" id="serialization-formats">
<span id="topics-feed-format"></span><h4>Serialization formats<a class="headerlink" href="#serialization-formats" title="Permalink to this headline">¶</a></h4>
<p>For serializing the scraped data, the feed exports use the <a class="reference internal" href="index.html#topics-exporters"><em>Item exporters</em></a> and these formats are supported out of the box:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#topics-feed-format-json"><em>JSON</em></a></li>
<li><a class="reference internal" href="index.html#topics-feed-format-jsonlines"><em>JSON lines</em></a></li>
<li><a class="reference internal" href="index.html#topics-feed-format-csv"><em>CSV</em></a></li>
<li><a class="reference internal" href="index.html#topics-feed-format-xml"><em>XML</em></a></li>
</ul>
</div></blockquote>
<p>But you can also extend the supported format through the
<a class="reference internal" href="index.html#std:setting-FEED_EXPORTERS"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_EXPORTERS</span></tt></a> setting.</p>
<div class="section" id="json">
<span id="topics-feed-format-json"></span><h5>JSON<a class="headerlink" href="#json" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-FEED_FORMAT"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></tt></a>: <tt class="docutils literal"><span class="pre">json</span></tt></li>
<li>Exporter used: <a class="reference internal" href="index.html#scrapy.contrib.exporter.JsonItemExporter" title="scrapy.contrib.exporter.JsonItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">JsonItemExporter</span></tt></a></li>
<li>See <a class="reference internal" href="index.html#json-with-large-data"><em>this warning</em></a> if you&#8217;re using JSON with large feeds</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="json-lines">
<span id="topics-feed-format-jsonlines"></span><h5>JSON lines<a class="headerlink" href="#json-lines" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-FEED_FORMAT"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></tt></a>: <tt class="docutils literal"><span class="pre">jsonlines</span></tt></li>
<li>Exporter used: <a class="reference internal" href="index.html#scrapy.contrib.exporter.JsonLinesItemExporter" title="scrapy.contrib.exporter.JsonLinesItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">JsonLinesItemExporter</span></tt></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="csv">
<span id="topics-feed-format-csv"></span><h5>CSV<a class="headerlink" href="#csv" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-FEED_FORMAT"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></tt></a>: <tt class="docutils literal"><span class="pre">csv</span></tt></li>
<li>Exporter used: <a class="reference internal" href="index.html#scrapy.contrib.exporter.CsvItemExporter" title="scrapy.contrib.exporter.CsvItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">CsvItemExporter</span></tt></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="xml">
<span id="topics-feed-format-xml"></span><h5>XML<a class="headerlink" href="#xml" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-FEED_FORMAT"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></tt></a>: <tt class="docutils literal"><span class="pre">xml</span></tt></li>
<li>Exporter used: <a class="reference internal" href="index.html#scrapy.contrib.exporter.XmlItemExporter" title="scrapy.contrib.exporter.XmlItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">XmlItemExporter</span></tt></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="pickle">
<span id="topics-feed-format-pickle"></span><h5>Pickle<a class="headerlink" href="#pickle" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-FEED_FORMAT"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></tt></a>: <tt class="docutils literal"><span class="pre">pickle</span></tt></li>
<li>Exporter used: <a class="reference internal" href="index.html#scrapy.contrib.exporter.PickleItemExporter" title="scrapy.contrib.exporter.PickleItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">PickleItemExporter</span></tt></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="marshal">
<span id="topics-feed-format-marshal"></span><h5>Marshal<a class="headerlink" href="#marshal" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-FEED_FORMAT"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></tt></a>: <tt class="docutils literal"><span class="pre">marshal</span></tt></li>
<li>Exporter used: <tt class="xref py py-class docutils literal"><span class="pre">MarshalItemExporter</span></tt></li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="storages">
<span id="topics-feed-storage"></span><h4>Storages<a class="headerlink" href="#storages" title="Permalink to this headline">¶</a></h4>
<p>When using the feed exports you define where to store the feed using a <a class="reference external" href="http://en.wikipedia.org/wiki/Uniform_Resource_Identifier">URI</a>
(through the <a class="reference internal" href="index.html#std:setting-FEED_URI"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_URI</span></tt></a> setting). The feed exports supports multiple
storage backend types which are defined by the URI scheme.</p>
<p>The storages backends supported out of the box are:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#topics-feed-storage-fs"><em>Local filesystem</em></a></li>
<li><a class="reference internal" href="index.html#topics-feed-storage-ftp"><em>FTP</em></a></li>
<li><a class="reference internal" href="index.html#topics-feed-storage-s3"><em>S3</em></a> (requires <a class="reference external" href="http://code.google.com/p/boto/">boto</a>)</li>
<li><a class="reference internal" href="index.html#topics-feed-storage-stdout"><em>Standard output</em></a></li>
</ul>
</div></blockquote>
<p>Some storage backends may be unavailable if the required external libraries are
not available. For example, the S3 backend is only available if the <a class="reference external" href="http://code.google.com/p/boto/">boto</a>
library is installed.</p>
</div>
<div class="section" id="storage-uri-parameters">
<span id="topics-feed-uri-params"></span><h4>Storage URI parameters<a class="headerlink" href="#storage-uri-parameters" title="Permalink to this headline">¶</a></h4>
<p>The storage URI can also contain parameters that get replaced when the feed is
being created. These parameters are:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">%(time)s</span></tt> - gets replaced by a timestamp when the feed is being created</li>
<li><tt class="docutils literal"><span class="pre">%(name)s</span></tt> - gets replaced by the spider name</li>
</ul>
</div></blockquote>
<p>Any other named parameter gets replaced by the spider attribute of the same
name. For example, <tt class="docutils literal"><span class="pre">%(site_id)s</span></tt> would get replaced by the <tt class="docutils literal"><span class="pre">spider.site_id</span></tt>
attribute the moment the feed is being created.</p>
<p>Here are some examples to illustrate:</p>
<blockquote>
<div><ul class="simple">
<li>Store in FTP using one directory per spider:<ul>
<li><tt class="docutils literal"><span class="pre">ftp://user:password&#64;ftp.example.com/scraping/feeds/%(name)s/%(time)s.json</span></tt></li>
</ul>
</li>
<li>Store in S3 using one directory per spider:<ul>
<li><tt class="docutils literal"><span class="pre">s3://mybucket/scraping/feeds/%(name)s/%(time)s.json</span></tt></li>
</ul>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="storage-backends">
<span id="topics-feed-storage-backends"></span><h4>Storage backends<a class="headerlink" href="#storage-backends" title="Permalink to this headline">¶</a></h4>
<div class="section" id="local-filesystem">
<span id="topics-feed-storage-fs"></span><h5>Local filesystem<a class="headerlink" href="#local-filesystem" title="Permalink to this headline">¶</a></h5>
<p>The feeds are stored in the local filesystem.</p>
<blockquote>
<div><ul class="simple">
<li>URI scheme: <tt class="docutils literal"><span class="pre">file</span></tt></li>
<li>Example URI: <tt class="docutils literal"><span class="pre">file:///tmp/export.csv</span></tt></li>
<li>Required external libraries: none</li>
</ul>
</div></blockquote>
<p>Note that for the local filesystem storage (only) you can omit the scheme if
you specify an absolute path like <tt class="docutils literal"><span class="pre">/tmp/export.csv</span></tt>. This only works on Unix
systems though.</p>
</div>
<div class="section" id="ftp">
<span id="topics-feed-storage-ftp"></span><h5>FTP<a class="headerlink" href="#ftp" title="Permalink to this headline">¶</a></h5>
<p>The feeds are stored in a FTP server.</p>
<blockquote>
<div><ul class="simple">
<li>URI scheme: <tt class="docutils literal"><span class="pre">ftp</span></tt></li>
<li>Example URI: <tt class="docutils literal"><span class="pre">ftp://user:pass&#64;ftp.example.com/path/to/export.csv</span></tt></li>
<li>Required external libraries: none</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="s3">
<span id="topics-feed-storage-s3"></span><h5>S3<a class="headerlink" href="#s3" title="Permalink to this headline">¶</a></h5>
<p>The feeds are stored on <a class="reference external" href="http://aws.amazon.com/s3/">Amazon S3</a>.</p>
<blockquote>
<div><ul class="simple">
<li>URI scheme: <tt class="docutils literal"><span class="pre">s3</span></tt></li>
<li>Example URIs:<ul>
<li><tt class="docutils literal"><span class="pre">s3://mybucket/path/to/export.csv</span></tt></li>
<li><tt class="docutils literal"><span class="pre">s3://aws_key:aws_secret&#64;mybucket/path/to/export.csv</span></tt></li>
</ul>
</li>
<li>Required external libraries: <a class="reference external" href="http://code.google.com/p/boto/">boto</a></li>
</ul>
</div></blockquote>
<p>The AWS credentials can be passed as user/password in the URI, or they can be
passed through the following settings:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-AWS_ACCESS_KEY_ID"><tt class="xref std std-setting docutils literal"><span class="pre">AWS_ACCESS_KEY_ID</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-AWS_SECRET_ACCESS_KEY"><tt class="xref std std-setting docutils literal"><span class="pre">AWS_SECRET_ACCESS_KEY</span></tt></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="standard-output">
<span id="topics-feed-storage-stdout"></span><h5>Standard output<a class="headerlink" href="#standard-output" title="Permalink to this headline">¶</a></h5>
<p>The feeds are written to the standard output of the Scrapy process.</p>
<blockquote>
<div><ul class="simple">
<li>URI scheme: <tt class="docutils literal"><span class="pre">stdout</span></tt></li>
<li>Example URI: <tt class="docutils literal"><span class="pre">stdout:</span></tt></li>
<li>Required external libraries: none</li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="settings">
<h4>Settings<a class="headerlink" href="#settings" title="Permalink to this headline">¶</a></h4>
<p>These are the settings used for configuring the feed exports:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-FEED_URI"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_URI</span></tt></a> (mandatory)</li>
<li><a class="reference internal" href="index.html#std:setting-FEED_FORMAT"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_STORAGES"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_STORAGES</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_EXPORTERS"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_EXPORTERS</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_STORE_EMPTY"><tt class="xref std std-setting docutils literal"><span class="pre">FEED_STORE_EMPTY</span></tt></a></li>
</ul>
</div></blockquote>
<div class="section" id="feed-uri">
<span id="std:setting-FEED_URI"></span><h5>FEED_URI<a class="headerlink" href="#feed-uri" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">None</span></tt></p>
<p>The URI of the export feed. See <a class="reference internal" href="index.html#topics-feed-storage-backends"><em>Storage backends</em></a> for
supported URI schemes.</p>
<p>This setting is required for enabling the feed exports.</p>
</div>
<div class="section" id="feed-format">
<span id="std:setting-FEED_FORMAT"></span><h5>FEED_FORMAT<a class="headerlink" href="#feed-format" title="Permalink to this headline">¶</a></h5>
<p>The serialization format to be used for the feed. See
<a class="reference internal" href="index.html#topics-feed-format"><em>Serialization formats</em></a> for possible values.</p>
</div>
<div class="section" id="feed-store-empty">
<span id="std:setting-FEED_STORE_EMPTY"></span><h5>FEED_STORE_EMPTY<a class="headerlink" href="#feed-store-empty" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Whether to export empty feeds (ie. feeds with no items).</p>
</div>
<div class="section" id="feed-storages">
<span id="std:setting-FEED_STORAGES"></span><h5>FEED_STORAGES<a class="headerlink" href="#feed-storages" title="Permalink to this headline">¶</a></h5>
<p>Default:: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>A dict containing additional feed storage backends supported by your project.
The keys are URI schemes and the values are paths to storage classes.</p>
</div>
<div class="section" id="feed-storages-base">
<span id="std:setting-FEED_STORAGES_BASE"></span><h5>FEED_STORAGES_BASE<a class="headerlink" href="#feed-storages-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">{</span>
    <span class="s">&#39;&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.feedexport.FileFeedStorage&#39;</span><span class="p">,</span>
    <span class="s">&#39;file&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.feedexport.FileFeedStorage&#39;</span><span class="p">,</span>
    <span class="s">&#39;stdout&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.feedexport.StdoutFeedStorage&#39;</span><span class="p">,</span>
    <span class="s">&#39;s3&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.feedexport.S3FeedStorage&#39;</span><span class="p">,</span>
    <span class="s">&#39;ftp&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.feedexport.FTPFeedStorage&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the built-in feed storage backends supported by Scrapy.</p>
</div>
<div class="section" id="feed-exporters">
<span id="std:setting-FEED_EXPORTERS"></span><h5>FEED_EXPORTERS<a class="headerlink" href="#feed-exporters" title="Permalink to this headline">¶</a></h5>
<p>Default:: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>A dict containing additional exporters supported by your project. The keys are
URI schemes and the values are paths to <a class="reference internal" href="index.html#topics-exporters"><em>Item exporter</em></a>
classes.</p>
</div>
<div class="section" id="feed-exporters-base">
<span id="std:setting-FEED_EXPORTERS_BASE"></span><h5>FEED_EXPORTERS_BASE<a class="headerlink" href="#feed-exporters-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">FEED_EXPORTERS_BASE</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">&#39;json&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.exporter.JsonItemExporter&#39;</span><span class="p">,</span>
    <span class="s">&#39;jsonlines&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.exporter.JsonLinesItemExporter&#39;</span><span class="p">,</span>
    <span class="s">&#39;csv&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.exporter.CsvItemExporter&#39;</span><span class="p">,</span>
    <span class="s">&#39;xml&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.exporter.XmlItemExporter&#39;</span><span class="p">,</span>
    <span class="s">&#39;marshal&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.contrib.exporter.MarshalItemExporter&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the built-in feed exporters supported by Scrapy.</p>
</div>
</div>
</div>
<span id="document-topics/link-extractors"></span><div class="section" id="link-extractors">
<span id="topics-link-extractors"></span><h3>Link Extractors<a class="headerlink" href="#link-extractors" title="Permalink to this headline">¶</a></h3>
<p>LinkExtractors are objects whose only purpose is to extract links from web
pages (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.http.Response</span></tt></a> objects) which will be eventually
followed.</p>
<p>There are two Link Extractors available in Scrapy by default, but you create
your own custom Link Extractors to suit your needs by implementing a simple
interface.</p>
<p>The only public method that every LinkExtractor has is <tt class="docutils literal"><span class="pre">extract_links</span></tt>,
which receives a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object and returns a list
of <tt class="xref py py-class docutils literal"><span class="pre">scrapy.link.Link</span></tt> objects. Link Extractors are meant to be instantiated once and their
<tt class="docutils literal"><span class="pre">extract_links</span></tt> method called several times with different responses, to
extract links to follow.</p>
<p>Link extractors are used in the <a class="reference internal" href="index.html#scrapy.contrib.spiders.CrawlSpider" title="scrapy.contrib.spiders.CrawlSpider"><tt class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></tt></a>
class (available in Scrapy), through a set of rules, but you can also use it in
your spiders, even if you don&#8217;t subclass from
<a class="reference internal" href="index.html#scrapy.contrib.spiders.CrawlSpider" title="scrapy.contrib.spiders.CrawlSpider"><tt class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></tt></a>, as its purpose is very simple: to
extract links.</p>
<div class="section" id="module-scrapy.contrib.linkextractors">
<span id="built-in-link-extractors-reference"></span><span id="topics-link-extractors-ref"></span><h4>Built-in link extractors reference<a class="headerlink" href="#module-scrapy.contrib.linkextractors" title="Permalink to this headline">¶</a></h4>
<p>All available link extractors classes bundled with Scrapy are provided in the
<a class="reference internal" href="index.html#module-scrapy.contrib.linkextractors" title="scrapy.contrib.linkextractors: Link extractors classes"><tt class="xref py py-mod docutils literal"><span class="pre">scrapy.contrib.linkextractors</span></tt></a> module.</p>
<p>If you don&#8217;t know what link extractor to choose, just use the default which is
the same as LxmlLinkExtractor (see below):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.linkextractors</span> <span class="kn">import</span> <span class="n">LinkExtractor</span>
</pre></div>
</div>
<div class="section" id="module-scrapy.contrib.linkextractors.lxmlhtml">
<span id="lxmllinkextractor"></span><h5>LxmlLinkExtractor<a class="headerlink" href="#module-scrapy.contrib.linkextractors.lxmlhtml" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.linkextractors.lxmlhtml.LxmlLinkExtractor">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.linkextractors.lxmlhtml.</tt><tt class="descname">LxmlLinkExtractor</tt><big>(</big><em>allow=()</em>, <em>deny=()</em>, <em>allow_domains=()</em>, <em>deny_domains=()</em>, <em>deny_extensions=None</em>, <em>restrict_xpaths=()</em>, <em>tags=('a'</em>, <em>'area')</em>, <em>attrs=('href'</em>, <em>)</em>, <em>canonicalize=True</em>, <em>unique=True</em>, <em>process_value=None</em><big>)</big><a class="headerlink" href="#scrapy.contrib.linkextractors.lxmlhtml.LxmlLinkExtractor" title="Permalink to this definition">¶</a></dt>
<dd><p>LxmlLinkExtractor is the recommended link extractor with handy filtering
options. It is implemented using lxml&#8217;s robust HTMLParser.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>allow</strong> (<em>a regular expression (or list of)</em>) &#8211; a single regular expression (or list of regular expressions)
that the (absolute) urls must match in order to be extracted. If not
given (or empty), it will match all links.</li>
<li><strong>deny</strong> (<em>a regular expression (or list of)</em>) &#8211; a single regular expression (or list of regular expressions)
that the (absolute) urls must match in order to be excluded (ie. not
extracted). It has precedence over the <tt class="docutils literal"><span class="pre">allow</span></tt> parameter. If not
given (or empty) it won&#8217;t exclude any links.</li>
<li><strong>allow_domains</strong> (<em>str or list</em>) &#8211; a single value or a list of string containing
domains which will be considered for extracting the links</li>
<li><strong>deny_domains</strong> (<em>str or list</em>) &#8211; a single value or a list of strings containing
domains which won&#8217;t be considered for extracting the links</li>
<li><strong>deny_extensions</strong> (<em>list</em>) &#8211; a single value or list of strings containing
extensions that should be ignored when extracting links.
If not given, it will default to the
<tt class="docutils literal"><span class="pre">IGNORED_EXTENSIONS</span></tt> list defined in the <a class="reference external" href="https://github.com/scrapy/scrapy/blob/master/scrapy/linkextractor.py">scrapy.linkextractor</a>
module.</li>
<li><strong>restrict_xpaths</strong> (<em>str or list</em>) &#8211; is a XPath (or list of XPath&#8217;s) which defines
regions inside the response where links should be extracted from.
If given, only the text selected by those XPath will be scanned for
links. See examples below.</li>
<li><strong>tags</strong> (<em>str or list</em>) &#8211; a tag or a list of tags to consider when extracting links.
Defaults to <tt class="docutils literal"><span class="pre">('a',</span> <span class="pre">'area')</span></tt>.</li>
<li><strong>attrs</strong> (<em>list</em>) &#8211; an attribute or list of attributes which should be considered when looking
for links to extract (only for those tags specified in the <tt class="docutils literal"><span class="pre">tags</span></tt>
parameter). Defaults to <tt class="docutils literal"><span class="pre">('href',)</span></tt></li>
<li><strong>canonicalize</strong> (<em>boolean</em>) &#8211; canonicalize each extracted url (using
scrapy.utils.url.canonicalize_url). Defaults to <tt class="docutils literal"><span class="pre">True</span></tt>.</li>
<li><strong>unique</strong> (<em>boolean</em>) &#8211; whether duplicate filtering should be applied to extracted
links.</li>
<li><strong>process_value</strong> (<em>callable</em>) &#8211; see <tt class="docutils literal"><span class="pre">process_value</span></tt> argument of
<tt class="xref py py-class docutils literal"><span class="pre">BaseSgmlLinkExtractor</span></tt> class constructor</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.linkextractors.sgml">
<span id="sgmllinkextractor"></span><h5>SgmlLinkExtractor<a class="headerlink" href="#module-scrapy.contrib.linkextractors.sgml" title="Permalink to this headline">¶</a></h5>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">SGMLParser based link extractors are unmantained and its usage is discouraged.
It is recommended to migrate to <tt class="xref py py-class docutils literal"><span class="pre">LxmlLinkExtractor</span></tt> if you are still
using <a class="reference internal" href="index.html#scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor" title="scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor"><tt class="xref py py-class docutils literal"><span class="pre">SgmlLinkExtractor</span></tt></a>.</p>
</div>
<dl class="class">
<dt id="scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.linkextractors.sgml.</tt><tt class="descname">SgmlLinkExtractor</tt><big>(</big><em>allow=()</em>, <em>deny=()</em>, <em>allow_domains=()</em>, <em>deny_domains=()</em>, <em>deny_extensions=None</em>, <em>restrict_xpaths=()</em>, <em>tags=('a'</em>, <em>'area')</em>, <em>attrs=('href')</em>, <em>canonicalize=True</em>, <em>unique=True</em>, <em>process_value=None</em><big>)</big><a class="headerlink" href="#scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor" title="Permalink to this definition">¶</a></dt>
<dd><p>The SgmlLinkExtractor is built upon the  base <a class="reference internal" href="index.html#scrapy.contrib.linkextractors.sgml.BaseSgmlLinkExtractor" title="scrapy.contrib.linkextractors.sgml.BaseSgmlLinkExtractor"><tt class="xref py py-class docutils literal"><span class="pre">BaseSgmlLinkExtractor</span></tt></a>
and provides additional filters that you can specify to extract links,
including regular expressions patterns that the links must match to be
extracted. All those filters are configured through these constructor
parameters:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>allow</strong> (<em>a regular expression (or list of)</em>) &#8211; a single regular expression (or list of regular expressions)
that the (absolute) urls must match in order to be extracted. If not
given (or empty), it will match all links.</li>
<li><strong>deny</strong> (<em>a regular expression (or list of)</em>) &#8211; a single regular expression (or list of regular expressions)
that the (absolute) urls must match in order to be excluded (ie. not
extracted). It has precedence over the <tt class="docutils literal"><span class="pre">allow</span></tt> parameter. If not
given (or empty) it won&#8217;t exclude any links.</li>
<li><strong>allow_domains</strong> (<em>str or list</em>) &#8211; a single value or a list of string containing
domains which will be considered for extracting the links</li>
<li><strong>deny_domains</strong> (<em>str or list</em>) &#8211; a single value or a list of strings containing
domains which won&#8217;t be considered for extracting the links</li>
<li><strong>deny_extensions</strong> (<em>list</em>) &#8211; a single value or list of strings containing
extensions that should be ignored when extracting links.
If not given, it will default to the
<tt class="docutils literal"><span class="pre">IGNORED_EXTENSIONS</span></tt> list defined in the <a class="reference external" href="https://github.com/scrapy/scrapy/blob/master/scrapy/linkextractor.py">scrapy.linkextractor</a>
module.</li>
<li><strong>restrict_xpaths</strong> (<em>str or list</em>) &#8211; is a XPath (or list of XPath&#8217;s) which defines
regions inside the response where links should be extracted from.
If given, only the text selected by those XPath will be scanned for
links. See examples below.</li>
<li><strong>tags</strong> (<em>str or list</em>) &#8211; a tag or a list of tags to consider when extracting links.
Defaults to <tt class="docutils literal"><span class="pre">('a',</span> <span class="pre">'area')</span></tt>.</li>
<li><strong>attrs</strong> (<em>list</em>) &#8211; an attribute or list of attributes which should be considered when looking
for links to extract (only for those tags specified in the <tt class="docutils literal"><span class="pre">tags</span></tt>
parameter). Defaults to <tt class="docutils literal"><span class="pre">('href',)</span></tt></li>
<li><strong>canonicalize</strong> (<em>boolean</em>) &#8211; canonicalize each extracted url (using
scrapy.utils.url.canonicalize_url). Defaults to <tt class="docutils literal"><span class="pre">True</span></tt>.</li>
<li><strong>unique</strong> (<em>boolean</em>) &#8211; whether duplicate filtering should be applied to extracted
links.</li>
<li><strong>process_value</strong> (<em>callable</em>) &#8211; see <tt class="docutils literal"><span class="pre">process_value</span></tt> argument of
<a class="reference internal" href="index.html#scrapy.contrib.linkextractors.sgml.BaseSgmlLinkExtractor" title="scrapy.contrib.linkextractors.sgml.BaseSgmlLinkExtractor"><tt class="xref py py-class docutils literal"><span class="pre">BaseSgmlLinkExtractor</span></tt></a> class constructor</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="basesgmllinkextractor">
<h5>BaseSgmlLinkExtractor<a class="headerlink" href="#basesgmllinkextractor" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.linkextractors.sgml.BaseSgmlLinkExtractor">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.linkextractors.sgml.</tt><tt class="descname">BaseSgmlLinkExtractor</tt><big>(</big><em>tag=&quot;a&quot;</em>, <em>attr=&quot;href&quot;</em>, <em>unique=False</em>, <em>process_value=None</em><big>)</big><a class="headerlink" href="#scrapy.contrib.linkextractors.sgml.BaseSgmlLinkExtractor" title="Permalink to this definition">¶</a></dt>
<dd><p>The purpose of this Link Extractor is only to serve as a base class for the
<a class="reference internal" href="index.html#scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor" title="scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor"><tt class="xref py py-class docutils literal"><span class="pre">SgmlLinkExtractor</span></tt></a>. You should use that one instead.</p>
<p>The constructor arguments are:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tag</strong> (<em>str or callable</em>) &#8211; either a string (with the name of a tag) or a function that
receives a tag name and returns <tt class="docutils literal"><span class="pre">True</span></tt> if links should be extracted from
that tag, or <tt class="docutils literal"><span class="pre">False</span></tt> if they shouldn&#8217;t. Defaults to <tt class="docutils literal"><span class="pre">'a'</span></tt>.  request
(once it&#8217;s downloaded) as its first parameter. For more information, see
<a class="reference internal" href="index.html#topics-request-response-ref-request-callback-arguments"><em>Passing additional data to callback functions</em></a>.</li>
<li><strong>attr</strong> (<em>str or callable</em>) &#8211; either string (with the name of a tag attribute), or a
function that receives an attribute name and returns <tt class="docutils literal"><span class="pre">True</span></tt> if
links should be extracted from it, or <tt class="docutils literal"><span class="pre">False</span></tt> if they shouldn&#8217;t.
Defaults to <tt class="docutils literal"><span class="pre">href</span></tt>.</li>
<li><strong>unique</strong> (<em>boolean</em>) &#8211; is a boolean that specifies if a duplicate filtering should
be applied to links extracted.</li>
<li><strong>process_value</strong> (<em>callable</em>) &#8211; <p>a function which receives each value extracted from
the tag and attributes scanned and can modify the value and return a
new one, or return <tt class="docutils literal"><span class="pre">None</span></tt> to ignore the link altogether. If not
given, <tt class="docutils literal"><span class="pre">process_value</span></tt> defaults to <tt class="docutils literal"><span class="pre">lambda</span> <span class="pre">x:</span> <span class="pre">x</span></tt>.</p>
<p>For example, to extract links from this code:</p>
<div class="highlight-html"><div class="highlight"><pre><span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">&quot;javascript:goToPage(&#39;../other/page.html&#39;); return false&quot;</span><span class="nt">&gt;</span>Link text<span class="nt">&lt;/a&gt;</span>
</pre></div>
</div>
<p>You can use the following function in <tt class="docutils literal"><span class="pre">process_value</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">process_value</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="s">&quot;javascript:goToPage\(&#39;(.*?)&#39;&quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">m</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-topics/commands"><em>Command line tool</em></a></dt>
<dd>Learn about the command-line tool used to manage your Scrapy project.</dd>
<dt><a class="reference internal" href="index.html#document-topics/items"><em>Items</em></a></dt>
<dd>Define the data you want to scrape.</dd>
<dt><a class="reference internal" href="index.html#document-topics/spiders"><em>Spiders</em></a></dt>
<dd>Write the rules to crawl your websites.</dd>
<dt><a class="reference internal" href="index.html#document-topics/selectors"><em>Selectors</em></a></dt>
<dd>Extract the data from web pages using XPath.</dd>
<dt><a class="reference internal" href="index.html#document-topics/shell"><em>Scrapy shell</em></a></dt>
<dd>Test your extraction code in an interactive environment.</dd>
<dt><a class="reference internal" href="index.html#document-topics/loaders"><em>Item Loaders</em></a></dt>
<dd>Populate your items with the extracted data.</dd>
<dt><a class="reference internal" href="index.html#document-topics/item-pipeline"><em>Item Pipeline</em></a></dt>
<dd>Post-process and store your scraped data.</dd>
<dt><a class="reference internal" href="index.html#document-topics/feed-exports"><em>Feed exports</em></a></dt>
<dd>Output your scraped data using different formats and storages.</dd>
<dt><a class="reference internal" href="index.html#document-topics/link-extractors"><em>Link Extractors</em></a></dt>
<dd>Convenient classes to extract links to follow from pages.</dd>
</dl>
</div>
<div class="section" id="built-in-services">
<h2>Built-in services<a class="headerlink" href="#built-in-services" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-topics/logging"></span><div class="section" id="logging">
<span id="topics-logging"></span><h3>Logging<a class="headerlink" href="#logging" title="Permalink to this headline">¶</a></h3>
<p>Scrapy provides a logging facility which can be used through the
<a class="reference internal" href="index.html#module-scrapy.log" title="scrapy.log: Logging facility"><tt class="xref py py-mod docutils literal"><span class="pre">scrapy.log</span></tt></a> module. The current underlying implementation uses <a class="reference external" href="http://twistedmatrix.com/projects/core/documentation/howto/logging.html">Twisted
logging</a> but this may change in the future.</p>
<p>The logging service must be explicitly started through the <a class="reference internal" href="index.html#scrapy.log.start" title="scrapy.log.start"><tt class="xref py py-func docutils literal"><span class="pre">scrapy.log.start()</span></tt></a> function.</p>
<div class="section" id="log-levels">
<span id="topics-logging-levels"></span><h4>Log levels<a class="headerlink" href="#log-levels" title="Permalink to this headline">¶</a></h4>
<p>Scrapy provides 5 logging levels:</p>
<ol class="arabic simple">
<li><a class="reference internal" href="index.html#scrapy.log.CRITICAL" title="scrapy.log.CRITICAL"><tt class="xref py py-data docutils literal"><span class="pre">CRITICAL</span></tt></a> - for critical errors</li>
<li><a class="reference internal" href="index.html#scrapy.log.ERROR" title="scrapy.log.ERROR"><tt class="xref py py-data docutils literal"><span class="pre">ERROR</span></tt></a> - for regular errors</li>
<li><a class="reference internal" href="index.html#scrapy.log.WARNING" title="scrapy.log.WARNING"><tt class="xref py py-data docutils literal"><span class="pre">WARNING</span></tt></a> - for warning messages</li>
<li><a class="reference internal" href="index.html#scrapy.log.INFO" title="scrapy.log.INFO"><tt class="xref py py-data docutils literal"><span class="pre">INFO</span></tt></a> - for informational messages</li>
<li><a class="reference internal" href="index.html#scrapy.log.DEBUG" title="scrapy.log.DEBUG"><tt class="xref py py-data docutils literal"><span class="pre">DEBUG</span></tt></a> - for debugging messages</li>
</ol>
</div>
<div class="section" id="how-to-set-the-log-level">
<h4>How to set the log level<a class="headerlink" href="#how-to-set-the-log-level" title="Permalink to this headline">¶</a></h4>
<p>You can set the log level using the <cite>&#8211;loglevel/-L</cite> command line option, or
using the <a class="reference internal" href="index.html#std:setting-LOG_LEVEL"><tt class="xref std std-setting docutils literal"><span class="pre">LOG_LEVEL</span></tt></a> setting.</p>
</div>
<div class="section" id="how-to-log-messages">
<h4>How to log messages<a class="headerlink" href="#how-to-log-messages" title="Permalink to this headline">¶</a></h4>
<p>Here&#8217;s a quick example of how to log a message using the <tt class="docutils literal"><span class="pre">WARNING</span></tt> level:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">log</span>
<span class="n">log</span><span class="o">.</span><span class="n">msg</span><span class="p">(</span><span class="s">&quot;This is a warning&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">log</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="logging-from-spiders">
<h4>Logging from Spiders<a class="headerlink" href="#logging-from-spiders" title="Permalink to this headline">¶</a></h4>
<p>The recommended way to log from spiders is by using the Spider
<a class="reference internal" href="index.html#scrapy.spider.Spider.log" title="scrapy.spider.Spider.log"><tt class="xref py py-meth docutils literal"><span class="pre">log()</span></tt></a> method, which already populates the
<tt class="docutils literal"><span class="pre">spider</span></tt> argument of the <a class="reference internal" href="index.html#scrapy.log.msg" title="scrapy.log.msg"><tt class="xref py py-func docutils literal"><span class="pre">scrapy.log.msg()</span></tt></a> function. The other arguments
are passed directly to the <a class="reference internal" href="index.html#scrapy.log.msg" title="scrapy.log.msg"><tt class="xref py py-func docutils literal"><span class="pre">msg()</span></tt></a> function.</p>
</div>
<div class="section" id="module-scrapy.log">
<span id="scrapy-log-module"></span><h4>scrapy.log module<a class="headerlink" href="#module-scrapy.log" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="scrapy.log.start">
<tt class="descclassname">scrapy.log.</tt><tt class="descname">start</tt><big>(</big><em>logfile=None</em>, <em>loglevel=None</em>, <em>logstdout=None</em><big>)</big><a class="headerlink" href="#scrapy.log.start" title="Permalink to this definition">¶</a></dt>
<dd><p>Start the logging facility. This must be called before actually logging any
messages. Otherwise, messages logged before this call will get lost.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>logfile</strong> (<em>str</em>) &#8211; the file path to use for logging output. If omitted, the
<a class="reference internal" href="index.html#std:setting-LOG_FILE"><tt class="xref std std-setting docutils literal"><span class="pre">LOG_FILE</span></tt></a> setting will be used. If both are <tt class="docutils literal"><span class="pre">None</span></tt>, the log
will be sent to standard error.</li>
<li><strong>loglevel</strong> &#8211; the minimum logging level to log. Available values are:
<a class="reference internal" href="index.html#scrapy.log.CRITICAL" title="scrapy.log.CRITICAL"><tt class="xref py py-data docutils literal"><span class="pre">CRITICAL</span></tt></a>, <a class="reference internal" href="index.html#scrapy.log.ERROR" title="scrapy.log.ERROR"><tt class="xref py py-data docutils literal"><span class="pre">ERROR</span></tt></a>, <a class="reference internal" href="index.html#scrapy.log.WARNING" title="scrapy.log.WARNING"><tt class="xref py py-data docutils literal"><span class="pre">WARNING</span></tt></a>, <a class="reference internal" href="index.html#scrapy.log.INFO" title="scrapy.log.INFO"><tt class="xref py py-data docutils literal"><span class="pre">INFO</span></tt></a> and
<a class="reference internal" href="index.html#scrapy.log.DEBUG" title="scrapy.log.DEBUG"><tt class="xref py py-data docutils literal"><span class="pre">DEBUG</span></tt></a>.</li>
<li><strong>logstdout</strong> (<em>boolean</em>) &#8211; if <tt class="docutils literal"><span class="pre">True</span></tt>, all standard output (and error) of your
application will be logged instead. For example if you &#8220;print &#8216;hello&#8217;&#8221;
it will appear in the Scrapy log. If omitted, the <a class="reference internal" href="index.html#std:setting-LOG_STDOUT"><tt class="xref std std-setting docutils literal"><span class="pre">LOG_STDOUT</span></tt></a>
setting will be used.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="scrapy.log.msg">
<tt class="descclassname">scrapy.log.</tt><tt class="descname">msg</tt><big>(</big><em>message</em>, <em>level=INFO</em>, <em>spider=None</em><big>)</big><a class="headerlink" href="#scrapy.log.msg" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a message</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>message</strong> (<em>str</em>) &#8211; the message to log</li>
<li><strong>level</strong> &#8211; the log level for this message. See
<a class="reference internal" href="index.html#topics-logging-levels"><em>Log levels</em></a>.</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider to use for logging this message. This parameter
should always be used when logging things related to a particular
spider.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="data">
<dt id="scrapy.log.CRITICAL">
<tt class="descclassname">scrapy.log.</tt><tt class="descname">CRITICAL</tt><a class="headerlink" href="#scrapy.log.CRITICAL" title="Permalink to this definition">¶</a></dt>
<dd><p>Log level for critical errors</p>
</dd></dl>

<dl class="data">
<dt id="scrapy.log.ERROR">
<tt class="descclassname">scrapy.log.</tt><tt class="descname">ERROR</tt><a class="headerlink" href="#scrapy.log.ERROR" title="Permalink to this definition">¶</a></dt>
<dd><p>Log level for errors</p>
</dd></dl>

<dl class="data">
<dt id="scrapy.log.WARNING">
<tt class="descclassname">scrapy.log.</tt><tt class="descname">WARNING</tt><a class="headerlink" href="#scrapy.log.WARNING" title="Permalink to this definition">¶</a></dt>
<dd><p>Log level for warnings</p>
</dd></dl>

<dl class="data">
<dt id="scrapy.log.INFO">
<tt class="descclassname">scrapy.log.</tt><tt class="descname">INFO</tt><a class="headerlink" href="#scrapy.log.INFO" title="Permalink to this definition">¶</a></dt>
<dd><p>Log level for informational messages (recommended level for production
deployments)</p>
</dd></dl>

<dl class="data">
<dt id="scrapy.log.DEBUG">
<tt class="descclassname">scrapy.log.</tt><tt class="descname">DEBUG</tt><a class="headerlink" href="#scrapy.log.DEBUG" title="Permalink to this definition">¶</a></dt>
<dd><p>Log level for debugging messages (recommended level for development)</p>
</dd></dl>

</div>
<div class="section" id="logging-settings">
<h4>Logging settings<a class="headerlink" href="#logging-settings" title="Permalink to this headline">¶</a></h4>
<p>These settings can be used to configure the logging:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-LOG_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">LOG_ENABLED</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_ENCODING"><tt class="xref std std-setting docutils literal"><span class="pre">LOG_ENCODING</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_FILE"><tt class="xref std std-setting docutils literal"><span class="pre">LOG_FILE</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_LEVEL"><tt class="xref std std-setting docutils literal"><span class="pre">LOG_LEVEL</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_STDOUT"><tt class="xref std std-setting docutils literal"><span class="pre">LOG_STDOUT</span></tt></a></li>
</ul>
</div>
</div>
<span id="document-topics/stats"></span><div class="section" id="stats-collection">
<span id="topics-stats"></span><h3>Stats Collection<a class="headerlink" href="#stats-collection" title="Permalink to this headline">¶</a></h3>
<p>Scrapy provides a convenient facility for collecting stats in the form of
key/values, where values are often counters. The facility is called the Stats
Collector, and can be accessed through the <a class="reference internal" href="index.html#scrapy.crawler.Crawler.stats" title="scrapy.crawler.Crawler.stats"><tt class="xref py py-attr docutils literal"><span class="pre">stats</span></tt></a>
attribute of the <a class="reference internal" href="index.html#topics-api-crawler"><em>Crawler API</em></a>, as illustrated by the examples in
the <a class="reference internal" href="index.html#topics-stats-usecases"><em>Common Stats Collector uses</em></a> section below.</p>
<p>However, the Stats Collector is always available, so you can always import it
in your module and use its API (to increment or set new stat keys), regardless
of whether the stats collection is enabled or not. If it&#8217;s disabled, the API
will still work but it won&#8217;t collect anything. This is aimed at simplifying the
stats collector usage: you should spend no more than one line of code for
collecting stats in your spider, Scrapy extension, or whatever code you&#8217;re
using the Stats Collector from.</p>
<p>Another feature of the Stats Collector is that it&#8217;s very efficient (when
enabled) and extremely efficient (almost unnoticeable) when disabled.</p>
<p>The Stats Collector keeps a stats table per open spider which is automatically
opened when the spider is opened, and closed when the spider is closed.</p>
<div class="section" id="common-stats-collector-uses">
<span id="topics-stats-usecases"></span><h4>Common Stats Collector uses<a class="headerlink" href="#common-stats-collector-uses" title="Permalink to this headline">¶</a></h4>
<p>Access the stats collector through the <a class="reference internal" href="index.html#scrapy.crawler.Crawler.stats" title="scrapy.crawler.Crawler.stats"><tt class="xref py py-attr docutils literal"><span class="pre">stats</span></tt></a>
attribute. Here is an example of an extension that access stats:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">ExtensionThatAccessStats</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stats</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stats</span> <span class="o">=</span> <span class="n">stats</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">cls</span><span class="p">(</span><span class="n">crawler</span><span class="o">.</span><span class="n">stats</span><span class="p">)</span>
</pre></div>
</div>
<p>Set stat value:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">stats</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="s">&#39;hostname&#39;</span><span class="p">,</span> <span class="n">socket</span><span class="o">.</span><span class="n">gethostname</span><span class="p">())</span>
</pre></div>
</div>
<p>Increment stat value:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">stats</span><span class="o">.</span><span class="n">inc_value</span><span class="p">(</span><span class="s">&#39;pages_crawled&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Set stat value only if greater than previous:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">stats</span><span class="o">.</span><span class="n">max_value</span><span class="p">(</span><span class="s">&#39;max_items_scraped&#39;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<p>Set stat value only if lower than previous:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">stats</span><span class="o">.</span><span class="n">min_value</span><span class="p">(</span><span class="s">&#39;min_free_memory_percent&#39;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<p>Get stat value:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">stats</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="s">&#39;pages_crawled&#39;</span><span class="p">)</span>
<span class="go">8</span>
</pre></div>
</div>
<p>Get all stats:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">stats</span><span class="o">.</span><span class="n">get_stats</span><span class="p">()</span>
<span class="go">{&#39;pages_crawled&#39;: 1238, &#39;start_time&#39;: datetime.datetime(2009, 7, 14, 21, 47, 28, 977139)}</span>
</pre></div>
</div>
</div>
<div class="section" id="available-stats-collectors">
<h4>Available Stats Collectors<a class="headerlink" href="#available-stats-collectors" title="Permalink to this headline">¶</a></h4>
<p>Besides the basic <tt class="xref py py-class docutils literal"><span class="pre">StatsCollector</span></tt> there are other Stats Collectors
available in Scrapy which extend the basic Stats Collector. You can select
which Stats Collector to use through the <a class="reference internal" href="index.html#std:setting-STATS_CLASS"><tt class="xref std std-setting docutils literal"><span class="pre">STATS_CLASS</span></tt></a> setting. The
default Stats Collector used is the <tt class="xref py py-class docutils literal"><span class="pre">MemoryStatsCollector</span></tt>.</p>
<span class="target" id="module-scrapy.statscol"></span><div class="section" id="memorystatscollector">
<h5>MemoryStatsCollector<a class="headerlink" href="#memorystatscollector" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.statscol.MemoryStatsCollector">
<em class="property">class </em><tt class="descclassname">scrapy.statscol.</tt><tt class="descname">MemoryStatsCollector</tt><a class="headerlink" href="#scrapy.statscol.MemoryStatsCollector" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple stats collector that keeps the stats of the last scraping run (for
each spider) in memory, after they&#8217;re closed. The stats can be accessed
through the <a class="reference internal" href="index.html#scrapy.statscol.MemoryStatsCollector.spider_stats" title="scrapy.statscol.MemoryStatsCollector.spider_stats"><tt class="xref py py-attr docutils literal"><span class="pre">spider_stats</span></tt></a> attribute, which is a dict keyed by spider
domain name.</p>
<p>This is the default Stats Collector used in Scrapy.</p>
<dl class="attribute">
<dt id="scrapy.statscol.MemoryStatsCollector.spider_stats">
<tt class="descname">spider_stats</tt><a class="headerlink" href="#scrapy.statscol.MemoryStatsCollector.spider_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>A dict of dicts (keyed by spider name) containing the stats of the last
scraping run for each spider.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="dummystatscollector">
<h5>DummyStatsCollector<a class="headerlink" href="#dummystatscollector" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.statscol.DummyStatsCollector">
<em class="property">class </em><tt class="descclassname">scrapy.statscol.</tt><tt class="descname">DummyStatsCollector</tt><a class="headerlink" href="#scrapy.statscol.DummyStatsCollector" title="Permalink to this definition">¶</a></dt>
<dd><p>A Stats collector which does nothing but is very efficient (because it does
nothing). This stats collector can be set via the <a class="reference internal" href="index.html#std:setting-STATS_CLASS"><tt class="xref std std-setting docutils literal"><span class="pre">STATS_CLASS</span></tt></a>
setting, to disable stats collect in order to improve performance. However,
the performance penalty of stats collection is usually marginal compared to
other Scrapy workload like parsing pages.</p>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/email"></span><div class="section" id="module-scrapy.mail">
<span id="sending-e-mail"></span><span id="topics-email"></span><h3>Sending e-mail<a class="headerlink" href="#module-scrapy.mail" title="Permalink to this headline">¶</a></h3>
<p>Although Python makes sending e-mails relatively easy via the <a class="reference external" href="http://docs.python.org/library/smtplib.html">smtplib</a>
library, Scrapy provides its own facility for sending e-mails which is very
easy to use and it&#8217;s implemented using <a class="reference external" href="http://twistedmatrix.com/documents/current/core/howto/defer-intro.html">Twisted non-blocking IO</a>, to avoid
interfering with the non-blocking IO of the crawler. It also provides a
simple API for sending attachments and it&#8217;s very easy to configure, with a few
<a class="reference internal" href="index.html#topics-email-settings"><em>settings</em></a>.</p>
<div class="section" id="quick-example">
<h4>Quick example<a class="headerlink" href="#quick-example" title="Permalink to this headline">¶</a></h4>
<p>There are two ways to instantiate the mail sender. You can instantiate it using
the standard constructor:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.mail</span> <span class="kn">import</span> <span class="n">MailSender</span>
<span class="n">mailer</span> <span class="o">=</span> <span class="n">MailSender</span><span class="p">()</span>
</pre></div>
</div>
<p>Or you can instantiate it passing a Scrapy settings object, which will respect
the <a class="reference internal" href="index.html#topics-email-settings"><em>settings</em></a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">mailer</span> <span class="o">=</span> <span class="n">MailSender</span><span class="o">.</span><span class="n">from_settings</span><span class="p">(</span><span class="n">settings</span><span class="p">)</span>
</pre></div>
</div>
<p>And here is how to use it to send an e-mail (without attachments):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">mailer</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">to</span><span class="o">=</span><span class="p">[</span><span class="s">&quot;someone@example.com&quot;</span><span class="p">],</span> <span class="n">subject</span><span class="o">=</span><span class="s">&quot;Some subject&quot;</span><span class="p">,</span> <span class="n">body</span><span class="o">=</span><span class="s">&quot;Some body&quot;</span><span class="p">,</span> <span class="n">cc</span><span class="o">=</span><span class="p">[</span><span class="s">&quot;another@example.com&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="mailsender-class-reference">
<h4>MailSender class reference<a class="headerlink" href="#mailsender-class-reference" title="Permalink to this headline">¶</a></h4>
<p>MailSender is the preferred class to use for sending emails from Scrapy, as it
uses <a class="reference external" href="http://twistedmatrix.com/documents/current/core/howto/defer-intro.html">Twisted non-blocking IO</a>, like the rest of the framework.</p>
<dl class="class">
<dt id="scrapy.mail.MailSender">
<em class="property">class </em><tt class="descclassname">scrapy.mail.</tt><tt class="descname">MailSender</tt><big>(</big><em>smtphost=None</em>, <em>mailfrom=None</em>, <em>smtpuser=None</em>, <em>smtppass=None</em>, <em>smtpport=None</em><big>)</big><a class="headerlink" href="#scrapy.mail.MailSender" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>smtphost</strong> (<em>str</em>) &#8211; the SMTP host to use for sending the emails. If omitted, the
<a class="reference internal" href="index.html#std:setting-MAIL_HOST"><tt class="xref std std-setting docutils literal"><span class="pre">MAIL_HOST</span></tt></a> setting will be used.</li>
<li><strong>mailfrom</strong> (<em>str</em>) &#8211; the address used to send emails (in the <tt class="docutils literal"><span class="pre">From:</span></tt> header).
If omitted, the <a class="reference internal" href="index.html#std:setting-MAIL_FROM"><tt class="xref std std-setting docutils literal"><span class="pre">MAIL_FROM</span></tt></a> setting will be used.</li>
<li><strong>smtpuser</strong> &#8211; the SMTP user. If omitted, the <a class="reference internal" href="index.html#std:setting-MAIL_USER"><tt class="xref std std-setting docutils literal"><span class="pre">MAIL_USER</span></tt></a>
setting will be used. If not given, no SMTP authentication will be
performed.</li>
<li><strong>smtppass</strong> (<em>str</em>) &#8211; the SMTP pass for authentication.</li>
<li><strong>smtpport</strong> (<em>boolean</em>) &#8211; the SMTP port to connect to</li>
<li><strong>smtptls</strong> &#8211; enforce using SMTP STARTTLS</li>
<li><strong>smtpssl</strong> &#8211; enforce using a secure SSL connection</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="classmethod">
<dt id="scrapy.mail.MailSender.from_settings">
<em class="property">classmethod </em><tt class="descname">from_settings</tt><big>(</big><em>settings</em><big>)</big><a class="headerlink" href="#scrapy.mail.MailSender.from_settings" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiate using a Scrapy settings object, which will respect
<a class="reference internal" href="index.html#topics-email-settings"><em>these Scrapy settings</em></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>settings</strong> (<a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.settings.Settings</span></tt></a> object) &#8211; the e-mail recipients</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.mail.MailSender.send">
<tt class="descname">send</tt><big>(</big><em>to</em>, <em>subject</em>, <em>body</em>, <em>cc=None</em>, <em>attachs=()</em>, <em>mimetype='text/plain'</em><big>)</big><a class="headerlink" href="#scrapy.mail.MailSender.send" title="Permalink to this definition">¶</a></dt>
<dd><p>Send email to the given recipients.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>to</strong> (<em>list</em>) &#8211; the e-mail recipients</li>
<li><strong>subject</strong> (<em>str</em>) &#8211; the subject of the e-mail</li>
<li><strong>cc</strong> (<em>list</em>) &#8211; the e-mails to CC</li>
<li><strong>body</strong> (<em>str</em>) &#8211; the e-mail body</li>
<li><strong>attachs</strong> (<em>iterable</em>) &#8211; an iterable of tuples <tt class="docutils literal"><span class="pre">(attach_name,</span> <span class="pre">mimetype,</span>
<span class="pre">file_object)</span></tt> where  <tt class="docutils literal"><span class="pre">attach_name</span></tt> is a string with the name that will
appear on the e-mail&#8217;s attachment, <tt class="docutils literal"><span class="pre">mimetype</span></tt> is the mimetype of the
attachment and <tt class="docutils literal"><span class="pre">file_object</span></tt> is a readable file object with the
contents of the attachment</li>
<li><strong>mimetype</strong> (<em>str</em>) &#8211; the MIME type of the e-mail</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="mail-settings">
<span id="topics-email-settings"></span><h4>Mail settings<a class="headerlink" href="#mail-settings" title="Permalink to this headline">¶</a></h4>
<p>These settings define the default constructor values of the <a class="reference internal" href="index.html#scrapy.mail.MailSender" title="scrapy.mail.MailSender"><tt class="xref py py-class docutils literal"><span class="pre">MailSender</span></tt></a>
class, and can be used to configure e-mail notifications in your project without
writing any code (for those extensions and code that uses <a class="reference internal" href="index.html#scrapy.mail.MailSender" title="scrapy.mail.MailSender"><tt class="xref py py-class docutils literal"><span class="pre">MailSender</span></tt></a>).</p>
<div class="section" id="mail-from">
<span id="std:setting-MAIL_FROM"></span><h5>MAIL_FROM<a class="headerlink" href="#mail-from" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">'scrapy&#64;localhost'</span></tt></p>
<p>Sender email to use (<tt class="docutils literal"><span class="pre">From:</span></tt> header) for sending emails.</p>
</div>
<div class="section" id="mail-host">
<span id="std:setting-MAIL_HOST"></span><h5>MAIL_HOST<a class="headerlink" href="#mail-host" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">'localhost'</span></tt></p>
<p>SMTP host to use for sending emails.</p>
</div>
<div class="section" id="mail-port">
<span id="std:setting-MAIL_PORT"></span><h5>MAIL_PORT<a class="headerlink" href="#mail-port" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">25</span></tt></p>
<p>SMTP port to use for sending emails.</p>
</div>
<div class="section" id="mail-user">
<span id="std:setting-MAIL_USER"></span><h5>MAIL_USER<a class="headerlink" href="#mail-user" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">None</span></tt></p>
<p>User to use for SMTP authentication. If disabled no SMTP authentication will be
performed.</p>
</div>
<div class="section" id="mail-pass">
<span id="std:setting-MAIL_PASS"></span><h5>MAIL_PASS<a class="headerlink" href="#mail-pass" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">None</span></tt></p>
<p>Password to use for SMTP authentication, along with <a class="reference internal" href="index.html#std:setting-MAIL_USER"><tt class="xref std std-setting docutils literal"><span class="pre">MAIL_USER</span></tt></a>.</p>
</div>
<div class="section" id="mail-tls">
<span id="std:setting-MAIL_TLS"></span><h5>MAIL_TLS<a class="headerlink" href="#mail-tls" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Enforce using STARTTLS. STARTTLS is a way to take an existing insecure connection, and upgrade it to a secure connection using SSL/TLS.</p>
</div>
<div class="section" id="mail-ssl">
<span id="std:setting-MAIL_SSL"></span><h5>MAIL_SSL<a class="headerlink" href="#mail-ssl" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Enforce connecting using an SSL encrypted connection</p>
</div>
</div>
</div>
<span id="document-topics/telnetconsole"></span><div class="section" id="module-scrapy.telnet">
<span id="telnet-console"></span><span id="topics-telnetconsole"></span><h3>Telnet Console<a class="headerlink" href="#module-scrapy.telnet" title="Permalink to this headline">¶</a></h3>
<p>Scrapy comes with a built-in telnet console for inspecting and controlling a
Scrapy running process. The telnet console is just a regular python shell
running inside the Scrapy process, so you can do literally anything from it.</p>
<p>The telnet console is a <a class="reference internal" href="index.html#topics-extensions-ref"><em>built-in Scrapy extension</em></a> which comes enabled by default, but you can also
disable it if you want. For more information about the extension itself see
<a class="reference internal" href="index.html#topics-extensions-ref-telnetconsole"><em>Telnet console extension</em></a>.</p>
<div class="section" id="how-to-access-the-telnet-console">
<h4>How to access the telnet console<a class="headerlink" href="#how-to-access-the-telnet-console" title="Permalink to this headline">¶</a></h4>
<p>The telnet console listens in the TCP port defined in the
<a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_PORT"><tt class="xref std std-setting docutils literal"><span class="pre">TELNETCONSOLE_PORT</span></tt></a> setting, which defaults to <tt class="docutils literal"><span class="pre">6023</span></tt>. To access
the console you need to type:</p>
<div class="highlight-none"><div class="highlight"><pre>telnet localhost 6023
&gt;&gt;&gt;
</pre></div>
</div>
<p>You need the telnet program which comes installed by default in Windows, and
most Linux distros.</p>
</div>
<div class="section" id="available-variables-in-the-telnet-console">
<h4>Available variables in the telnet console<a class="headerlink" href="#available-variables-in-the-telnet-console" title="Permalink to this headline">¶</a></h4>
<p>The telnet console is like a regular Python shell running inside the Scrapy
process, so you can do anything from it including importing new modules, etc.</p>
<p>However, the telnet console comes with some default variables defined for
convenience:</p>
<table border="1" class="docutils">
<colgroup>
<col width="19%" />
<col width="81%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Shortcut</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><tt class="docutils literal"><span class="pre">crawler</span></tt></td>
<td>the Scrapy Crawler (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.crawler.Crawler</span></tt></a> object)</td>
</tr>
<tr class="row-odd"><td><tt class="docutils literal"><span class="pre">engine</span></tt></td>
<td>Crawler.engine attribute</td>
</tr>
<tr class="row-even"><td><tt class="docutils literal"><span class="pre">spider</span></tt></td>
<td>the active spider</td>
</tr>
<tr class="row-odd"><td><tt class="docutils literal"><span class="pre">slot</span></tt></td>
<td>the engine slot</td>
</tr>
<tr class="row-even"><td><tt class="docutils literal"><span class="pre">extensions</span></tt></td>
<td>the Extension Manager (Crawler.extensions attribute)</td>
</tr>
<tr class="row-odd"><td><tt class="docutils literal"><span class="pre">stats</span></tt></td>
<td>the Stats Collector (Crawler.stats attribute)</td>
</tr>
<tr class="row-even"><td><tt class="docutils literal"><span class="pre">settings</span></tt></td>
<td>the Scrapy settings object (Crawler.settings attribute)</td>
</tr>
<tr class="row-odd"><td><tt class="docutils literal"><span class="pre">est</span></tt></td>
<td>print a report of the engine status</td>
</tr>
<tr class="row-even"><td><tt class="docutils literal"><span class="pre">prefs</span></tt></td>
<td>for memory debugging (see <a class="reference internal" href="index.html#topics-leaks"><em>Debugging memory leaks</em></a>)</td>
</tr>
<tr class="row-odd"><td><tt class="docutils literal"><span class="pre">p</span></tt></td>
<td>a shortcut to the <a class="reference external" href="http://docs.python.org/library/pprint.html#pprint.pprint">pprint.pprint</a> function</td>
</tr>
<tr class="row-even"><td><tt class="docutils literal"><span class="pre">hpy</span></tt></td>
<td>for memory debugging (see <a class="reference internal" href="index.html#topics-leaks"><em>Debugging memory leaks</em></a>)</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="telnet-console-usage-examples">
<h4>Telnet console usage examples<a class="headerlink" href="#telnet-console-usage-examples" title="Permalink to this headline">¶</a></h4>
<p>Here are some example tasks you can do with the telnet console:</p>
<div class="section" id="view-engine-status">
<h5>View engine status<a class="headerlink" href="#view-engine-status" title="Permalink to this headline">¶</a></h5>
<p>You can use the <tt class="docutils literal"><span class="pre">est()</span></tt> method of the Scrapy engine to quickly show its state
using the telnet console:</p>
<div class="highlight-none"><div class="highlight"><pre>telnet localhost 6023
&gt;&gt;&gt; est()
Execution engine status

time()-engine.start_time                        : 8.62972998619
engine.has_capacity()                           : False
len(engine.downloader.active)                   : 16
engine.scraper.is_idle()                        : False
engine.spider.name                              : followall
engine.spider_is_idle(engine.spider)            : False
engine.slot.closing                             : False
len(engine.slot.inprogress)                     : 16
len(engine.slot.scheduler.dqs or [])            : 0
len(engine.slot.scheduler.mqs)                  : 92
len(engine.scraper.slot.queue)                  : 0
len(engine.scraper.slot.active)                 : 0
engine.scraper.slot.active_size                 : 0
engine.scraper.slot.itemproc_size               : 0
engine.scraper.slot.needs_backout()             : False
</pre></div>
</div>
</div>
<div class="section" id="pause-resume-and-stop-the-scrapy-engine">
<h5>Pause, resume and stop the Scrapy engine<a class="headerlink" href="#pause-resume-and-stop-the-scrapy-engine" title="Permalink to this headline">¶</a></h5>
<p>To pause:</p>
<div class="highlight-none"><div class="highlight"><pre>telnet localhost 6023
&gt;&gt;&gt; engine.pause()
&gt;&gt;&gt;
</pre></div>
</div>
<p>To resume:</p>
<div class="highlight-none"><div class="highlight"><pre>telnet localhost 6023
&gt;&gt;&gt; engine.unpause()
&gt;&gt;&gt;
</pre></div>
</div>
<p>To stop:</p>
<div class="highlight-none"><div class="highlight"><pre>telnet localhost 6023
&gt;&gt;&gt; engine.stop()
Connection closed by foreign host.
</pre></div>
</div>
</div>
</div>
<div class="section" id="telnet-console-signals">
<h4>Telnet Console signals<a class="headerlink" href="#telnet-console-signals" title="Permalink to this headline">¶</a></h4>
<span class="target" id="std:signal-update_telnet_vars"></span><dl class="function">
<dt id="scrapy.telnet.update_telnet_vars">
<tt class="descclassname">scrapy.telnet.</tt><tt class="descname">update_telnet_vars</tt><big>(</big><em>telnet_vars</em><big>)</big><a class="headerlink" href="#scrapy.telnet.update_telnet_vars" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent just before the telnet console is opened. You can hook up to this
signal to add, remove or update the variables that will be available in the
telnet local namespace. In order to do that, you need to update the
<tt class="docutils literal"><span class="pre">telnet_vars</span></tt> dict in your handler.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>telnet_vars</strong> (<em>dict</em>) &#8211; the dict of telnet variables</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="telnet-settings">
<h4>Telnet settings<a class="headerlink" href="#telnet-settings" title="Permalink to this headline">¶</a></h4>
<p>These are the settings that control the telnet console&#8217;s behaviour:</p>
<div class="section" id="telnetconsole-port">
<span id="std:setting-TELNETCONSOLE_PORT"></span><h5>TELNETCONSOLE_PORT<a class="headerlink" href="#telnetconsole-port" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">[6023,</span> <span class="pre">6073]</span></tt></p>
<p>The port range to use for the telnet console. If set to <tt class="docutils literal"><span class="pre">None</span></tt> or <tt class="docutils literal"><span class="pre">0</span></tt>, a
dynamically assigned port is used.</p>
</div>
<div class="section" id="telnetconsole-host">
<span id="std:setting-TELNETCONSOLE_HOST"></span><h5>TELNETCONSOLE_HOST<a class="headerlink" href="#telnetconsole-host" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">'127.0.0.1'</span></tt></p>
<p>The interface the telnet console should listen on</p>
</div>
</div>
</div>
<span id="document-topics/webservice"></span><div class="section" id="web-service">
<span id="topics-webservice"></span><h3>Web Service<a class="headerlink" href="#web-service" title="Permalink to this headline">¶</a></h3>
<p>Scrapy comes with a built-in web service for monitoring and controlling a
running crawler. The service exposes most resources using the <a class="reference external" href="http://www.jsonrpc.org/">JSON-RPC 2.0</a>
protocol, but there are also other (read-only) resources which just output JSON
data.</p>
<p>Provides an extensible web service for managing a Scrapy process. It&#8217;s enabled
by the <a class="reference internal" href="index.html#std:setting-WEBSERVICE_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">WEBSERVICE_ENABLED</span></tt></a> setting. The web server will listen in the
port specified in <a class="reference internal" href="index.html#std:setting-WEBSERVICE_PORT"><tt class="xref std std-setting docutils literal"><span class="pre">WEBSERVICE_PORT</span></tt></a>, and will log to the file
specified in <a class="reference internal" href="index.html#std:setting-WEBSERVICE_LOGFILE"><tt class="xref std std-setting docutils literal"><span class="pre">WEBSERVICE_LOGFILE</span></tt></a>.</p>
<p>The web service is a <a class="reference internal" href="index.html#topics-extensions-ref"><em>built-in Scrapy extension</em></a>
which comes enabled by default, but you can also disable it if you&#8217;re running
tight on memory.</p>
<div class="section" id="web-service-resources">
<span id="topics-webservice-resources"></span><h4>Web service resources<a class="headerlink" href="#web-service-resources" title="Permalink to this headline">¶</a></h4>
<p>The web service contains several resources, defined in the
<tt class="xref std std-setting docutils literal"><span class="pre">WEBSERVICE_RESOURCES</span></tt> setting. Each resource provides a different
functionality. See <a class="reference internal" href="index.html#topics-webservice-resources-ref"><em>Available JSON-RPC resources</em></a> for a list of
resources available by default.</p>
<p>Although you can implement your own resources using any protocol, there are
two kinds of resources bundled with Scrapy:</p>
<ul class="simple">
<li>Simple JSON resources - which are read-only and just output JSON data</li>
<li>JSON-RPC resources - which provide direct access to certain Scrapy objects
using the <a class="reference external" href="http://www.jsonrpc.org/">JSON-RPC 2.0</a> protocol</li>
</ul>
<span class="target" id="module-scrapy.contrib.webservice"></span><div class="section" id="available-json-rpc-resources">
<span id="topics-webservice-resources-ref"></span><h5>Available JSON-RPC resources<a class="headerlink" href="#available-json-rpc-resources" title="Permalink to this headline">¶</a></h5>
<p>These are the JSON-RPC resources available by default in Scrapy:</p>
<div class="section" id="module-scrapy.contrib.webservice.crawler">
<span id="crawler-json-rpc-resource"></span><span id="topics-webservice-crawler"></span><h6>Crawler JSON-RPC resource<a class="headerlink" href="#module-scrapy.contrib.webservice.crawler" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.webservice.crawler.CrawlerResource">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.webservice.crawler.</tt><tt class="descname">CrawlerResource</tt><a class="headerlink" href="#scrapy.contrib.webservice.crawler.CrawlerResource" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides access to the main Crawler object that controls the Scrapy
process.</p>
<p>Available by default at: <a class="reference external" href="http://localhost:6080/crawler">http://localhost:6080/crawler</a></p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.webservice.stats">
<span id="stats-collector-json-rpc-resource"></span><h6>Stats Collector JSON-RPC resource<a class="headerlink" href="#module-scrapy.contrib.webservice.stats" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.webservice.stats.StatsResource">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.webservice.stats.</tt><tt class="descname">StatsResource</tt><a class="headerlink" href="#scrapy.contrib.webservice.stats.StatsResource" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides access to the Stats Collector used by the crawler.</p>
<p>Available by default at: <a class="reference external" href="http://localhost:6080/stats">http://localhost:6080/stats</a></p>
</dd></dl>

</div>
<div class="section" id="spider-manager-json-rpc-resource">
<h6>Spider Manager JSON-RPC resource<a class="headerlink" href="#spider-manager-json-rpc-resource" title="Permalink to this headline">¶</a></h6>
<p>You can access the spider manager JSON-RPC resource through the
<a class="reference internal" href="index.html#topics-webservice-crawler"><em>Crawler JSON-RPC resource</em></a> at: <a class="reference external" href="http://localhost:6080/crawler/spiders">http://localhost:6080/crawler/spiders</a></p>
</div>
<div class="section" id="extension-manager-json-rpc-resource">
<h6>Extension Manager JSON-RPC resource<a class="headerlink" href="#extension-manager-json-rpc-resource" title="Permalink to this headline">¶</a></h6>
<p>You can access the extension manager JSON-RPC resource through the
<a class="reference internal" href="index.html#topics-webservice-crawler"><em>Crawler JSON-RPC resource</em></a> at: <a class="reference external" href="http://localhost:6080/crawler/spiders">http://localhost:6080/crawler/spiders</a></p>
</div>
</div>
<div class="section" id="available-json-resources">
<h5>Available JSON resources<a class="headerlink" href="#available-json-resources" title="Permalink to this headline">¶</a></h5>
<p>These are the JSON resources available by default:</p>
<div class="section" id="module-scrapy.contrib.webservice.enginestatus">
<span id="engine-status-json-resource"></span><h6>Engine status JSON resource<a class="headerlink" href="#module-scrapy.contrib.webservice.enginestatus" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.webservice.enginestatus.EngineStatusResource">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.webservice.enginestatus.</tt><tt class="descname">EngineStatusResource</tt><a class="headerlink" href="#scrapy.contrib.webservice.enginestatus.EngineStatusResource" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides access to engine status metrics.</p>
<p>Available by default at: <a class="reference external" href="http://localhost:6080/enginestatus">http://localhost:6080/enginestatus</a></p>
</dd></dl>

</div>
</div>
</div>
<div class="section" id="web-service-settings">
<h4>Web service settings<a class="headerlink" href="#web-service-settings" title="Permalink to this headline">¶</a></h4>
<p>These are the settings that control the web service behaviour:</p>
<div class="section" id="webservice-enabled">
<span id="std:setting-WEBSERVICE_ENABLED"></span><h5>WEBSERVICE_ENABLED<a class="headerlink" href="#webservice-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>A boolean which specifies if the web service will be enabled (provided its
extension is also enabled).</p>
</div>
<div class="section" id="webservice-logfile">
<span id="std:setting-WEBSERVICE_LOGFILE"></span><h5>WEBSERVICE_LOGFILE<a class="headerlink" href="#webservice-logfile" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">None</span></tt></p>
<p>A file to use for logging HTTP requests made to the web service. If unset web
the log is sent to standard scrapy log.</p>
</div>
<div class="section" id="webservice-port">
<span id="std:setting-WEBSERVICE_PORT"></span><h5>WEBSERVICE_PORT<a class="headerlink" href="#webservice-port" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">[6080,</span> <span class="pre">7030]</span></tt></p>
<p>The port range to use for the web service. If set to <tt class="docutils literal"><span class="pre">None</span></tt> or <tt class="docutils literal"><span class="pre">0</span></tt>, a
dynamically assigned port is used.</p>
</div>
<div class="section" id="webservice-host">
<span id="std:setting-WEBSERVICE_HOST"></span><h5>WEBSERVICE_HOST<a class="headerlink" href="#webservice-host" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">'127.0.0.1'</span></tt></p>
<p>The interface the web service should listen on</p>
</div>
<div class="section" id="webservice-resources">
<h5>WEBSERVICE_RESOURCES<a class="headerlink" href="#webservice-resources" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>The list of web service resources enabled for your project. See
<a class="reference internal" href="index.html#topics-webservice-resources"><em>Web service resources</em></a>. These are added to the ones available by
default in Scrapy, defined in the <tt class="xref std std-setting docutils literal"><span class="pre">WEBSERVICE_RESOURCES_BASE</span></tt> setting.</p>
</div>
<div class="section" id="webservice-resources-base">
<h5>WEBSERVICE_RESOURCES_BASE<a class="headerlink" href="#webservice-resources-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-none"><div class="highlight"><pre>{
    &#39;scrapy.contrib.webservice.crawler.CrawlerResource&#39;: 1,
    &#39;scrapy.contrib.webservice.enginestatus.EngineStatusResource&#39;: 1,
    &#39;scrapy.contrib.webservice.stats.StatsResource&#39;: 1,
}
</pre></div>
</div>
<p>The list of web service resources available by default in Scrapy. You shouldn&#8217;t
change this setting in your project, change <tt class="xref std std-setting docutils literal"><span class="pre">WEBSERVICE_RESOURCES</span></tt>
instead. If you want to disable some resource set its value to <tt class="docutils literal"><span class="pre">None</span></tt> in
<tt class="xref std std-setting docutils literal"><span class="pre">WEBSERVICE_RESOURCES</span></tt>.</p>
</div>
</div>
<div class="section" id="writing-a-web-service-resource">
<h4>Writing a web service resource<a class="headerlink" href="#writing-a-web-service-resource" title="Permalink to this headline">¶</a></h4>
<p>Web service resources are implemented using the Twisted Web API. See this
<a class="reference external" href="http://jcalderone.livejournal.com/50562.html">Twisted Web guide</a> for more information on Twisted web and Twisted web
resources.</p>
<p>To write a web service resource you should subclass the <tt class="xref py py-class docutils literal"><span class="pre">JsonResource</span></tt> or
<tt class="xref py py-class docutils literal"><span class="pre">JsonRpcResource</span></tt> classes and implement the <tt class="xref py py-class docutils literal"><span class="pre">renderGET</span></tt> method.</p>
<dl class="class">
<dt id="scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonResource">
<em class="property">class </em><tt class="descclassname">scrapy.webservice.</tt><tt class="descname">JsonResource</tt><a class="headerlink" href="#scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonResource" title="Permalink to this definition">¶</a></dt>
<dd><p>A subclass of <a class="reference external" href="http://twistedmatrix.com/documents/10.0.0/api/twisted.web.resource.Resource.html">twisted.web.resource.Resource</a> that implements a JSON web
service resource. See</p>
<dl class="attribute">
<dt id="scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonResource.ws_name">
<tt class="descname">ws_name</tt><a class="headerlink" href="#scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonResource.ws_name" title="Permalink to this definition">¶</a></dt>
<dd><p>The name by which the Scrapy web service will known this resource, and
also the path where this resource will listen. For example, assuming
Scrapy web service is listening on <a class="reference external" href="http://localhost:6080/">http://localhost:6080/</a> and the
<tt class="docutils literal"><span class="pre">ws_name</span></tt> is <tt class="docutils literal"><span class="pre">'resource1'</span></tt> the URL for that resource will be:</p>
<blockquote>
<div><a class="reference external" href="http://localhost:6080/resource1/">http://localhost:6080/resource1/</a></div></blockquote>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonRpcResource">
<em class="property">class </em><tt class="descclassname">scrapy.webservice.</tt><tt class="descname">JsonRpcResource</tt><big>(</big><em>crawler</em>, <em>target=None</em><big>)</big><a class="headerlink" href="#scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonRpcResource" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a subclass of <tt class="xref py py-class docutils literal"><span class="pre">JsonResource</span></tt> for implementing JSON-RPC
resources. JSON-RPC resources wrap Python (Scrapy) objects around a
JSON-RPC API. The resource wrapped must be returned by the
<a class="reference internal" href="index.html#scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonRpcResource.get_target" title="scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonRpcResource.get_target"><tt class="xref py py-meth docutils literal"><span class="pre">get_target()</span></tt></a> method, which returns the target passed in the
constructor by default</p>
<dl class="method">
<dt id="scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonRpcResource.get_target">
<tt class="descname">get_target</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.contrib.webservice.enginestatus.scrapy.webservice.JsonRpcResource.get_target" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the object wrapped by this JSON-RPC resource. By default, it
returns the object passed on the constructor.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="examples-of-web-service-resources">
<h4>Examples of web service resources<a class="headerlink" href="#examples-of-web-service-resources" title="Permalink to this headline">¶</a></h4>
<div class="section" id="statsresource-json-rpc-resource">
<h5>StatsResource (JSON-RPC resource)<a class="headerlink" href="#statsresource-json-rpc-resource" title="Permalink to this headline">¶</a></h5>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.webservice import JsonRpcResource

class StatsResource(JsonRpcResource):

    ws_name = &#39;stats&#39;

    def __init__(self, crawler):
        JsonRpcResource.__init__(self, crawler, crawler.stats)
</pre></div>
</div>
</div>
<div class="section" id="enginestatusresource-json-resource">
<h5>EngineStatusResource (JSON resource)<a class="headerlink" href="#enginestatusresource-json-resource" title="Permalink to this headline">¶</a></h5>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.webservice import JsonResource
from scrapy.utils.engine import get_engine_status

class EngineStatusResource(JsonResource):

    ws_name = &#39;enginestatus&#39;

    def __init__(self, crawler, spider_name=None):
        JsonResource.__init__(self, crawler)
        self._spider_name = spider_name
        self.isLeaf = spider_name is not None

    def render_GET(self, txrequest):
        status = get_engine_status(self.crawler.engine)
        if self._spider_name is None:
            return status
        for sp, st in status[&#39;spiders&#39;].items():
            if sp.name == self._spider_name:
                return st

    def getChild(self, name, txrequest):
        return EngineStatusResource(name, self.crawler)
</pre></div>
</div>
</div>
</div>
<div class="section" id="example-of-web-service-client">
<h4>Example of web service client<a class="headerlink" href="#example-of-web-service-client" title="Permalink to this headline">¶</a></h4>
<div class="section" id="scrapy-ws-py-script">
<h5>scrapy-ws.py script<a class="headerlink" href="#scrapy-ws-py-script" title="Permalink to this headline">¶</a></h5>
<div class="highlight-none"><div class="highlight"><pre>#!/usr/bin/env python
&quot;&quot;&quot;
Example script to control a Scrapy server using its JSON-RPC web service.

It only provides a reduced functionality as its main purpose is to illustrate
how to write a web service client. Feel free to improve or write you own.

Also, keep in mind that the JSON-RPC API is not stable. The recommended way for
controlling a Scrapy server is through the execution queue (see the &quot;queue&quot;
command).

&quot;&quot;&quot;

from __future__ import print_function
import sys, optparse, urllib, json
from urlparse import urljoin

from scrapy.utils.jsonrpc import jsonrpc_client_call, JsonRpcError

def get_commands():
    return {
        &#39;help&#39;: cmd_help,
        &#39;stop&#39;: cmd_stop,
        &#39;list-available&#39;: cmd_list_available,
        &#39;list-running&#39;: cmd_list_running,
        &#39;list-resources&#39;: cmd_list_resources,
        &#39;get-global-stats&#39;: cmd_get_global_stats,
        &#39;get-spider-stats&#39;: cmd_get_spider_stats,
    }

def cmd_help(args, opts):
    &quot;&quot;&quot;help - list available commands&quot;&quot;&quot;
    print(&quot;Available commands:&quot;)
    for _, func in sorted(get_commands().items()):
        print(&quot;  &quot;, func.__doc__)

def cmd_stop(args, opts):
    &quot;&quot;&quot;stop &lt;spider&gt; - stop a running spider&quot;&quot;&quot;
    jsonrpc_call(opts, &#39;crawler/engine&#39;, &#39;close_spider&#39;, args[0])

def cmd_list_running(args, opts):
    &quot;&quot;&quot;list-running - list running spiders&quot;&quot;&quot;
    for x in json_get(opts, &#39;crawler/engine/open_spiders&#39;):
        print(x)

def cmd_list_available(args, opts):
    &quot;&quot;&quot;list-available - list name of available spiders&quot;&quot;&quot;
    for x in jsonrpc_call(opts, &#39;crawler/spiders&#39;, &#39;list&#39;):
        print(x)

def cmd_list_resources(args, opts):
    &quot;&quot;&quot;list-resources - list available web service resources&quot;&quot;&quot;
    for x in json_get(opts, &#39;&#39;)[&#39;resources&#39;]:
        print(x)

def cmd_get_spider_stats(args, opts):
    &quot;&quot;&quot;get-spider-stats &lt;spider&gt; - get stats of a running spider&quot;&quot;&quot;
    stats = jsonrpc_call(opts, &#39;stats&#39;, &#39;get_stats&#39;, args[0])
    for name, value in stats.items():
        print(&quot;%-40s %s&quot; % (name, value))

def cmd_get_global_stats(args, opts):
    &quot;&quot;&quot;get-global-stats - get global stats&quot;&quot;&quot;
    stats = jsonrpc_call(opts, &#39;stats&#39;, &#39;get_stats&#39;)
    for name, value in stats.items():
        print(&quot;%-40s %s&quot; % (name, value))

def get_wsurl(opts, path):
    return urljoin(&quot;http://%s:%s/&quot;% (opts.host, opts.port), path)

def jsonrpc_call(opts, path, method, *args, **kwargs):
    url = get_wsurl(opts, path)
    return jsonrpc_client_call(url, method, *args, **kwargs)

def json_get(opts, path):
    url = get_wsurl(opts, path)
    return json.loads(urllib.urlopen(url).read())

def parse_opts():
    usage = &quot;%prog [options] &lt;command&gt; [arg] ...&quot;
    description = &quot;Scrapy web service control script. Use &#39;%prog help&#39; &quot; \
        &quot;to see the list of available commands.&quot;
    op = optparse.OptionParser(usage=usage, description=description)
    op.add_option(&quot;-H&quot;, dest=&quot;host&quot;, default=&quot;localhost&quot;, \
        help=&quot;Scrapy host to connect to&quot;)
    op.add_option(&quot;-P&quot;, dest=&quot;port&quot;, type=&quot;int&quot;, default=6080, \
        help=&quot;Scrapy port to connect to&quot;)
    opts, args = op.parse_args()
    if not args:
        op.print_help()
        sys.exit(2)
    cmdname, cmdargs, opts = args[0], args[1:], opts
    commands = get_commands()
    if cmdname not in commands:
        sys.stderr.write(&quot;Unknown command: %s\n\n&quot; % cmdname)
        cmd_help(None, None)
        sys.exit(1)
    return commands[cmdname], cmdargs, opts

def main():
    cmd, args, opts = parse_opts()
    try:
        cmd(args, opts)
    except IndexError:
        print(cmd.__doc__)
    except JsonRpcError as e:
        print(str(e))
        if e.data:
            print(&quot;Server Traceback below:&quot;)
            print(e.data)


if __name__ == &#39;__main__&#39;:
    main()
</pre></div>
</div>
</div>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-topics/logging"><em>Logging</em></a></dt>
<dd>Understand the simple logging facility provided by Scrapy.</dd>
<dt><a class="reference internal" href="index.html#document-topics/stats"><em>Stats Collection</em></a></dt>
<dd>Collect statistics about your scraping crawler.</dd>
<dt><a class="reference internal" href="index.html#document-topics/email"><em>Sending e-mail</em></a></dt>
<dd>Send email notifications when certain events occur.</dd>
<dt><a class="reference internal" href="index.html#document-topics/telnetconsole"><em>Telnet Console</em></a></dt>
<dd>Inspect a running crawler using a built-in Python console.</dd>
<dt><a class="reference internal" href="index.html#document-topics/webservice"><em>Web Service</em></a></dt>
<dd>Monitor and control a crawler using a web service.</dd>
</dl>
</div>
<div class="section" id="solving-specific-problems">
<h2>Solving specific problems<a class="headerlink" href="#solving-specific-problems" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-faq"></span><div class="section" id="frequently-asked-questions">
<span id="faq"></span><h3>Frequently Asked Questions<a class="headerlink" href="#frequently-asked-questions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="how-does-scrapy-compare-to-beautifulsoup-or-lxml">
<h4>How does Scrapy compare to BeautifulSoup or lxml?<a class="headerlink" href="#how-does-scrapy-compare-to-beautifulsoup-or-lxml" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> and <a class="reference external" href="http://lxml.de/">lxml</a> are libraries for parsing HTML and XML. Scrapy is
an application framework for writing web spiders that crawl web sites and
extract data from them.</p>
<p>Scrapy provides a built-in mechanism for extracting data (called
<a class="reference internal" href="index.html#topics-selectors"><em>selectors</em></a>) but you can easily use <a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a>
(or <a class="reference external" href="http://lxml.de/">lxml</a>) instead, if you feel more comfortable working with them. After
all, they&#8217;re just parsing libraries which can be imported and used from any
Python code.</p>
<p>In other words, comparing <a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> (or <a class="reference external" href="http://lxml.de/">lxml</a>) to Scrapy is like
comparing <a class="reference external" href="http://jinja.pocoo.org/2/">jinja2</a> to <a class="reference external" href="http://www.djangoproject.com">Django</a>.</p>
</div>
<div class="section" id="what-python-versions-does-scrapy-support">
<span id="faq-python-versions"></span><h4>What Python versions does Scrapy support?<a class="headerlink" href="#what-python-versions-does-scrapy-support" title="Permalink to this headline">¶</a></h4>
<p>Scrapy is supported under Python 2.7 only.
Python 2.6 support was dropped starting at Scrapy 0.20.</p>
</div>
<div class="section" id="does-scrapy-work-with-python-3">
<h4>Does Scrapy work with Python 3?<a class="headerlink" href="#does-scrapy-work-with-python-3" title="Permalink to this headline">¶</a></h4>
<p>No, but there are plans to support Python 3.3+.
At the moment, Scrapy works with Python 2.7.</p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="index.html#faq-python-versions"><em>What Python versions does Scrapy support?</em></a>.</p>
</div>
</div>
<div class="section" id="did-scrapy-steal-x-from-django">
<h4>Did Scrapy &#8220;steal&#8221; X from Django?<a class="headerlink" href="#did-scrapy-steal-x-from-django" title="Permalink to this headline">¶</a></h4>
<p>Probably, but we don&#8217;t like that word. We think <a class="reference external" href="http://www.djangoproject.com">Django</a> is a great open source
project and an example to follow, so we&#8217;ve used it as an inspiration for
Scrapy.</p>
<p>We believe that, if something is already done well, there&#8217;s no need to reinvent
it. This concept, besides being one of the foundations for open source and free
software, not only applies to software but also to documentation, procedures,
policies, etc. So, instead of going through each problem ourselves, we choose
to copy ideas from those projects that have already solved them properly, and
focus on the real problems we need to solve.</p>
<p>We&#8217;d be proud if Scrapy serves as an inspiration for other projects. Feel free
to steal from us!</p>
</div>
<div class="section" id="does-scrapy-work-with-http-proxies">
<h4>Does Scrapy work with HTTP proxies?<a class="headerlink" href="#does-scrapy-work-with-http-proxies" title="Permalink to this headline">¶</a></h4>
<p>Yes. Support for HTTP proxies is provided (since Scrapy 0.8) through the HTTP
Proxy downloader middleware. See
<a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware" title="scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">HttpProxyMiddleware</span></tt></a>.</p>
</div>
<div class="section" id="how-can-i-scrape-an-item-with-attributes-in-different-pages">
<h4>How can I scrape an item with attributes in different pages?<a class="headerlink" href="#how-can-i-scrape-an-item-with-attributes-in-different-pages" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#topics-request-response-ref-request-callback-arguments"><em>Passing additional data to callback functions</em></a>.</p>
</div>
<div class="section" id="scrapy-crashes-with-importerror-no-module-named-win32api">
<h4>Scrapy crashes with: ImportError: No module named win32api<a class="headerlink" href="#scrapy-crashes-with-importerror-no-module-named-win32api" title="Permalink to this headline">¶</a></h4>
<p>You need to install <a class="reference external" href="http://sourceforge.net/projects/pywin32/">pywin32</a> because of <a class="reference external" href="http://twistedmatrix.com/trac/ticket/3707">this Twisted bug</a>.</p>
</div>
<div class="section" id="how-can-i-simulate-a-user-login-in-my-spider">
<h4>How can I simulate a user login in my spider?<a class="headerlink" href="#how-can-i-simulate-a-user-login-in-my-spider" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#topics-request-response-ref-request-userlogin"><em>Using FormRequest.from_response() to simulate a user login</em></a>.</p>
</div>
<div class="section" id="does-scrapy-crawl-in-breadth-first-or-depth-first-order">
<h4>Does Scrapy crawl in breadth-first or depth-first order?<a class="headerlink" href="#does-scrapy-crawl-in-breadth-first-or-depth-first-order" title="Permalink to this headline">¶</a></h4>
<p>By default, Scrapy uses a <a class="reference external" href="http://en.wikipedia.org/wiki/LIFO">LIFO</a> queue for storing pending requests, which
basically means that it crawls in <a class="reference external" href="http://en.wikipedia.org/wiki/Depth-first_search">DFO order</a>. This order is more convenient
in most cases. If you do want to crawl in true <a class="reference external" href="http://en.wikipedia.org/wiki/Breadth-first_search">BFO order</a>, you can do it by
setting the following settings:</p>
<div class="highlight-none"><div class="highlight"><pre>DEPTH_PRIORITY = 1
SCHEDULER_DISK_QUEUE = &#39;scrapy.squeue.PickleFifoDiskQueue&#39;
SCHEDULER_MEMORY_QUEUE = &#39;scrapy.squeue.FifoMemoryQueue&#39;
</pre></div>
</div>
</div>
<div class="section" id="my-scrapy-crawler-has-memory-leaks-what-can-i-do">
<h4>My Scrapy crawler has memory leaks. What can I do?<a class="headerlink" href="#my-scrapy-crawler-has-memory-leaks-what-can-i-do" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#topics-leaks"><em>Debugging memory leaks</em></a>.</p>
<p>Also, Python has a builtin memory leak issue which is described in
<a class="reference internal" href="index.html#topics-leaks-without-leaks"><em>Leaks without leaks</em></a>.</p>
</div>
<div class="section" id="how-can-i-make-scrapy-consume-less-memory">
<h4>How can I make Scrapy consume less memory?<a class="headerlink" href="#how-can-i-make-scrapy-consume-less-memory" title="Permalink to this headline">¶</a></h4>
<p>See previous question.</p>
</div>
<div class="section" id="can-i-use-basic-http-authentication-in-my-spiders">
<h4>Can I use Basic HTTP Authentication in my spiders?<a class="headerlink" href="#can-i-use-basic-http-authentication-in-my-spiders" title="Permalink to this headline">¶</a></h4>
<p>Yes, see <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware" title="scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">HttpAuthMiddleware</span></tt></a>.</p>
</div>
<div class="section" id="why-does-scrapy-download-pages-in-english-instead-of-my-native-language">
<h4>Why does Scrapy download pages in English instead of my native language?<a class="headerlink" href="#why-does-scrapy-download-pages-in-english-instead-of-my-native-language" title="Permalink to this headline">¶</a></h4>
<p>Try changing the default <a class="reference external" href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4">Accept-Language</a> request header by overriding the
<a class="reference internal" href="index.html#std:setting-DEFAULT_REQUEST_HEADERS"><tt class="xref std std-setting docutils literal"><span class="pre">DEFAULT_REQUEST_HEADERS</span></tt></a> setting.</p>
</div>
<div class="section" id="where-can-i-find-some-example-scrapy-projects">
<h4>Where can I find some example Scrapy projects?<a class="headerlink" href="#where-can-i-find-some-example-scrapy-projects" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#intro-examples"><em>Examples</em></a>.</p>
</div>
<div class="section" id="can-i-run-a-spider-without-creating-a-project">
<h4>Can I run a spider without creating a project?<a class="headerlink" href="#can-i-run-a-spider-without-creating-a-project" title="Permalink to this headline">¶</a></h4>
<p>Yes. You can use the <a class="reference internal" href="index.html#std:command-runspider"><tt class="xref std std-command docutils literal"><span class="pre">runspider</span></tt></a> command. For example, if you have a
spider written in a <tt class="docutils literal"><span class="pre">my_spider.py</span></tt> file you can run it with:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy runspider my_spider.py
</pre></div>
</div>
<p>See <a class="reference internal" href="index.html#std:command-runspider"><tt class="xref std std-command docutils literal"><span class="pre">runspider</span></tt></a> command for more info.</p>
</div>
<div class="section" id="i-get-filtered-offsite-request-messages-how-can-i-fix-them">
<h4>I get &#8220;Filtered offsite request&#8221; messages. How can I fix them?<a class="headerlink" href="#i-get-filtered-offsite-request-messages-how-can-i-fix-them" title="Permalink to this headline">¶</a></h4>
<p>Those messages (logged with <tt class="docutils literal"><span class="pre">DEBUG</span></tt> level) don&#8217;t necessarily mean there is a
problem, so you may not need to fix them.</p>
<p>Those message are thrown by the Offsite Spider Middleware, which is a spider
middleware (enabled by default) whose purpose is to filter out requests to
domains outside the ones covered by the spider.</p>
<p>For more info see:
<a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware" title="scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">OffsiteMiddleware</span></tt></a>.</p>
</div>
<div class="section" id="what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-production">
<h4>What is the recommended way to deploy a Scrapy crawler in production?<a class="headerlink" href="#what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-production" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#topics-scrapyd"><em>Scrapyd</em></a>.</p>
</div>
<div class="section" id="can-i-use-json-for-large-exports">
<h4>Can I use JSON for large exports?<a class="headerlink" href="#can-i-use-json-for-large-exports" title="Permalink to this headline">¶</a></h4>
<p>It&#8217;ll depend on how large your output is. See <a class="reference internal" href="index.html#json-with-large-data"><em>this warning</em></a> in <a class="reference internal" href="index.html#scrapy.contrib.exporter.JsonItemExporter" title="scrapy.contrib.exporter.JsonItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">JsonItemExporter</span></tt></a>
documentation.</p>
</div>
<div class="section" id="can-i-return-twisted-deferreds-from-signal-handlers">
<h4>Can I return (Twisted) deferreds from signal handlers?<a class="headerlink" href="#can-i-return-twisted-deferreds-from-signal-handlers" title="Permalink to this headline">¶</a></h4>
<p>Some signals support returning deferreds from their handlers, others don&#8217;t. See
the <a class="reference internal" href="index.html#topics-signals-ref"><em>Built-in signals reference</em></a> to know which ones.</p>
</div>
<div class="section" id="what-does-the-response-status-code-999-means">
<h4>What does the response status code 999 means?<a class="headerlink" href="#what-does-the-response-status-code-999-means" title="Permalink to this headline">¶</a></h4>
<p>999 is a custom response status code used by Yahoo sites to throttle requests.
Try slowing down the crawling speed by using a download delay of <tt class="docutils literal"><span class="pre">2</span></tt> (or
higher) in your spider:</p>
<div class="highlight-none"><div class="highlight"><pre>class MySpider(CrawlSpider):

    name = &#39;myspider&#39;

    download_delay = 2

    # [ ... rest of the spider code ... ]
</pre></div>
</div>
<p>Or by setting a global download delay in your project with the
<a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></tt></a> setting.</p>
</div>
<div class="section" id="can-i-call-pdb-set-trace-from-my-spiders-to-debug-them">
<h4>Can I call <tt class="docutils literal"><span class="pre">pdb.set_trace()</span></tt> from my spiders to debug them?<a class="headerlink" href="#can-i-call-pdb-set-trace-from-my-spiders-to-debug-them" title="Permalink to this headline">¶</a></h4>
<p>Yes, but you can also use the Scrapy shell which allows you too quickly analyze
(and even modify) the response being processed by your spider, which is, quite
often, more useful than plain old <tt class="docutils literal"><span class="pre">pdb.set_trace()</span></tt>.</p>
<p>For more info see <a class="reference internal" href="index.html#topics-shell-inspect-response"><em>Invoking the shell from spiders to inspect responses</em></a>.</p>
</div>
<div class="section" id="simplest-way-to-dump-all-my-scraped-items-into-a-json-csv-xml-file">
<h4>Simplest way to dump all my scraped items into a JSON/CSV/XML file?<a class="headerlink" href="#simplest-way-to-dump-all-my-scraped-items-into-a-json-csv-xml-file" title="Permalink to this headline">¶</a></h4>
<p>To dump into a JSON file:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy crawl myspider -o items.json
</pre></div>
</div>
<p>To dump into a CSV file:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy crawl myspider -o items.csv
</pre></div>
</div>
<p>To dump into a XML file:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy crawl myspider -o items.xml
</pre></div>
</div>
<p>For more information see <a class="reference internal" href="index.html#topics-feed-exports"><em>Feed exports</em></a></p>
</div>
<div class="section" id="what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms">
<h4>What&#8217;s this huge cryptic <tt class="docutils literal"><span class="pre">__VIEWSTATE</span></tt> parameter used in some forms?<a class="headerlink" href="#what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms" title="Permalink to this headline">¶</a></h4>
<p>The <tt class="docutils literal"><span class="pre">__VIEWSTATE</span></tt> parameter is used in sites built with ASP.NET/VB.NET. For
more info on how it works see <a class="reference external" href="http://search.cpan.org/~ecarroll/HTML-TreeBuilderX-ASP_NET-0.09/lib/HTML/TreeBuilderX/ASP_NET.pm">this page</a>. Also, here&#8217;s an <a class="reference external" href="http://github.com/AmbientLighter/rpn-fas/blob/master/fas/spiders/rnp.py">example spider</a>
which scrapes one of these sites.</p>
</div>
<div class="section" id="what-s-the-best-way-to-parse-big-xml-csv-data-feeds">
<h4>What&#8217;s the best way to parse big XML/CSV data feeds?<a class="headerlink" href="#what-s-the-best-way-to-parse-big-xml-csv-data-feeds" title="Permalink to this headline">¶</a></h4>
<p>Parsing big feeds with XPath selectors can be problematic since they need to
build the DOM of the entire feed in memory, and this can be quite slow and
consume a lot of memory.</p>
<p>In order to avoid parsing all the entire feed at once in memory, you can use
the functions <tt class="docutils literal"><span class="pre">xmliter</span></tt> and <tt class="docutils literal"><span class="pre">csviter</span></tt> from <tt class="docutils literal"><span class="pre">scrapy.utils.iterators</span></tt>
module. In fact, this is what the feed spiders (see <a class="reference internal" href="index.html#topics-spiders"><em>Spiders</em></a>) use
under the cover.</p>
</div>
<div class="section" id="does-scrapy-manage-cookies-automatically">
<h4>Does Scrapy manage cookies automatically?<a class="headerlink" href="#does-scrapy-manage-cookies-automatically" title="Permalink to this headline">¶</a></h4>
<p>Yes, Scrapy receives and keeps track of cookies sent by servers, and sends them
back on subsequent requests, like any regular web browser does.</p>
<p>For more info see <a class="reference internal" href="index.html#topics-request-response"><em>Requests and Responses</em></a> and <a class="reference internal" href="index.html#cookies-mw"><em>CookiesMiddleware</em></a>.</p>
</div>
<div class="section" id="how-can-i-see-the-cookies-being-sent-and-received-from-scrapy">
<h4>How can I see the cookies being sent and received from Scrapy?<a class="headerlink" href="#how-can-i-see-the-cookies-being-sent-and-received-from-scrapy" title="Permalink to this headline">¶</a></h4>
<p>Enable the <a class="reference internal" href="index.html#std:setting-COOKIES_DEBUG"><tt class="xref std std-setting docutils literal"><span class="pre">COOKIES_DEBUG</span></tt></a> setting.</p>
</div>
<div class="section" id="how-can-i-instruct-a-spider-to-stop-itself">
<h4>How can I instruct a spider to stop itself?<a class="headerlink" href="#how-can-i-instruct-a-spider-to-stop-itself" title="Permalink to this headline">¶</a></h4>
<p>Raise the <a class="reference internal" href="index.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider"><tt class="xref py py-exc docutils literal"><span class="pre">CloseSpider</span></tt></a> exception from a callback. For
more info see: <a class="reference internal" href="index.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider"><tt class="xref py py-exc docutils literal"><span class="pre">CloseSpider</span></tt></a>.</p>
</div>
<div class="section" id="how-can-i-prevent-my-scrapy-bot-from-getting-banned">
<h4>How can I prevent my Scrapy bot from getting banned?<a class="headerlink" href="#how-can-i-prevent-my-scrapy-bot-from-getting-banned" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#bans"><em>Avoiding getting banned</em></a>.</p>
</div>
<div class="section" id="should-i-use-spider-arguments-or-settings-to-configure-my-spider">
<h4>Should I use spider arguments or settings to configure my spider?<a class="headerlink" href="#should-i-use-spider-arguments-or-settings-to-configure-my-spider" title="Permalink to this headline">¶</a></h4>
<p>Both <a class="reference internal" href="index.html#spiderargs"><em>spider arguments</em></a> and <a class="reference internal" href="index.html#topics-settings"><em>settings</em></a>
can be used to configure your spider. There is no strict rule that mandates to
use one or the other, but settings are more suited for parameters that, once
set, don&#8217;t change much, while spider arguments are meant to change more often,
even on each spider run and sometimes are required for the spider to run at all
(for example, to set the start url of a spider).</p>
<p>To illustrate with an example, assuming you have a spider that needs to log
into a site to scrape data, and you only want to scrape data from a certain
section of the site (which varies each time). In that case, the credentials to
log in would be settings, while the url of the section to scrape would be a
spider argument.</p>
</div>
<div class="section" id="i-m-scraping-a-xml-document-and-my-xpath-selector-doesn-t-return-any-items">
<h4>I&#8217;m scraping a XML document and my XPath selector doesn&#8217;t return any items<a class="headerlink" href="#i-m-scraping-a-xml-document-and-my-xpath-selector-doesn-t-return-any-items" title="Permalink to this headline">¶</a></h4>
<p>You may need to remove namespaces. See <a class="reference internal" href="index.html#removing-namespaces"><em>Removing namespaces</em></a>.</p>
</div>
<div class="section" id="i-m-getting-an-error-cannot-import-name-crawler">
<h4>I&#8217;m getting an error: &#8220;cannot import name crawler&#8221;<a class="headerlink" href="#i-m-getting-an-error-cannot-import-name-crawler" title="Permalink to this headline">¶</a></h4>
<p>This is caused by Scrapy changes due to the singletons removal. The error is
most likely raised by a module (extension, middleware, pipeline or spider) in
your Scrapy project that imports <tt class="docutils literal"><span class="pre">crawler</span></tt> from <tt class="docutils literal"><span class="pre">scrapy.project</span></tt>. For
example:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.project import crawler

class SomeExtension(object):
    def __init__(self):
        self.crawler = crawler
        # ...
</pre></div>
</div>
<p>This way to access the crawler object is deprecated, the code should be ported
to use <tt class="docutils literal"><span class="pre">from_crawler</span></tt> class method, for example:</p>
<div class="highlight-none"><div class="highlight"><pre>class SomeExtension(object):

    @classmethod
    def from_crawler(cls, crawler):
        o = cls()
        o.crawler = crawler
        return o
</pre></div>
</div>
<p>Scrapy command line tool has some backwards compatibility in place to support
the old import mechanism (with a deprecation warning), but this mechanism may
not work if you use Scrapy differently (for example, as a library).</p>
</div>
</div>
<span id="document-topics/debug"></span><div class="section" id="debugging-spiders">
<span id="topics-debug"></span><h3>Debugging Spiders<a class="headerlink" href="#debugging-spiders" title="Permalink to this headline">¶</a></h3>
<p>This document explains the most common techniques for debugging spiders.
Consider the following scrapy spider below:</p>
<div class="highlight-none"><div class="highlight"><pre>import scrapy
from myproject.items import MyItem

class MySpider(scrapy.Spider):
    name = &#39;myspider&#39;
    start_urls = (
        &#39;http://example.com/page1&#39;,
        &#39;http://example.com/page2&#39;,
        )

    def parse(self, response):
        # collect `item_urls`
        for item_url in item_urls:
            yield scrapy.Request(item_url, self.parse_item)

    def parse_item(self, response):
        item = MyItem()
        # populate `item` fields
        # and extract item_details_url
        yield scrapy.Request(item_details_url, self.parse_details, meta={&#39;item&#39;: item})

    def parse_details(self, response):
        item = response.meta[&#39;item&#39;]
        # populate more `item` fields
        return item
</pre></div>
</div>
<p>Basically this is a simple spider which parses two pages of items (the
start_urls). Items also have a details page with additional information, so we
use the <tt class="docutils literal"><span class="pre">meta</span></tt> functionality of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> to pass a
partially populated item.</p>
<div class="section" id="parse-command">
<h4>Parse Command<a class="headerlink" href="#parse-command" title="Permalink to this headline">¶</a></h4>
<p>The most basic way of checking the output of your spider is to use the
<a class="reference internal" href="index.html#std:command-parse"><tt class="xref std std-command docutils literal"><span class="pre">parse</span></tt></a> command. It allows to check the behaviour of different parts
of the spider at the method level. It has the advantage of being flexible and
simple to use, but does not allow debugging code inside a method.</p>
<p>In order to see the item scraped from a specific url:</p>
<div class="highlight-none"><div class="highlight"><pre>$ scrapy parse --spider=myspider -c parse_item -d 2 &lt;item_url&gt;
[ ... scrapy log lines crawling example.com spider ... ]

&gt;&gt;&gt; STATUS DEPTH LEVEL 2 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{&#39;url&#39;: &lt;item_url&gt;}]

# Requests  -----------------------------------------------------------------
[]
</pre></div>
</div>
<p>Using the <tt class="docutils literal"><span class="pre">--verbose</span></tt> or <tt class="docutils literal"><span class="pre">-v</span></tt> option we can see the status at each depth level:</p>
<div class="highlight-none"><div class="highlight"><pre>$ scrapy parse --spider=myspider -c parse_item -d 2 -v &lt;item_url&gt;
[ ... scrapy log lines crawling example.com spider ... ]

&gt;&gt;&gt; DEPTH LEVEL: 1 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[]

# Requests  -----------------------------------------------------------------
[&lt;GET item_details_url&gt;]


&gt;&gt;&gt; DEPTH LEVEL: 2 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{&#39;url&#39;: &lt;item_url&gt;}]

# Requests  -----------------------------------------------------------------
[]
</pre></div>
</div>
<p>Checking items scraped from a single start_url, can also be easily achieved
using:</p>
<div class="highlight-none"><div class="highlight"><pre>$ scrapy parse --spider=myspider -d 3 &#39;http://example.com/page1&#39;
</pre></div>
</div>
</div>
<div class="section" id="scrapy-shell">
<h4>Scrapy Shell<a class="headerlink" href="#scrapy-shell" title="Permalink to this headline">¶</a></h4>
<p>While the <a class="reference internal" href="index.html#std:command-parse"><tt class="xref std std-command docutils literal"><span class="pre">parse</span></tt></a> command is very useful for checking behaviour of a
spider, it is of little help to check what happens inside a callback, besides
showing the response received and the output. How to debug the situation when
<tt class="docutils literal"><span class="pre">parse_details</span></tt> sometimes receives no item?</p>
<p>Fortunately, the <a class="reference internal" href="index.html#std:command-shell"><tt class="xref std std-command docutils literal"><span class="pre">shell</span></tt></a> is your bread and butter in this case (see
<a class="reference internal" href="index.html#topics-shell-inspect-response"><em>Invoking the shell from spiders to inspect responses</em></a>):</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.shell import inspect_response

def parse_details(self, response):
    item = response.meta.get(&#39;item&#39;, None)
    if item:
        # populate more `item` fields
        return item
    else:
        inspect_response(response, self)
</pre></div>
</div>
<p>See also: <a class="reference internal" href="index.html#topics-shell-inspect-response"><em>Invoking the shell from spiders to inspect responses</em></a>.</p>
</div>
<div class="section" id="open-in-browser">
<h4>Open in browser<a class="headerlink" href="#open-in-browser" title="Permalink to this headline">¶</a></h4>
<p>Sometimes you just want to see how a certain response looks in a browser, you
can use the <tt class="docutils literal"><span class="pre">open_in_browser</span></tt> function for that. Here is an example of how
you would use it:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.utils.response import open_in_browser

def parse_details(self, response):
    if &quot;item name&quot; not in response.body:
        open_in_browser(response)
</pre></div>
</div>
<p><tt class="docutils literal"><span class="pre">open_in_browser</span></tt> will open a browser with the response received by Scrapy at
that point, adjusting the <a class="reference external" href="http://www.w3schools.com/tags/tag_base.asp">base tag</a> so that images and styles are displayed
properly.</p>
</div>
<div class="section" id="logging">
<h4>Logging<a class="headerlink" href="#logging" title="Permalink to this headline">¶</a></h4>
<p>Logging is another useful option for getting information about your spider run.
Although not as convenient, it comes with the advantage that the logs will be
available in all future runs should they be necessary again:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy import log

def parse_details(self, response):
    item = response.meta.get(&#39;item&#39;, None)
    if item:
        # populate more `item` fields
        return item
    else:
        self.log(&#39;No item received for %s&#39; % response.url,
            level=log.WARNING)
</pre></div>
</div>
<p>For more information, check the <a class="reference internal" href="index.html#topics-logging"><em>Logging</em></a> section.</p>
</div>
</div>
<span id="document-topics/contracts"></span><div class="section" id="spiders-contracts">
<span id="topics-contracts"></span><h3>Spiders Contracts<a class="headerlink" href="#spiders-contracts" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is a new feature (introduced in Scrapy 0.15) and may be subject
to minor functionality/API updates. Check the <a class="reference internal" href="index.html#news"><em>release notes</em></a> to
be notified of updates.</p>
</div>
<p>Testing spiders can get particularly annoying and while nothing prevents you
from writing unit tests the task gets cumbersome quickly. Scrapy offers an
integrated way of testing your spiders by the means of contracts.</p>
<p>This allows you to test each callback of your spider by hardcoding a sample url
and check various constraints for how the callback processes the response. Each
contract is prefixed with an <tt class="docutils literal"><span class="pre">&#64;</span></tt> and included in the docstring. See the
following example:</p>
<div class="highlight-none"><div class="highlight"><pre>def parse(self, response):
    &quot;&quot;&quot; This function parses a sample response. Some contracts are mingled
    with this docstring.

    @url http://www.amazon.com/s?field-keywords=selfish+gene
    @returns items 1 16
    @returns requests 0 0
    @scrapes Title Author Year Price
    &quot;&quot;&quot;
</pre></div>
</div>
<p>This callback is tested using three built-in contracts:</p>
<span class="target" id="module-scrapy.contracts.default"></span><dl class="class">
<dt id="scrapy.contracts.default.UrlContract">
<em class="property">class </em><tt class="descclassname">scrapy.contracts.default.</tt><tt class="descname">UrlContract</tt><a class="headerlink" href="#scrapy.contracts.default.UrlContract" title="Permalink to this definition">¶</a></dt>
<dd><p>This contract (<tt class="docutils literal"><span class="pre">&#64;url</span></tt>) sets the sample url used when checking other
contract conditions for this spider. This contract is mandatory. All
callbacks lacking this contract are ignored when running the checks:</p>
<div class="highlight-none"><div class="highlight"><pre>@url url
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.contracts.default.ReturnsContract">
<em class="property">class </em><tt class="descclassname">scrapy.contracts.default.</tt><tt class="descname">ReturnsContract</tt><a class="headerlink" href="#scrapy.contracts.default.ReturnsContract" title="Permalink to this definition">¶</a></dt>
<dd><p>This contract (<tt class="docutils literal"><span class="pre">&#64;returns</span></tt>) sets lower and upper bounds for the items and
requests returned by the spider. The upper bound is optional:</p>
<div class="highlight-none"><div class="highlight"><pre>@returns item(s)|request(s) [min [max]]
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.contracts.default.ScrapesContract">
<em class="property">class </em><tt class="descclassname">scrapy.contracts.default.</tt><tt class="descname">ScrapesContract</tt><a class="headerlink" href="#scrapy.contracts.default.ScrapesContract" title="Permalink to this definition">¶</a></dt>
<dd><p>This contract (<tt class="docutils literal"><span class="pre">&#64;scrapes</span></tt>) checks that all the items returned by the
callback have the specified fields:</p>
<div class="highlight-none"><div class="highlight"><pre>@scrapes field_1 field_2 ...
</pre></div>
</div>
</dd></dl>

<p>Use the <a class="reference internal" href="index.html#std:command-check"><tt class="xref std std-command docutils literal"><span class="pre">check</span></tt></a> command to run the contract checks.</p>
<div class="section" id="custom-contracts">
<h4>Custom Contracts<a class="headerlink" href="#custom-contracts" title="Permalink to this headline">¶</a></h4>
<p>If you find you need more power than the built-in scrapy contracts you can
create and load your own contracts in the project by using the
<a class="reference internal" href="index.html#std:setting-SPIDER_CONTRACTS"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_CONTRACTS</span></tt></a> setting:</p>
<div class="highlight-none"><div class="highlight"><pre>SPIDER_CONTRACTS = {
    &#39;myproject.contracts.ResponseCheck&#39;: 10,
    &#39;myproject.contracts.ItemValidate&#39;: 10,
}
</pre></div>
</div>
<p>Each contract must inherit from <a class="reference internal" href="index.html#scrapy.contracts.Contract" title="scrapy.contracts.Contract"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.contracts.Contract</span></tt></a> and can
override three methods:</p>
<span class="target" id="module-scrapy.contracts"></span><dl class="class">
<dt id="scrapy.contracts.Contract">
<em class="property">class </em><tt class="descclassname">scrapy.contracts.</tt><tt class="descname">Contract</tt><big>(</big><em>method</em>, <em>*args</em><big>)</big><a class="headerlink" href="#scrapy.contracts.Contract" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>method</strong> (<em>function</em>) &#8211; callback function to which the contract is associated</li>
<li><strong>args</strong> (<em>list</em>) &#8211; list of arguments passed into the docstring (whitespace
separated)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="scrapy.contracts.Contract.adjust_request_args">
<tt class="descname">adjust_request_args</tt><big>(</big><em>args</em><big>)</big><a class="headerlink" href="#scrapy.contracts.Contract.adjust_request_args" title="Permalink to this definition">¶</a></dt>
<dd><p>This receives a <tt class="docutils literal"><span class="pre">dict</span></tt> as an argument containing default arguments
for <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object. Must return the same or a
modified version of it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contracts.Contract.pre_process">
<tt class="descname">pre_process</tt><big>(</big><em>response</em><big>)</big><a class="headerlink" href="#scrapy.contracts.Contract.pre_process" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows hooking in various checks on the response received from the
sample request, before it&#8217;s being passed to the callback.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contracts.Contract.post_process">
<tt class="descname">post_process</tt><big>(</big><em>output</em><big>)</big><a class="headerlink" href="#scrapy.contracts.Contract.post_process" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows processing the output of the callback. Iterators are
converted listified before being passed to this hook.</p>
</dd></dl>

</dd></dl>

<p>Here is a demo contract which checks the presence of a custom header in the
response received. Raise <tt class="xref py py-class docutils literal"><span class="pre">scrapy.exceptions.ContractFail</span></tt> in order to
get the failures pretty printed:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.contracts import Contract
from scrapy.exceptions import ContractFail

class HasHeaderContract(Contract):
    &quot;&quot;&quot; Demo contract which checks the presence of a custom header
        @has_header X-CustomHeader
    &quot;&quot;&quot;

    name = &#39;has_header&#39;

    def pre_process(self, response):
        for header in self.args:
            if header not in response.headers:
                raise ContractFail(&#39;X-CustomHeader not present&#39;)
</pre></div>
</div>
</div>
</div>
<span id="document-topics/practices"></span><div class="section" id="common-practices">
<span id="topics-practices"></span><h3>Common Practices<a class="headerlink" href="#common-practices" title="Permalink to this headline">¶</a></h3>
<p>This section documents common practices when using Scrapy. These are things
that cover many topics and don&#8217;t often fall into any other specific section.</p>
<div class="section" id="run-scrapy-from-a-script">
<span id="run-from-script"></span><h4>Run Scrapy from a script<a class="headerlink" href="#run-scrapy-from-a-script" title="Permalink to this headline">¶</a></h4>
<p>You can use the <a class="reference internal" href="index.html#topics-api"><em>API</em></a> to run Scrapy from a script, instead of
the typical way of running Scrapy via <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">crawl</span></tt>.</p>
<p>Remember that Scrapy is built on top of the Twisted
asynchronous networking library, so you need to run it inside the Twisted reactor.</p>
<p>Note that you will also have to shutdown the Twisted reactor yourself after the
spider is finished. This can be achieved by connecting a handler to the
<tt class="docutils literal"><span class="pre">signals.spider_closed</span></tt> signal.</p>
<p>What follows is a working example of how to do that, using the <a class="reference external" href="https://github.com/scrapinghub/testspiders">testspiders</a>
project as example.</p>
<div class="highlight-none"><div class="highlight"><pre>from twisted.internet import reactor
from scrapy.crawler import Crawler
from scrapy import log, signals
from testspiders.spiders.followall import FollowAllSpider
from scrapy.utils.project import get_project_settings

spider = FollowAllSpider(domain=&#39;scrapinghub.com&#39;)
settings = get_project_settings()
crawler = Crawler(settings)
crawler.signals.connect(reactor.stop, signal=signals.spider_closed)
crawler.configure()
crawler.crawl(spider)
crawler.start()
log.start()
reactor.run() # the script will block here until the spider_closed signal was sent
</pre></div>
</div>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference external" href="http://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">Twisted Reactor Overview</a>.</p>
</div>
</div>
<div class="section" id="running-multiple-spiders-in-the-same-process">
<h4>Running multiple spiders in the same process<a class="headerlink" href="#running-multiple-spiders-in-the-same-process" title="Permalink to this headline">¶</a></h4>
<p>By default, Scrapy runs a single spider per process when you run <tt class="docutils literal"><span class="pre">scrapy</span>
<span class="pre">crawl</span></tt>. However, Scrapy supports running multiple spiders per process using
the <a class="reference internal" href="index.html#topics-api"><em>internal API</em></a>.</p>
<p>Here is an example, using the <a class="reference external" href="https://github.com/scrapinghub/testspiders">testspiders</a> project:</p>
<div class="highlight-none"><div class="highlight"><pre>from twisted.internet import reactor
from scrapy.crawler import Crawler
from scrapy import log
from testspiders.spiders.followall import FollowAllSpider
from scrapy.utils.project import get_project_settings

def setup_crawler(domain):
    spider = FollowAllSpider(domain=domain)
    settings = get_project_settings()
    crawler = Crawler(settings)
    crawler.configure()
    crawler.crawl(spider)
    crawler.start()

for domain in [&#39;scrapinghub.com&#39;, &#39;insophia.com&#39;]:
    setup_crawler(domain)
log.start()
reactor.run()
</pre></div>
</div>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="index.html#run-from-script"><em>Run Scrapy from a script</em></a>.</p>
</div>
</div>
<div class="section" id="distributed-crawls">
<span id="id1"></span><h4>Distributed crawls<a class="headerlink" href="#distributed-crawls" title="Permalink to this headline">¶</a></h4>
<p>Scrapy doesn&#8217;t provide any built-in facility for running crawls in a distribute
(multi-server) manner. However, there are some ways to distribute crawls, which
vary depending on how you plan to distribute them.</p>
<p>If you have many spiders, the obvious way to distribute the load is to setup
many Scrapyd instances and distribute spider runs among those.</p>
<p>If you instead want to run a single (big) spider through many machines, what
you usually do is partition the urls to crawl and send them to each separate
spider. Here is a concrete example:</p>
<p>First, you prepare the list of urls to crawl and put them into separate
files/urls:</p>
<div class="highlight-none"><div class="highlight"><pre>http://somedomain.com/urls-to-crawl/spider1/part1.list
http://somedomain.com/urls-to-crawl/spider1/part2.list
http://somedomain.com/urls-to-crawl/spider1/part3.list
</pre></div>
</div>
<p>Then you fire a spider run on 3 different Scrapyd servers. The spider would
receive a (spider) argument <tt class="docutils literal"><span class="pre">part</span></tt> with the number of the partition to
crawl:</p>
<div class="highlight-none"><div class="highlight"><pre>curl http://scrapy1.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=1
curl http://scrapy2.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=2
curl http://scrapy3.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=3
</pre></div>
</div>
</div>
<div class="section" id="avoiding-getting-banned">
<span id="bans"></span><h4>Avoiding getting banned<a class="headerlink" href="#avoiding-getting-banned" title="Permalink to this headline">¶</a></h4>
<p>Some websites implement certain measures to prevent bots from crawling them,
with varying degrees of sophistication. Getting around those measures can be
difficult and tricky, and may sometimes require special infrastructure. Please
consider contacting <a class="reference external" href="http://scrapy.org/support/">commercial support</a> if in doubt.</p>
<p>Here are some tips to keep in mind when dealing with these kind of sites:</p>
<ul class="simple">
<li>rotate your user agent from a pool of well-known ones from browsers (google
around to get a list of them)</li>
<li>disable cookies (see <a class="reference internal" href="index.html#std:setting-COOKIES_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">COOKIES_ENABLED</span></tt></a>) as some sites may use
cookies to spot bot behaviour</li>
<li>use download delays (2 or higher). See <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></tt></a> setting.</li>
<li>if possible, use <a class="reference external" href="http://www.googleguide.com/cached_pages.html">Google cache</a> to fetch pages, instead of hitting the sites
directly</li>
<li>use a pool of rotating IPs. For example, the free <a class="reference external" href="https://www.torproject.org/">Tor project</a> or paid
services like <a class="reference external" href="http://proxymesh.com/">ProxyMesh</a></li>
<li>use a highly distributed downloader that circumvents bans internally, so you
can just focus on parsing clean pages. One example of such downloaders is
<a class="reference external" href="http://crawlera.com">Crawlera</a></li>
</ul>
<p>If you are still unable to prevent your bot getting banned, consider contacting
<a class="reference external" href="http://scrapy.org/support/">commercial support</a>.</p>
</div>
<div class="section" id="dynamic-creation-of-item-classes">
<span id="dynamic-item-classes"></span><h4>Dynamic Creation of Item Classes<a class="headerlink" href="#dynamic-creation-of-item-classes" title="Permalink to this headline">¶</a></h4>
<p>For applications in which the structure of item class is to be determined by
user input, or other changing conditions, you can dynamically create item
classes instead of manually coding them.</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.item import DictItem, Field

def create_item_class(class_name, field_list):
    field_dict = {}
    for field_name in field_list:
        field_dict[field_name] = Field()

    return type(class_name, (DictItem,), field_dict)
</pre></div>
</div>
</div>
</div>
<span id="document-topics/broad-crawls"></span><div class="section" id="broad-crawls">
<span id="topics-broad-crawls"></span><h3>Broad Crawls<a class="headerlink" href="#broad-crawls" title="Permalink to this headline">¶</a></h3>
<p>Scrapy defaults are optimized for crawling specific sites. These sites are
often handled by a single Scrapy spider, although this is not necessary or
required (for example, there are generic spiders that handle any given site
thrown at them).</p>
<p>In addition to this &#8220;focused crawl&#8221;, there is another common type of crawling
which covers a large (potentially unlimited) number of domains, and is only
limited by time or other arbitrary constraint, rather than stopping when the
domain was crawled to completion or when there are no more requests to perform.
These are called &#8220;broad crawls&#8221; and is the typical crawlers employed by search
engines.</p>
<p>These are some common properties often found in broad crawls:</p>
<ul class="simple">
<li>they crawl many domains (often, unbounded) instead of a specific set of sites</li>
<li>they don&#8217;t necessarily crawl domains to completion, because it would
impractical (or impossible) to do so, and instead limit the crawl by time or
number of pages crawled</li>
<li>they are simpler in logic (as opposed to very complex spiders with many
extraction rules) because data is often post-processed in a separate stage</li>
<li>they crawl many domains concurrently, which allows them to achieve faster
crawl speeds by not being limited by any particular site constraint (each site
is crawled slowly to respect politeness, but many sites are crawled in
parallel)</li>
</ul>
<p>As said above, Scrapy default settings are optimized for focused crawls, not
broad crawls. However, due to its asynchronous architecture, Scrapy is very
well suited for performing fast broad crawls. This page summarize some things
you need to keep in mind when using Scrapy for doing broad crawls, along with
concrete suggestions of Scrapy settings to tune in order to achieve an
efficient broad crawl.</p>
<div class="section" id="increase-concurrency">
<h4>Increase concurrency<a class="headerlink" href="#increase-concurrency" title="Permalink to this headline">¶</a></h4>
<p>Concurrency is the number of requests that are processed in parallel. There is
a global limit and a per-domain limit.</p>
<p>The default global concurrency limit in Scrapy is not suitable for crawling
many different  domains in parallel, so you will want to increase it. How much
to increase it will depend on how much CPU you crawler will have available. A
good starting point is <tt class="docutils literal"><span class="pre">100</span></tt>, but the best way to find out is by doing some
trials and identifying at what concurrency your Scrapy process gets CPU
bounded. For optimum performance, You should pick a concurrency where CPU usage
is at 80-90%.</p>
<p>To increase the global concurrency use:</p>
<div class="highlight-none"><div class="highlight"><pre>CONCURRENT_REQUESTS = 100
</pre></div>
</div>
</div>
<div class="section" id="reduce-log-level">
<h4>Reduce log level<a class="headerlink" href="#reduce-log-level" title="Permalink to this headline">¶</a></h4>
<p>When doing broad crawls you are often only interested in the crawl rates you
get and any errors found. These stats are reported by Scrapy when using the
<tt class="docutils literal"><span class="pre">INFO</span></tt> log level. In order to save CPU (and log storage requirements) you
should not use <tt class="docutils literal"><span class="pre">DEBUG</span></tt> log level when preforming large broad crawls in
production. Using <tt class="docutils literal"><span class="pre">DEBUG</span></tt> level when developing your (broad) crawler may fine
though.</p>
<p>To set the log level use:</p>
<div class="highlight-none"><div class="highlight"><pre>LOG_LEVEL = &#39;INFO&#39;
</pre></div>
</div>
</div>
<div class="section" id="disable-cookies">
<h4>Disable cookies<a class="headerlink" href="#disable-cookies" title="Permalink to this headline">¶</a></h4>
<p>Disable cookies unless you <em>really</em> need. Cookies are often not needed when
doing broad crawls (search engine crawlers ignore them), and they improve
performance by saving some CPU cycles and reducing the memory footprint of your
Scrapy crawler.</p>
<p>To disable cookies use:</p>
<div class="highlight-none"><div class="highlight"><pre>COOKIES_ENABLED = False
</pre></div>
</div>
</div>
<div class="section" id="disable-retries">
<h4>Disable retries<a class="headerlink" href="#disable-retries" title="Permalink to this headline">¶</a></h4>
<p>Retrying failed HTTP requests can slow down the crawls substantially, specially
when sites causes are very slow (or fail) to respond, thus causing a timeout
error which gets retried many times, unnecessarily, preventing crawler capacity
to be reused for other domains.</p>
<p>To disable retries use:</p>
<div class="highlight-none"><div class="highlight"><pre>RETRY_ENABLED = False
</pre></div>
</div>
</div>
<div class="section" id="reduce-download-timeout">
<h4>Reduce download timeout<a class="headerlink" href="#reduce-download-timeout" title="Permalink to this headline">¶</a></h4>
<p>Unless you are crawling from a very slow connection (which shouldn&#8217;t be the
case for broad crawls) reduce the download timeout so that stuck requests are
discarded quickly and free up capacity to process the next ones.</p>
<p>To reduce the download timeout use:</p>
<div class="highlight-none"><div class="highlight"><pre>DOWNLOAD_TIMEOUT = 15
</pre></div>
</div>
</div>
<div class="section" id="disable-redirects">
<h4>Disable redirects<a class="headerlink" href="#disable-redirects" title="Permalink to this headline">¶</a></h4>
<p>Consider disabling redirects, unless you are interested in following them. When
doing broad crawls it&#8217;s common to save redirects and resolve them when
revisiting the site at a later crawl. This also help to keep the number of
request constant per crawl batch, otherwise redirect loops may cause the
crawler to dedicate too many resources on any specific domain.</p>
<p>To disable redirects use:</p>
<div class="highlight-none"><div class="highlight"><pre>REDIRECT_ENABLED = False
</pre></div>
</div>
</div>
<div class="section" id="enable-crawling-of-ajax-crawlable-pages">
<h4>Enable crawling of &#8220;Ajax Crawlable Pages&#8221;<a class="headerlink" href="#enable-crawling-of-ajax-crawlable-pages" title="Permalink to this headline">¶</a></h4>
<p>Some pages (up to 1%, based on empirical data from year 2013) declare
themselves as <a class="reference external" href="https://developers.google.com/webmasters/ajax-crawling/docs/getting-started">ajax crawlable</a>. This means they provide plain HTML
version of content that is usually available only via AJAX.
Pages can indicate it in two ways:</p>
<ol class="arabic simple">
<li>by using <tt class="docutils literal"><span class="pre">#!</span></tt> in URL - this is the default way;</li>
<li>by using a special meta tag - this way is used on
&#8220;main&#8221;, &#8220;index&#8221; website pages.</li>
</ol>
<p>Scrapy handles (1) automatically; to handle (2) enable
<a class="reference internal" href="index.html#ajaxcrawl-middleware"><em>AjaxCrawlMiddleware</em></a>:</p>
<div class="highlight-none"><div class="highlight"><pre>AJAXCRAWL_ENABLED = True
</pre></div>
</div>
<p>When doing broad crawls it&#8217;s common to crawl a lot of &#8220;index&#8221; web pages;
AjaxCrawlMiddleware helps to crawl them correctly.
It is turned OFF by default because it has some performance overhead,
and enabling it for focused crawls doesn&#8217;t make much sense.</p>
</div>
</div>
<span id="document-topics/firefox"></span><div class="section" id="using-firefox-for-scraping">
<span id="topics-firefox"></span><h3>Using Firefox for scraping<a class="headerlink" href="#using-firefox-for-scraping" title="Permalink to this headline">¶</a></h3>
<p>Here is a list of tips and advice on using Firefox for scraping, along with a
list of useful Firefox add-ons to ease the scraping process.</p>
<div class="section" id="caveats-with-inspecting-the-live-browser-dom">
<span id="topics-firefox-livedom"></span><h4>Caveats with inspecting the live browser DOM<a class="headerlink" href="#caveats-with-inspecting-the-live-browser-dom" title="Permalink to this headline">¶</a></h4>
<p>Since Firefox add-ons operate on a live browser DOM, what you&#8217;ll actually see
when inspecting the page source is not the original HTML, but a modified one
after applying some browser clean up and executing Javascript code.  Firefox,
in particular, is known for adding <tt class="docutils literal"><span class="pre">&lt;tbody&gt;</span></tt> elements to tables.  Scrapy, on
the other hand, does not modify the original page HTML, so you won&#8217;t be able to
extract any data if you use <tt class="docutils literal"><span class="pre">&lt;tbody</span></tt> in your XPath expressions.</p>
<p>Therefore, you should keep in mind the following things when working with
Firefox and XPath:</p>
<ul class="simple">
<li>Disable Firefox Javascript while inspecting the DOM looking for XPaths to be
used in Scrapy</li>
<li>Never use full XPath paths, use relative and clever ones based on attributes
(such as <tt class="docutils literal"><span class="pre">id</span></tt>, <tt class="docutils literal"><span class="pre">class</span></tt>, <tt class="docutils literal"><span class="pre">width</span></tt>, etc) or any identifying features like
<tt class="docutils literal"><span class="pre">contains(&#64;href,</span> <span class="pre">'image')</span></tt>.</li>
<li>Never include <tt class="docutils literal"><span class="pre">&lt;tbody&gt;</span></tt> elements in your XPath expressions unless you
really know what you&#8217;re doing</li>
</ul>
</div>
<div class="section" id="useful-firefox-add-ons-for-scraping">
<span id="topics-firefox-addons"></span><h4>Useful Firefox add-ons for scraping<a class="headerlink" href="#useful-firefox-add-ons-for-scraping" title="Permalink to this headline">¶</a></h4>
<div class="section" id="firebug">
<h5>Firebug<a class="headerlink" href="#firebug" title="Permalink to this headline">¶</a></h5>
<p><a class="reference external" href="http://getfirebug.com">Firebug</a> is a widely known tool among web developers and it&#8217;s also very
useful for scraping. In particular, its <a class="reference external" href="http://www.youtube.com/watch?v=-pT_pDe54aA">Inspect Element</a> feature comes very
handy when you need to construct the XPaths for extracting data because it
allows you to view the HTML code of each page element while moving your mouse
over it.</p>
<p>See <a class="reference internal" href="index.html#topics-firebug"><em>Using Firebug for scraping</em></a> for a detailed guide on how to use Firebug with
Scrapy.</p>
</div>
<div class="section" id="xpather">
<h5>XPather<a class="headerlink" href="#xpather" title="Permalink to this headline">¶</a></h5>
<p><a class="reference external" href="https://addons.mozilla.org/firefox/addon/1192">XPather</a> allows you to test XPath expressions directly on the pages.</p>
</div>
<div class="section" id="xpath-checker">
<h5>XPath Checker<a class="headerlink" href="#xpath-checker" title="Permalink to this headline">¶</a></h5>
<p><a class="reference external" href="https://addons.mozilla.org/firefox/addon/1095">XPath Checker</a> is another Firefox add-on for testing XPaths on your pages.</p>
</div>
<div class="section" id="tamper-data">
<h5>Tamper Data<a class="headerlink" href="#tamper-data" title="Permalink to this headline">¶</a></h5>
<p><a class="reference external" href="http://addons.mozilla.org/firefox/addon/966">Tamper Data</a> is a Firefox add-on which allows you to view and modify the HTTP
request headers sent by Firefox. Firebug also allows to view HTTP headers, but
not to modify them.</p>
</div>
<div class="section" id="firecookie">
<h5>Firecookie<a class="headerlink" href="#firecookie" title="Permalink to this headline">¶</a></h5>
<p><a class="reference external" href="https://addons.mozilla.org/firefox/addon/6683">Firecookie</a> makes it easier to view and manage cookies. You can use this
extension to create a new cookie, delete existing cookies, see a list of cookies
for the current site, manage cookies permissions and a lot more.</p>
</div>
</div>
</div>
<span id="document-topics/firebug"></span><div class="section" id="using-firebug-for-scraping">
<span id="topics-firebug"></span><h3>Using Firebug for scraping<a class="headerlink" href="#using-firebug-for-scraping" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Google Directory, the example website used in this guide is no longer
available as it <a class="reference external" href="http://searchenginewatch.com/article/2096661/Google-Directory-Has-Been-Shut-Down">has been shut down by Google</a>. The concepts in this guide
are still valid though. If you want to update this guide to use a new
(working) site, your contribution will be more than welcome!. See <a class="reference internal" href="index.html#topics-contributing"><em>Contributing to Scrapy</em></a>
for information on how to do so.</p>
</div>
<div class="section" id="introduction">
<h4>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h4>
<p>This document explains how to use <a class="reference external" href="http://getfirebug.com">Firebug</a> (a Firefox add-on) to make the
scraping process easier and more fun. For other useful Firefox add-ons see
<a class="reference internal" href="index.html#topics-firefox-addons"><em>Useful Firefox add-ons for scraping</em></a>. There are some caveats with using Firefox add-ons
to inspect pages, see <a class="reference internal" href="index.html#topics-firefox-livedom"><em>Caveats with inspecting the live browser DOM</em></a>.</p>
<p>In this example, we&#8217;ll show how to use <a class="reference external" href="http://getfirebug.com">Firebug</a> to scrape data from the
<a class="reference external" href="http://directory.google.com/">Google Directory</a>, which contains the same data as the <a class="reference external" href="http://www.dmoz.org">Open Directory
Project</a> used in the <a class="reference internal" href="index.html#intro-tutorial"><em>tutorial</em></a> but with a different
face.</p>
<p>Firebug comes with a very useful feature called <a class="reference external" href="http://www.youtube.com/watch?v=-pT_pDe54aA">Inspect Element</a> which allows
you to inspect the HTML code of the different page elements just by hovering
your mouse over them. Otherwise you would have to search for the tags manually
through the HTML body which can be a very tedious task.</p>
<p>In the following screenshot you can see the <a class="reference external" href="http://www.youtube.com/watch?v=-pT_pDe54aA">Inspect Element</a> tool in action.</p>
<a class="reference internal image-reference" href="_images/firebug1.png"><img alt="Inspecting elements with Firebug" src="_images/firebug1.png" style="width: 913px; height: 600px;" /></a>
<p>At first sight, we can see that the directory is divided in categories, which
are also divided in subcategories.</p>
<p>However, it seems that there are more subcategories than the ones being shown
in this page, so we&#8217;ll keep looking:</p>
<a class="reference internal image-reference" href="_images/firebug2.png"><img alt="Inspecting elements with Firebug" src="_images/firebug2.png" style="width: 819px; height: 629px;" /></a>
<p>As expected, the subcategories contain links to other subcategories, and also
links to actual websites, which is the purpose of the directory.</p>
</div>
<div class="section" id="getting-links-to-follow">
<h4>Getting links to follow<a class="headerlink" href="#getting-links-to-follow" title="Permalink to this headline">¶</a></h4>
<p>By looking at the category URLs we can see they share a pattern:</p>
<blockquote>
<div><a class="reference external" href="http://directory.google.com/Category/Subcategory/Another_Subcategory">http://directory.google.com/Category/Subcategory/Another_Subcategory</a></div></blockquote>
<p>Once we know that, we are able to construct a regular expression to follow
those links. For example, the following one:</p>
<div class="highlight-none"><div class="highlight"><pre>directory\.google\.com/[A-Z][a-zA-Z_/]+$
</pre></div>
</div>
<p>So, based on that regular expression we can create the first crawling rule:</p>
<div class="highlight-none"><div class="highlight"><pre>Rule(LinkExtractor(allow=&#39;directory.google.com/[A-Z][a-zA-Z_/]+$&#39;, ),
    &#39;parse_category&#39;,
    follow=True,
),
</pre></div>
</div>
<p>The <a class="reference internal" href="index.html#scrapy.contrib.spiders.Rule" title="scrapy.contrib.spiders.Rule"><tt class="xref py py-class docutils literal"><span class="pre">Rule</span></tt></a> object instructs
<a class="reference internal" href="index.html#scrapy.contrib.spiders.CrawlSpider" title="scrapy.contrib.spiders.CrawlSpider"><tt class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></tt></a> based spiders how to follow the
category links. <tt class="docutils literal"><span class="pre">parse_category</span></tt> will be a method of the spider which will
process and extract data from those pages.</p>
<p>This is how the spider would look so far:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.contrib.linkextractors import LinkExtractor
from scrapy.contrib.spiders import CrawlSpider, Rule

class GoogleDirectorySpider(CrawlSpider):
    name = &#39;directory.google.com&#39;
    allowed_domains = [&#39;directory.google.com&#39;]
    start_urls = [&#39;http://directory.google.com/&#39;]

    rules = (
        Rule(LinkExtractor(allow=&#39;directory\.google\.com/[A-Z][a-zA-Z_/]+$&#39;),
            &#39;parse_category&#39;, follow=True,
        ),
    )

    def parse_category(self, response):
        # write the category page data extraction code here
        pass
</pre></div>
</div>
</div>
<div class="section" id="extracting-the-data">
<h4>Extracting the data<a class="headerlink" href="#extracting-the-data" title="Permalink to this headline">¶</a></h4>
<p>Now we&#8217;re going to write the code to extract data from those pages.</p>
<p>With the help of Firebug, we&#8217;ll take a look at some page containing links to
websites (say <a class="reference external" href="http://directory.google.com/Top/Arts/Awards/">http://directory.google.com/Top/Arts/Awards/</a>) and find out how we can
extract those links using <a class="reference internal" href="index.html#topics-selectors"><em>Selectors</em></a>. We&#8217;ll also
use the <a class="reference internal" href="index.html#topics-shell"><em>Scrapy shell</em></a> to test those XPath&#8217;s and make sure
they work as we expect.</p>
<a class="reference internal image-reference" href="_images/firebug3.png"><img alt="Inspecting elements with Firebug" src="_images/firebug3.png" style="width: 965px; height: 751px;" /></a>
<p>As you can see, the page markup is not very descriptive: the elements don&#8217;t
contain <tt class="docutils literal"><span class="pre">id</span></tt>, <tt class="docutils literal"><span class="pre">class</span></tt> or any attribute that clearly identifies them, so
we&#8217;&#8216;ll use the ranking bars as a reference point to select the data to extract
when we construct our XPaths.</p>
<p>After using FireBug, we can see that each link is inside a <tt class="docutils literal"><span class="pre">td</span></tt> tag, which is
itself inside a <tt class="docutils literal"><span class="pre">tr</span></tt> tag that also contains the link&#8217;s ranking bar (in
another <tt class="docutils literal"><span class="pre">td</span></tt>).</p>
<p>So we can select the ranking bar, then find its parent (the <tt class="docutils literal"><span class="pre">tr</span></tt>), and then
finally, the link&#8217;s <tt class="docutils literal"><span class="pre">td</span></tt> (which contains the data we want to scrape).</p>
<p>This results in the following XPath:</p>
<div class="highlight-none"><div class="highlight"><pre>//td[descendant::a[contains(@href, &quot;#pagerank&quot;)]]/following-sibling::td//a
</pre></div>
</div>
<p>It&#8217;s important to use the <a class="reference internal" href="index.html#topics-shell"><em>Scrapy shell</em></a> to test these
complex XPath expressions and make sure they work as expected.</p>
<p>Basically, that expression will look for the ranking bar&#8217;s <tt class="docutils literal"><span class="pre">td</span></tt> element, and
then select any <tt class="docutils literal"><span class="pre">td</span></tt> element who has a descendant <tt class="docutils literal"><span class="pre">a</span></tt> element whose
<tt class="docutils literal"><span class="pre">href</span></tt> attribute contains the string <tt class="docutils literal"><span class="pre">#pagerank</span></tt>&#8220;</p>
<p>Of course, this is not the only XPath, and maybe not the simpler one to select
that data. Another approach could be, for example, to find any <tt class="docutils literal"><span class="pre">font</span></tt> tags
that have that grey colour of the links,</p>
<p>Finally, we can write our <tt class="docutils literal"><span class="pre">parse_category()</span></tt> method:</p>
<div class="highlight-none"><div class="highlight"><pre>def parse_category(self, response):
    # The path to website links in directory page
    links = response.xpath(&#39;//td[descendant::a[contains(@href, &quot;#pagerank&quot;)]]/following-sibling::td/font&#39;)

    for link in links:
        item = DirectoryItem()
        item[&#39;name&#39;] = link.xpath(&#39;a/text()&#39;).extract()
        item[&#39;url&#39;] = link.xpath(&#39;a/@href&#39;).extract()
        item[&#39;description&#39;] = link.xpath(&#39;font[2]/text()&#39;).extract()
        yield item
</pre></div>
</div>
<p>Be aware that you may find some elements which appear in Firebug but
not in the original HTML, such as the typical case of <tt class="docutils literal"><span class="pre">&lt;tbody&gt;</span></tt>
elements.</p>
<p>or tags which Therefer   in page HTML
sources may on Firebug inspects the live DOM</p>
</div>
</div>
<span id="document-topics/leaks"></span><div class="section" id="debugging-memory-leaks">
<span id="topics-leaks"></span><h3>Debugging memory leaks<a class="headerlink" href="#debugging-memory-leaks" title="Permalink to this headline">¶</a></h3>
<p>In Scrapy, objects such as Requests, Responses and Items have a finite
lifetime: they are created, used for a while, and finally destroyed.</p>
<p>From all those objects, the Request is probably the one with the longest
lifetime, as it stays waiting in the Scheduler queue until it&#8217;s time to process
it. For more info see <a class="reference internal" href="index.html#topics-architecture"><em>Architecture overview</em></a>.</p>
<p>As these Scrapy objects have a (rather long) lifetime, there is always the risk
of accumulating them in memory without releasing them properly and thus causing
what is known as a &#8220;memory leak&#8221;.</p>
<p>To help debugging memory leaks, Scrapy provides a built-in mechanism for
tracking objects references called <a class="reference internal" href="index.html#topics-leaks-trackrefs"><em>trackref</em></a>,
and you can also use a third-party library called <a class="reference internal" href="index.html#topics-leaks-guppy"><em>Guppy</em></a> for more advanced memory debugging (see below for more
info). Both mechanisms must be used from the <a class="reference internal" href="index.html#topics-telnetconsole"><em>Telnet Console</em></a>.</p>
<div class="section" id="common-causes-of-memory-leaks">
<h4>Common causes of memory leaks<a class="headerlink" href="#common-causes-of-memory-leaks" title="Permalink to this headline">¶</a></h4>
<p>It happens quite often (sometimes by accident, sometimes on purpose) that the
Scrapy developer passes objects referenced in Requests (for example, using the
<a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">meta</span></tt></a> attribute or the request callback function)
and that effectively bounds the lifetime of those referenced objects to the
lifetime of the Request. This is, by far, the most common cause of memory leaks
in Scrapy projects, and a quite difficult one to debug for newcomers.</p>
<p>In big projects, the spiders are typically written by different people and some
of those spiders could be &#8220;leaking&#8221; and thus affecting the rest of the other
(well-written) spiders when they get to run concurrently, which, in turn,
affects the whole crawling process.</p>
<p>At the same time, it&#8217;s hard to avoid the reasons that cause these leaks
without restricting the power of the framework, so we have decided not to
restrict the functionally but provide useful tools for debugging these leaks,
which quite often consist in an answer to the question: <em>which spider is leaking?</em>.</p>
<p>The leak could also come from a custom middleware, pipeline or extension that
you have written, if you are not releasing the (previously allocated) resources
properly. For example, if you&#8217;re allocating resources on
<a class="reference internal" href="index.html#std:signal-spider_opened"><tt class="xref std std-signal docutils literal"><span class="pre">spider_opened</span></tt></a> but not releasing them on <a class="reference internal" href="index.html#std:signal-spider_closed"><tt class="xref std std-signal docutils literal"><span class="pre">spider_closed</span></tt></a>.</p>
</div>
<div class="section" id="debugging-memory-leaks-with-trackref">
<span id="topics-leaks-trackrefs"></span><h4>Debugging memory leaks with <tt class="docutils literal"><span class="pre">trackref</span></tt><a class="headerlink" href="#debugging-memory-leaks-with-trackref" title="Permalink to this headline">¶</a></h4>
<p><tt class="docutils literal"><span class="pre">trackref</span></tt> is a module provided by Scrapy to debug the most common cases of
memory leaks. It basically tracks the references to all live Requests,
Responses, Item and Selector objects.</p>
<p>You can enter the telnet console and inspect how many objects (of the classes
mentioned above) are currently alive using the <tt class="docutils literal"><span class="pre">prefs()</span></tt> function which is an
alias to the <a class="reference internal" href="index.html#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><tt class="xref py py-func docutils literal"><span class="pre">print_live_refs()</span></tt></a> function:</p>
<div class="highlight-none"><div class="highlight"><pre>telnet localhost 6023

&gt;&gt;&gt; prefs()
Live References

ExampleSpider                       1   oldest: 15s ago
HtmlResponse                       10   oldest: 1s ago
Selector                            2   oldest: 0s ago
FormRequest                       878   oldest: 7s ago
</pre></div>
</div>
<p>As you can see, that report also shows the &#8220;age&#8221; of the oldest object in each
class.</p>
<p>If you do have leaks, chances are you can figure out which spider is leaking by
looking at the oldest request or response. You can get the oldest object of
each class using the <a class="reference internal" href="index.html#scrapy.utils.trackref.get_oldest" title="scrapy.utils.trackref.get_oldest"><tt class="xref py py-func docutils literal"><span class="pre">get_oldest()</span></tt></a> function like
this (from the telnet console).</p>
<div class="section" id="which-objects-are-tracked">
<h5>Which objects are tracked?<a class="headerlink" href="#which-objects-are-tracked" title="Permalink to this headline">¶</a></h5>
<p>The objects tracked by <tt class="docutils literal"><span class="pre">trackrefs</span></tt> are all from these classes (and all its
subclasses):</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">scrapy.http.Request</span></tt></li>
<li><tt class="docutils literal"><span class="pre">scrapy.http.Response</span></tt></li>
<li><tt class="docutils literal"><span class="pre">scrapy.item.Item</span></tt></li>
<li><tt class="docutils literal"><span class="pre">scrapy.selector.Selector</span></tt></li>
<li><tt class="docutils literal"><span class="pre">scrapy.spider.Spider</span></tt></li>
</ul>
</div>
<div class="section" id="a-real-example">
<h5>A real example<a class="headerlink" href="#a-real-example" title="Permalink to this headline">¶</a></h5>
<p>Let&#8217;s see a concrete example of an hypothetical case of memory leaks.</p>
<p>Suppose we have some spider with a line similar to this one:</p>
<div class="highlight-none"><div class="highlight"><pre>return Request(&quot;http://www.somenastyspider.com/product.php?pid=%d&quot; % product_id,
    callback=self.parse, meta={referer: response}&quot;)
</pre></div>
</div>
<p>That line is passing a response reference inside a request which effectively
ties the response lifetime to the requests&#8217; one, and that would definitely
cause memory leaks.</p>
<p>Let&#8217;s see how we can discover which one is the nasty spider (without knowing it
a-priori, of course) by using the <tt class="docutils literal"><span class="pre">trackref</span></tt> tool.</p>
<p>After the crawler is running for a few minutes and we notice its memory usage
has grown a lot, we can enter its telnet console and check the live
references:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; prefs()
Live References

SomenastySpider                     1   oldest: 15s ago
HtmlResponse                     3890   oldest: 265s ago
Selector                            2   oldest: 0s ago
Request                          3878   oldest: 250s ago
</pre></div>
</div>
<p>The fact that there are so many live responses (and that they&#8217;re so old) is
definitely suspicious, as responses should have a relatively short lifetime
compared to Requests. So let&#8217;s check the oldest response:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; from scrapy.utils.trackref import get_oldest
&gt;&gt;&gt; r = get_oldest(&#39;HtmlResponse&#39;)
&gt;&gt;&gt; r.url
&#39;http://www.somenastyspider.com/product.php?pid=123&#39;
</pre></div>
</div>
<p>There it is. By looking at the URL of the oldest response we can see it belongs
to the <tt class="docutils literal"><span class="pre">somenastyspider.com</span></tt> spider. We can now go and check the code of that
spider to discover the nasty line that is generating the leaks (passing
response references inside requests).</p>
<p>If you want to iterate over all objects, instead of getting the oldest one, you
can use the <tt class="xref py py-func docutils literal"><span class="pre">iter_all()</span></tt> function:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; from scrapy.utils.trackref import iter_all
&gt;&gt;&gt; [r.url for r in iter_all(&#39;HtmlResponse&#39;)]
[&#39;http://www.somenastyspider.com/product.php?pid=123&#39;,
 &#39;http://www.somenastyspider.com/product.php?pid=584&#39;,
...
</pre></div>
</div>
</div>
<div class="section" id="too-many-spiders">
<h5>Too many spiders?<a class="headerlink" href="#too-many-spiders" title="Permalink to this headline">¶</a></h5>
<p>If your project has too many spiders, the output of <tt class="docutils literal"><span class="pre">prefs()</span></tt> can be
difficult to read. For this reason, that function has a <tt class="docutils literal"><span class="pre">ignore</span></tt> argument
which can be used to ignore a particular class (and all its subclases). For
example, using:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; from scrapy.spider import Spider
&gt;&gt;&gt; prefs(ignore=Spider)
</pre></div>
</div>
<p>Won&#8217;t show any live references to spiders.</p>
<span class="target" id="module-scrapy.utils.trackref"></span></div>
<div class="section" id="scrapy-utils-trackref-module">
<h5>scrapy.utils.trackref module<a class="headerlink" href="#scrapy-utils-trackref-module" title="Permalink to this headline">¶</a></h5>
<p>Here are the functions available in the <a class="reference internal" href="index.html#module-scrapy.utils.trackref" title="scrapy.utils.trackref: Track references of live objects"><tt class="xref py py-mod docutils literal"><span class="pre">trackref</span></tt></a> module.</p>
<dl class="class">
<dt id="scrapy.utils.trackref.object_ref">
<em class="property">class </em><tt class="descclassname">scrapy.utils.trackref.</tt><tt class="descname">object_ref</tt><a class="headerlink" href="#scrapy.utils.trackref.object_ref" title="Permalink to this definition">¶</a></dt>
<dd><p>Inherit from this class (instead of object) if you want to track live
instances with the <tt class="docutils literal"><span class="pre">trackref</span></tt> module.</p>
</dd></dl>

<dl class="function">
<dt id="scrapy.utils.trackref.print_live_refs">
<tt class="descclassname">scrapy.utils.trackref.</tt><tt class="descname">print_live_refs</tt><big>(</big><em>class_name</em>, <em>ignore=NoneType</em><big>)</big><a class="headerlink" href="#scrapy.utils.trackref.print_live_refs" title="Permalink to this definition">¶</a></dt>
<dd><p>Print a report of live references, grouped by class name.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>ignore</strong> (<em>class or classes tuple</em>) &#8211; if given, all objects from the specified class (or tuple of
classes) will be ignored.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="scrapy.utils.trackref.get_oldest">
<tt class="descclassname">scrapy.utils.trackref.</tt><tt class="descname">get_oldest</tt><big>(</big><em>class_name</em><big>)</big><a class="headerlink" href="#scrapy.utils.trackref.get_oldest" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the oldest object alive with the given class name, or <tt class="docutils literal"><span class="pre">None</span></tt> if
none is found. Use <a class="reference internal" href="index.html#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><tt class="xref py py-func docutils literal"><span class="pre">print_live_refs()</span></tt></a> first to get a list of all
tracked live objects per class name.</p>
</dd></dl>

<dl class="function">
<dt id="scrapy.utils.trackref.iter_all">
<tt class="descclassname">scrapy.utils.trackref.</tt><tt class="descname">iter_all</tt><big>(</big><em>class_name</em><big>)</big><a class="headerlink" href="#scrapy.utils.trackref.iter_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterator over all objects alive with the given class name, or
<tt class="docutils literal"><span class="pre">None</span></tt> if none is found. Use <a class="reference internal" href="index.html#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><tt class="xref py py-func docutils literal"><span class="pre">print_live_refs()</span></tt></a> first to get a list
of all tracked live objects per class name.</p>
</dd></dl>

</div>
</div>
<div class="section" id="debugging-memory-leaks-with-guppy">
<span id="topics-leaks-guppy"></span><h4>Debugging memory leaks with Guppy<a class="headerlink" href="#debugging-memory-leaks-with-guppy" title="Permalink to this headline">¶</a></h4>
<p><tt class="docutils literal"><span class="pre">trackref</span></tt> provides a very convenient mechanism for tracking down memory
leaks, but it only keeps track of the objects that are more likely to cause
memory leaks (Requests, Responses, Items, and Selectors). However, there are
other cases where the memory leaks could come from other (more or less obscure)
objects. If this is your case, and you can&#8217;t find your leaks using <tt class="docutils literal"><span class="pre">trackref</span></tt>,
you still have another resource: the <a class="reference external" href="http://pypi.python.org/pypi/guppy">Guppy library</a>.</p>
<p>If you use <tt class="docutils literal"><span class="pre">setuptools</span></tt>, you can install Guppy with the following command:</p>
<div class="highlight-none"><div class="highlight"><pre>easy_install guppy
</pre></div>
</div>
<p>The telnet console also comes with a built-in shortcut (<tt class="docutils literal"><span class="pre">hpy</span></tt>) for accessing
Guppy heap objects. Here&#8217;s an example to view all Python objects available in
the heap using Guppy:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; x = hpy.heap()
&gt;&gt;&gt; x.bytype
Partition of a set of 297033 objects. Total size = 52587824 bytes.
 Index  Count   %     Size   % Cumulative  % Type
     0  22307   8 16423880  31  16423880  31 dict
     1 122285  41 12441544  24  28865424  55 str
     2  68346  23  5966696  11  34832120  66 tuple
     3    227   0  5836528  11  40668648  77 unicode
     4   2461   1  2222272   4  42890920  82 type
     5  16870   6  2024400   4  44915320  85 function
     6  13949   5  1673880   3  46589200  89 types.CodeType
     7  13422   5  1653104   3  48242304  92 list
     8   3735   1  1173680   2  49415984  94 _sre.SRE_Pattern
     9   1209   0   456936   1  49872920  95 scrapy.http.headers.Headers
&lt;1676 more rows. Type e.g. &#39;_.more&#39; to view.&gt;
</pre></div>
</div>
<p>You can see that most space is used by dicts. Then, if you want to see from
which attribute those dicts are referenced, you could do:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; x.bytype[0].byvia
Partition of a set of 22307 objects. Total size = 16423880 bytes.
 Index  Count   %     Size   % Cumulative  % Referred Via:
     0  10982  49  9416336  57   9416336  57 &#39;.__dict__&#39;
     1   1820   8  2681504  16  12097840  74 &#39;.__dict__&#39;, &#39;.func_globals&#39;
     2   3097  14  1122904   7  13220744  80
     3    990   4   277200   2  13497944  82 &quot;[&#39;cookies&#39;]&quot;
     4    987   4   276360   2  13774304  84 &quot;[&#39;cache&#39;]&quot;
     5    985   4   275800   2  14050104  86 &quot;[&#39;meta&#39;]&quot;
     6    897   4   251160   2  14301264  87 &#39;[2]&#39;
     7      1   0   196888   1  14498152  88 &quot;[&#39;moduleDict&#39;]&quot;, &quot;[&#39;modules&#39;]&quot;
     8    672   3   188160   1  14686312  89 &quot;[&#39;cb_kwargs&#39;]&quot;
     9     27   0   155016   1  14841328  90 &#39;[1]&#39;
&lt;333 more rows. Type e.g. &#39;_.more&#39; to view.&gt;
</pre></div>
</div>
<p>As you can see, the Guppy module is very powerful but also requires some deep
knowledge about Python internals. For more info about Guppy, refer to the
<a class="reference external" href="http://guppy-pe.sourceforge.net/">Guppy documentation</a>.</p>
</div>
<div class="section" id="leaks-without-leaks">
<span id="topics-leaks-without-leaks"></span><h4>Leaks without leaks<a class="headerlink" href="#leaks-without-leaks" title="Permalink to this headline">¶</a></h4>
<p>Sometimes, you may notice that the memory usage of your Scrapy process will
only increase, but never decrease. Unfortunately, this could happen even
though neither Scrapy nor your project are leaking memory. This is due to a
(not so well) known problem of Python, which may not return released memory to
the operating system in some cases. For more information on this issue see:</p>
<ul class="simple">
<li><a class="reference external" href="http://evanjones.ca/python-memory.html">Python Memory Management</a></li>
<li><a class="reference external" href="http://evanjones.ca/python-memory-part2.html">Python Memory Management Part 2</a></li>
<li><a class="reference external" href="http://evanjones.ca/python-memory-part3.html">Python Memory Management Part 3</a></li>
</ul>
<p>The improvements proposed by Evan Jones, which are detailed in <a class="reference external" href="http://evanjones.ca/memoryallocator/">this paper</a>,
got merged in Python 2.5, but this only reduces the problem, it doesn&#8217;t fix it
completely. To quote the paper:</p>
<blockquote>
<div><em>Unfortunately, this patch can only free an arena if there are no more
objects allocated in it anymore. This means that fragmentation is a large
issue. An application could have many megabytes of free memory, scattered
throughout all the arenas, but it will be unable to free any of it. This is
a problem experienced by all memory allocators. The only way to solve it is
to move to a compacting garbage collector, which is able to move objects in
memory. This would require significant changes to the Python interpreter.</em></div></blockquote>
<p>This problem will be fixed in future Scrapy releases, where we plan to adopt a
new process model and run spiders in a pool of recyclable sub-processes.</p>
</div>
</div>
<span id="document-topics/images"></span><div class="section" id="downloading-item-images">
<span id="topics-images"></span><h3>Downloading Item Images<a class="headerlink" href="#downloading-item-images" title="Permalink to this headline">¶</a></h3>
<p>Scrapy provides an <a class="reference internal" href="index.html#document-topics/item-pipeline"><em>item pipeline</em></a> for downloading
images attached to a particular item, for example, when you scrape products and
also want to download their images locally.</p>
<p>This pipeline, called the Images Pipeline and implemented in the
<a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline" title="scrapy.contrib.pipeline.images.ImagesPipeline"><tt class="xref py py-class docutils literal"><span class="pre">ImagesPipeline</span></tt></a> class, provides a convenient way for
downloading and storing images locally with some additional features:</p>
<ul class="simple">
<li>Convert all downloaded images to a common format (JPG) and mode (RGB)</li>
<li>Avoid re-downloading images which were downloaded recently</li>
<li>Thumbnail generation</li>
<li>Check images width/height to make sure they meet a minimum constraint</li>
</ul>
<p>This pipeline also keeps an internal queue of those images which are currently
being scheduled for download, and connects those items that arrive containing
the same image, to that queue. This avoids downloading the same image more than
once when it&#8217;s shared by several items.</p>
<p><a class="reference external" href="https://github.com/python-imaging/Pillow">Pillow</a> is used for thumbnailing and normalizing images to JPEG/RGB format,
so you need to install this library in order to use the images pipeline.
<a class="reference external" href="http://www.pythonware.com/products/pil/">Python Imaging Library</a> (PIL) should also work in most cases, but it
is known to cause troubles in some setups, so we recommend to use <a class="reference external" href="https://github.com/python-imaging/Pillow">Pillow</a>
instead of <a class="reference external" href="PythonImagingLibrary">PIL</a>.</p>
<div class="section" id="using-the-images-pipeline">
<h4>Using the Images Pipeline<a class="headerlink" href="#using-the-images-pipeline" title="Permalink to this headline">¶</a></h4>
<p>The typical workflow, when using the <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline" title="scrapy.contrib.pipeline.images.ImagesPipeline"><tt class="xref py py-class docutils literal"><span class="pre">ImagesPipeline</span></tt></a> goes like
this:</p>
<ol class="arabic simple">
<li>In a Spider, you scrape an item and put the URLs of its images into a
<tt class="docutils literal"><span class="pre">image_urls</span></tt> field.</li>
<li>The item is returned from the spider and goes to the item pipeline.</li>
<li>When the item reaches the <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline" title="scrapy.contrib.pipeline.images.ImagesPipeline"><tt class="xref py py-class docutils literal"><span class="pre">ImagesPipeline</span></tt></a>, the URLs in the
<tt class="docutils literal"><span class="pre">image_urls</span></tt> field are scheduled for download using the standard
Scrapy scheduler and downloader (which means the scheduler and downloader
middlewares are reused), but with a higher priority, processing them before other
pages are scraped. The item remains &#8220;locked&#8221; at that particular pipeline stage
until the images have finish downloading (or fail for some reason).</li>
<li>When the images are downloaded another field (<tt class="docutils literal"><span class="pre">images</span></tt>) will be populated
with the results. This field will contain a list of dicts with information
about the images downloaded, such as the downloaded path, the original
scraped url (taken from the <tt class="docutils literal"><span class="pre">image_urls</span></tt> field) , and the image checksum.
The images in the list of the <tt class="docutils literal"><span class="pre">images</span></tt> field will retain the same order of
the original <tt class="docutils literal"><span class="pre">image_urls</span></tt> field. If some image failed downloading, an
error will be logged and the image won&#8217;t be present in the <tt class="docutils literal"><span class="pre">images</span></tt> field.</li>
</ol>
</div>
<div class="section" id="usage-example">
<h4>Usage example<a class="headerlink" href="#usage-example" title="Permalink to this headline">¶</a></h4>
<p>In order to use the image pipeline you just need to <a class="reference internal" href="index.html#topics-images-enabling"><em>enable it</em></a> and define an item with the <tt class="docutils literal"><span class="pre">image_urls</span></tt> and
<tt class="docutils literal"><span class="pre">images</span></tt> fields:</p>
<div class="highlight-none"><div class="highlight"><pre>import scrapy

class MyItem(scrapy.Item):

    # ... other item fields ...
    image_urls = scrapy.Field()
    images = scrapy.Field()
</pre></div>
</div>
<p>If you need something more complex and want to override the custom images
pipeline behaviour, see <a class="reference internal" href="index.html#topics-images-override"><em>Implementing your custom Images Pipeline</em></a>.</p>
</div>
<div class="section" id="enabling-your-images-pipeline">
<span id="topics-images-enabling"></span><h4>Enabling your Images Pipeline<a class="headerlink" href="#enabling-your-images-pipeline" title="Permalink to this headline">¶</a></h4>
<p id="std:setting-IMAGES_STORE">To enable your images pipeline you must first add it to your project
<a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><tt class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></tt></a> setting:</p>
<div class="highlight-none"><div class="highlight"><pre>ITEM_PIPELINES = {&#39;scrapy.contrib.pipeline.images.ImagesPipeline&#39;: 1}
</pre></div>
</div>
<p>And set the <a class="reference internal" href="index.html#std:setting-IMAGES_STORE"><tt class="xref std std-setting docutils literal"><span class="pre">IMAGES_STORE</span></tt></a> setting to a valid directory that will be
used for storing the downloaded images. Otherwise the pipeline will remain
disabled, even if you include it in the <a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><tt class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></tt></a> setting.</p>
<p>For example:</p>
<div class="highlight-none"><div class="highlight"><pre>IMAGES_STORE = &#39;/path/to/valid/dir&#39;
</pre></div>
</div>
</div>
<div class="section" id="images-storage">
<h4>Images Storage<a class="headerlink" href="#images-storage" title="Permalink to this headline">¶</a></h4>
<p>File system is currently the only officially supported storage, but there is
also (undocumented) support for <a class="reference external" href="https://s3.amazonaws.com/">Amazon S3</a>.</p>
<div class="section" id="file-system-storage">
<h5>File system storage<a class="headerlink" href="#file-system-storage" title="Permalink to this headline">¶</a></h5>
<p>The images are stored in files (one per image), using a <a class="reference external" href="http://en.wikipedia.org/wiki/SHA_hash_functions">SHA1 hash</a> of their
URLs for the file names.</p>
<p>For example, the following image URL:</p>
<div class="highlight-none"><div class="highlight"><pre>http://www.example.com/image.jpg
</pre></div>
</div>
<p>Whose <cite>SHA1 hash</cite> is:</p>
<div class="highlight-none"><div class="highlight"><pre>3afec3b4765f8f0a07b78f98c07b83f013567a0a
</pre></div>
</div>
<p>Will be downloaded and stored in the following file:</p>
<div class="highlight-none"><div class="highlight"><pre>&lt;IMAGES_STORE&gt;/full/3afec3b4765f8f0a07b78f98c07b83f013567a0a.jpg
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">&lt;IMAGES_STORE&gt;</span></tt> is the directory defined in <a class="reference internal" href="index.html#std:setting-IMAGES_STORE"><tt class="xref std std-setting docutils literal"><span class="pre">IMAGES_STORE</span></tt></a> setting</li>
<li><tt class="docutils literal"><span class="pre">full</span></tt> is a sub-directory to separate full images from thumbnails (if
used). For more info see <a class="reference internal" href="index.html#topics-images-thumbnails"><em>Thumbnail generation</em></a>.</li>
</ul>
</div>
</div>
<div class="section" id="additional-features">
<h4>Additional features<a class="headerlink" href="#additional-features" title="Permalink to this headline">¶</a></h4>
<div class="section" id="image-expiration">
<h5>Image expiration<a class="headerlink" href="#image-expiration" title="Permalink to this headline">¶</a></h5>
<p id="std:setting-IMAGES_EXPIRES">The Image Pipeline avoids downloading images that were downloaded recently. To
adjust this retention delay use the <a class="reference internal" href="index.html#std:setting-IMAGES_EXPIRES"><tt class="xref std std-setting docutils literal"><span class="pre">IMAGES_EXPIRES</span></tt></a> setting, which
specifies the delay in number of days:</p>
<div class="highlight-none"><div class="highlight"><pre># 90 days of delay for image expiration
IMAGES_EXPIRES = 90
</pre></div>
</div>
</div>
<div class="section" id="thumbnail-generation">
<span id="topics-images-thumbnails"></span><h5>Thumbnail generation<a class="headerlink" href="#thumbnail-generation" title="Permalink to this headline">¶</a></h5>
<p>The Images Pipeline can automatically create thumbnails of the downloaded
images.</p>
<p id="std:setting-IMAGES_THUMBS">In order use this feature, you must set <a class="reference internal" href="index.html#std:setting-IMAGES_THUMBS"><tt class="xref std std-setting docutils literal"><span class="pre">IMAGES_THUMBS</span></tt></a> to a dictionary
where the keys are the thumbnail names and the values are their dimensions.</p>
<p>For example:</p>
<div class="highlight-none"><div class="highlight"><pre>IMAGES_THUMBS = {
    &#39;small&#39;: (50, 50),
    &#39;big&#39;: (270, 270),
}
</pre></div>
</div>
<p>When you use this feature, the Images Pipeline will create thumbnails of the
each specified size with this format:</p>
<div class="highlight-none"><div class="highlight"><pre>&lt;IMAGES_STORE&gt;/thumbs/&lt;size_name&gt;/&lt;image_id&gt;.jpg
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">&lt;size_name&gt;</span></tt> is the one specified in the <a class="reference internal" href="index.html#std:setting-IMAGES_THUMBS"><tt class="xref std std-setting docutils literal"><span class="pre">IMAGES_THUMBS</span></tt></a>
dictionary keys (<tt class="docutils literal"><span class="pre">small</span></tt>, <tt class="docutils literal"><span class="pre">big</span></tt>, etc)</li>
<li><tt class="docutils literal"><span class="pre">&lt;image_id&gt;</span></tt> is the <a class="reference external" href="http://en.wikipedia.org/wiki/SHA_hash_functions">SHA1 hash</a> of the image url</li>
</ul>
<p>Example of image files stored using <tt class="docutils literal"><span class="pre">small</span></tt> and <tt class="docutils literal"><span class="pre">big</span></tt> thumbnail names:</p>
<div class="highlight-none"><div class="highlight"><pre>&lt;IMAGES_STORE&gt;/full/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
&lt;IMAGES_STORE&gt;/thumbs/small/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
&lt;IMAGES_STORE&gt;/thumbs/big/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
</pre></div>
</div>
<p>The first one is the full image, as downloaded from the site.</p>
</div>
<div class="section" id="filtering-out-small-images">
<h5>Filtering out small images<a class="headerlink" href="#filtering-out-small-images" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:setting-IMAGES_MIN_HEIGHT"></span><p id="std:setting-IMAGES_MIN_WIDTH">You can drop images which are too small, by specifying the minimum allowed size
in the <a class="reference internal" href="index.html#std:setting-IMAGES_MIN_HEIGHT"><tt class="xref std std-setting docutils literal"><span class="pre">IMAGES_MIN_HEIGHT</span></tt></a> and <a class="reference internal" href="index.html#std:setting-IMAGES_MIN_WIDTH"><tt class="xref std std-setting docutils literal"><span class="pre">IMAGES_MIN_WIDTH</span></tt></a> settings.</p>
<p>For example:</p>
<div class="highlight-none"><div class="highlight"><pre>IMAGES_MIN_HEIGHT = 110
IMAGES_MIN_WIDTH = 110
</pre></div>
</div>
<p>Note: these size constraints don&#8217;t affect thumbnail generation at all.</p>
<p>By default, there are no size constraints, so all images are processed.</p>
</div>
</div>
<div class="section" id="module-scrapy.contrib.pipeline.images">
<span id="implementing-your-custom-images-pipeline"></span><span id="topics-images-override"></span><h4>Implementing your custom Images Pipeline<a class="headerlink" href="#module-scrapy.contrib.pipeline.images" title="Permalink to this headline">¶</a></h4>
<p>Here are the methods that you should override in your custom Images Pipeline:</p>
<dl class="class">
<dt id="scrapy.contrib.pipeline.images.ImagesPipeline">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.pipeline.images.</tt><tt class="descname">ImagesPipeline</tt><a class="headerlink" href="#scrapy.contrib.pipeline.images.ImagesPipeline" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests">
<tt class="descname">get_media_requests</tt><big>(</big><em>item</em>, <em>info</em><big>)</big><a class="headerlink" href="#scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests" title="Permalink to this definition">¶</a></dt>
<dd><p>As seen on the workflow, the pipeline will get the URLs of the images to
download from the item. In order to do this, you must override the
<a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests" title="scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests"><tt class="xref py py-meth docutils literal"><span class="pre">get_media_requests()</span></tt></a> method and return a Request for each
image URL:</p>
<div class="highlight-none"><div class="highlight"><pre>def get_media_requests(self, item, info):
    for image_url in item[&#39;image_urls&#39;]:
        yield scrapy.Request(image_url)
</pre></div>
</div>
<p>Those requests will be processed by the pipeline and, when they have finished
downloading, the results will be sent to the
<a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.item_completed" title="scrapy.contrib.pipeline.images.ImagesPipeline.item_completed"><tt class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></tt></a> method, as a list of 2-element tuples.
Each tuple will contain <tt class="docutils literal"><span class="pre">(success,</span> <span class="pre">image_info_or_failure)</span></tt> where:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">success</span></tt> is a boolean which is <tt class="docutils literal"><span class="pre">True</span></tt> if the image was downloaded
successfully or <tt class="docutils literal"><span class="pre">False</span></tt> if it failed for some reason</li>
<li><tt class="docutils literal"><span class="pre">image_info_or_error</span></tt> is a dict containing the following keys (if success
is <tt class="docutils literal"><span class="pre">True</span></tt>) or a <a class="reference external" href="http://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Twisted Failure</a> if there was a problem.<ul>
<li><tt class="docutils literal"><span class="pre">url</span></tt> - the url where the image was downloaded from. This is the url of
the request returned from the <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests" title="scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests"><tt class="xref py py-meth docutils literal"><span class="pre">get_media_requests()</span></tt></a>
method.</li>
<li><tt class="docutils literal"><span class="pre">path</span></tt> - the path (relative to <a class="reference internal" href="index.html#std:setting-IMAGES_STORE"><tt class="xref std std-setting docutils literal"><span class="pre">IMAGES_STORE</span></tt></a>) where the image
was stored</li>
<li><tt class="docutils literal"><span class="pre">checksum</span></tt> - a <a class="reference external" href="http://en.wikipedia.org/wiki/MD5">MD5 hash</a> of the image contents</li>
</ul>
</li>
</ul>
<p>The list of tuples received by <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.item_completed" title="scrapy.contrib.pipeline.images.ImagesPipeline.item_completed"><tt class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></tt></a> is
guaranteed to retain the same order of the requests returned from the
<a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests" title="scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests"><tt class="xref py py-meth docutils literal"><span class="pre">get_media_requests()</span></tt></a> method.</p>
<p>Here&#8217;s a typical value of the <tt class="docutils literal"><span class="pre">results</span></tt> argument:</p>
<div class="highlight-none"><div class="highlight"><pre>[(True,
  {&#39;checksum&#39;: &#39;2b00042f7481c7b056c4b410d28f33cf&#39;,
   &#39;path&#39;: &#39;full/7d97e98f8af710c7e7fe703abc8f639e0ee507c4.jpg&#39;,
   &#39;url&#39;: &#39;http://www.example.com/images/product1.jpg&#39;}),
 (True,
  {&#39;checksum&#39;: &#39;b9628c4ab9b595f72f280b90c4fd093d&#39;,
   &#39;path&#39;: &#39;full/1ca5879492b8fd606df1964ea3c1e2f4520f076f.jpg&#39;,
   &#39;url&#39;: &#39;http://www.example.com/images/product2.jpg&#39;}),
 (False,
  Failure(...))]
</pre></div>
</div>
<p>By default the <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests" title="scrapy.contrib.pipeline.images.ImagesPipeline.get_media_requests"><tt class="xref py py-meth docutils literal"><span class="pre">get_media_requests()</span></tt></a> method returns <tt class="docutils literal"><span class="pre">None</span></tt> which
means there are no images to download for the item.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.pipeline.images.ImagesPipeline.item_completed">
<tt class="descname">item_completed</tt><big>(</big><em>results</em>, <em>items</em>, <em>info</em><big>)</big><a class="headerlink" href="#scrapy.contrib.pipeline.images.ImagesPipeline.item_completed" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.item_completed" title="scrapy.contrib.pipeline.images.ImagesPipeline.item_completed"><tt class="xref py py-meth docutils literal"><span class="pre">ImagesPipeline.item_completed()</span></tt></a> method called when all image
requests for a single item have completed (either finished downloading, or
failed for some reason).</p>
<p>The <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.item_completed" title="scrapy.contrib.pipeline.images.ImagesPipeline.item_completed"><tt class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></tt></a> method must return the
output that will be sent to subsequent item pipeline stages, so you must
return (or drop) the item, as you would in any pipeline.</p>
<p>Here is an example of the <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.item_completed" title="scrapy.contrib.pipeline.images.ImagesPipeline.item_completed"><tt class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></tt></a> method where we
store the downloaded image paths (passed in results) in the <tt class="docutils literal"><span class="pre">image_paths</span></tt>
item field, and we drop the item if it doesn&#8217;t contain any images:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.exceptions import DropItem

def item_completed(self, results, item, info):
    image_paths = [x[&#39;path&#39;] for ok, x in results if ok]
    if not image_paths:
        raise DropItem(&quot;Item contains no images&quot;)
    item[&#39;image_paths&#39;] = image_paths
    return item
</pre></div>
</div>
<p>By default, the <a class="reference internal" href="index.html#scrapy.contrib.pipeline.images.ImagesPipeline.item_completed" title="scrapy.contrib.pipeline.images.ImagesPipeline.item_completed"><tt class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></tt></a> method returns the item.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="custom-images-pipeline-example">
<h4>Custom Images pipeline example<a class="headerlink" href="#custom-images-pipeline-example" title="Permalink to this headline">¶</a></h4>
<p>Here is a full example of the Images Pipeline whose methods are examplified
above:</p>
<div class="highlight-none"><div class="highlight"><pre>import scrapy
from scrapy.contrib.pipeline.images import ImagesPipeline
from scrapy.exceptions import DropItem

class MyImagesPipeline(ImagesPipeline):

    def get_media_requests(self, item, info):
        for image_url in item[&#39;image_urls&#39;]:
            yield scrapy.Request(image_url)

    def item_completed(self, results, item, info):
        image_paths = [x[&#39;path&#39;] for ok, x in results if ok]
        if not image_paths:
            raise DropItem(&quot;Item contains no images&quot;)
        item[&#39;image_paths&#39;] = image_paths
        return item
</pre></div>
</div>
</div>
</div>
<span id="document-topics/ubuntu"></span><div class="section" id="ubuntu-packages">
<span id="topics-ubuntu"></span><h3>Ubuntu packages<a class="headerlink" href="#ubuntu-packages" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p><a class="reference external" href="http://scrapinghub.com/">Scrapinghub</a> publishes apt-gettable packages which are generally fresher than
those in Ubuntu, and more stable too since they&#8217;re continuously built from
<a class="reference external" href="https://github.com/scrapy/scrapy">Github repo</a> (master &amp; stable branches) and so they contain the latest bug
fixes.</p>
<p>To use the packages:</p>
<ol class="arabic">
<li><p class="first">Import the GPG key used to sign Scrapy packages into APT keyring:</p>
<div class="highlight-none"><div class="highlight"><pre>sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 627220E7
</pre></div>
</div>
</li>
<li><p class="first">Create <cite>/etc/apt/sources.list.d/scrapy.list</cite> file using the following command:</p>
<div class="highlight-none"><div class="highlight"><pre>echo &#39;deb http://archive.scrapy.org/ubuntu scrapy main&#39; | sudo tee /etc/apt/sources.list.d/scrapy.list
</pre></div>
</div>
</li>
<li><p class="first">Update package lists and install the scrapy-0.24 package:</p>
<pre class="literal-block">
sudo apt-get update &amp;&amp; sudo apt-get install scrapy-0.24
</pre>
</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Repeat step 3 if you are trying to upgrade Scrapy.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last"><cite>python-scrapy</cite> is a different package provided by official debian
repositories, it&#8217;s very outdated and it isn&#8217;t supported by Scrapy team.</p>
</div>
</div>
<span id="document-topics/scrapyd"></span><div class="section" id="scrapyd">
<span id="topics-scrapyd"></span><h3>Scrapyd<a class="headerlink" href="#scrapyd" title="Permalink to this headline">¶</a></h3>
<p>Scrapyd has been moved into a separate project.</p>
<p>Its documentation is now hosted at:</p>
<blockquote>
<div><a class="reference external" href="http://scrapyd.readthedocs.org/">http://scrapyd.readthedocs.org/</a></div></blockquote>
</div>
<span id="document-topics/autothrottle"></span><div class="section" id="autothrottle-extension">
<h3>AutoThrottle extension<a class="headerlink" href="#autothrottle-extension" title="Permalink to this headline">¶</a></h3>
<p>This is an extension for automatically throttling crawling speed based on load
of both the Scrapy server and the website you are crawling.</p>
<div class="section" id="design-goals">
<h4>Design goals<a class="headerlink" href="#design-goals" title="Permalink to this headline">¶</a></h4>
<ol class="arabic simple">
<li>be nicer to sites instead of using default download delay of zero</li>
<li>automatically adjust scrapy to the optimum crawling speed, so the user
doesn&#8217;t have to tune the download delays and concurrent requests to find the
optimum one. the user only needs to specify the maximum concurrent requests
it allows, and the extension does the rest.</li>
</ol>
</div>
<div class="section" id="how-it-works">
<h4>How it works<a class="headerlink" href="#how-it-works" title="Permalink to this headline">¶</a></h4>
<p>In Scrapy, the download latency is measured as the time elapsed between
establishing the TCP connection and receiving the HTTP headers.</p>
<p>Note that these latencies are very hard to measure accurately in a cooperative
multitasking environment because Scrapy may be busy processing a spider
callback, for example, and unable to attend downloads. However, these latencies
should still give a reasonable estimate of how busy Scrapy (and ultimately, the
server) is, and this extension builds on that premise.</p>
</div>
<div class="section" id="throttling-algorithm">
<span id="autothrottle-algorithm"></span><h4>Throttling algorithm<a class="headerlink" href="#throttling-algorithm" title="Permalink to this headline">¶</a></h4>
<p>This adjusts download delays and concurrency based on the following rules:</p>
<ol class="arabic simple">
<li>spiders always start with one concurrent request and a download delay of
<a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_START_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_START_DELAY</span></tt></a></li>
<li>when a response is received, the download delay is adjusted to the
average of previous download delay and the latency of the response.</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The AutoThrottle extension honours the standard Scrapy settings for
concurrency and delay. This means that it will never set a download delay
lower than <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></tt></a> or a concurrency higher than
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></tt></a>
(or <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></tt></a>, depending on which one you use).</p>
</div>
</div>
<div class="section" id="settings">
<h4>Settings<a class="headerlink" href="#settings" title="Permalink to this headline">¶</a></h4>
<p>The settings used to control the AutoThrottle extension are:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_ENABLED</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_START_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_START_DELAY</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_MAX_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_MAX_DELAY</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_DEBUG"><tt class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_DEBUG</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></tt></a></li>
</ul>
<p>For more information see <a class="reference internal" href="index.html#autothrottle-algorithm"><em>Throttling algorithm</em></a>.</p>
<div class="section" id="autothrottle-enabled">
<span id="std:setting-AUTOTHROTTLE_ENABLED"></span><h5>AUTOTHROTTLE_ENABLED<a class="headerlink" href="#autothrottle-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Enables the AutoThrottle extension.</p>
</div>
<div class="section" id="autothrottle-start-delay">
<span id="std:setting-AUTOTHROTTLE_START_DELAY"></span><h5>AUTOTHROTTLE_START_DELAY<a class="headerlink" href="#autothrottle-start-delay" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">5.0</span></tt></p>
<p>The initial download delay (in seconds).</p>
</div>
<div class="section" id="autothrottle-max-delay">
<span id="std:setting-AUTOTHROTTLE_MAX_DELAY"></span><h5>AUTOTHROTTLE_MAX_DELAY<a class="headerlink" href="#autothrottle-max-delay" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">60.0</span></tt></p>
<p>The maximum download delay (in seconds) to be set in case of high latencies.</p>
</div>
<div class="section" id="autothrottle-debug">
<span id="std:setting-AUTOTHROTTLE_DEBUG"></span><h5>AUTOTHROTTLE_DEBUG<a class="headerlink" href="#autothrottle-debug" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Enable AutoThrottle debug mode which will display stats on every response
received, so you can see how the throttling parameters are being adjusted in
real time.</p>
</div>
</div>
</div>
<span id="document-topics/benchmarking"></span><div class="section" id="benchmarking">
<span id="id1"></span><h3>Benchmarking<a class="headerlink" href="#benchmarking" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p>Scrapy comes with a simple benchmarking suite that spawns a local HTTP server
and crawls it at the maximum possible speed. The goal of this benchmarking is
to get an idea of how Scrapy performs in your hardware, in order to have a
common baseline for comparisons. It uses a simple spider that does nothing and
just follows links.</p>
<p>To run it use:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy bench
</pre></div>
</div>
<p>You should see an output like this:</p>
<div class="highlight-none"><div class="highlight"><pre>2013-05-16 13:08:46-0300 [scrapy] INFO: Scrapy 0.17.0 started (bot: scrapybot)
2013-05-16 13:08:47-0300 [follow] INFO: Spider opened
2013-05-16 13:08:47-0300 [follow] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:48-0300 [follow] INFO: Crawled 74 pages (at 4440 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:49-0300 [follow] INFO: Crawled 143 pages (at 4140 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:50-0300 [follow] INFO: Crawled 210 pages (at 4020 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:51-0300 [follow] INFO: Crawled 274 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:52-0300 [follow] INFO: Crawled 343 pages (at 4140 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:53-0300 [follow] INFO: Crawled 410 pages (at 4020 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:54-0300 [follow] INFO: Crawled 474 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:55-0300 [follow] INFO: Crawled 538 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:56-0300 [follow] INFO: Crawled 602 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:57-0300 [follow] INFO: Closing spider (closespider_timeout)
2013-05-16 13:08:57-0300 [follow] INFO: Crawled 666 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:57-0300 [follow] INFO: Dumping Scrapy stats:
    {&#39;downloader/request_bytes&#39;: 231508,
     &#39;downloader/request_count&#39;: 682,
     &#39;downloader/request_method_count/GET&#39;: 682,
     &#39;downloader/response_bytes&#39;: 1172802,
     &#39;downloader/response_count&#39;: 682,
     &#39;downloader/response_status_count/200&#39;: 682,
     &#39;finish_reason&#39;: &#39;closespider_timeout&#39;,
     &#39;finish_time&#39;: datetime.datetime(2013, 5, 16, 16, 8, 57, 985539),
     &#39;log_count/INFO&#39;: 14,
     &#39;request_depth_max&#39;: 34,
     &#39;response_received_count&#39;: 682,
     &#39;scheduler/dequeued&#39;: 682,
     &#39;scheduler/dequeued/memory&#39;: 682,
     &#39;scheduler/enqueued&#39;: 12767,
     &#39;scheduler/enqueued/memory&#39;: 12767,
     &#39;start_time&#39;: datetime.datetime(2013, 5, 16, 16, 8, 47, 676539)}
2013-05-16 13:08:57-0300 [follow] INFO: Spider closed (closespider_timeout)
</pre></div>
</div>
<p>That tells you that Scrapy is able to crawl about 3900 pages per minute in the
hardware where you run it. Note that this is a very simple spider intended to
follow links, any custom spider you write will probably do more stuff which
results in slower crawl rates. How slower depends on how much your spider does
and how well it&#8217;s written.</p>
<p>In the future, more cases will be added to the benchmarking suite to cover
other common scenarios.</p>
</div>
<span id="document-topics/jobs"></span><div class="section" id="jobs-pausing-and-resuming-crawls">
<span id="topics-jobs"></span><h3>Jobs: pausing and resuming crawls<a class="headerlink" href="#jobs-pausing-and-resuming-crawls" title="Permalink to this headline">¶</a></h3>
<p>Sometimes, for big sites, it&#8217;s desirable to pause crawls and be able to resume
them later.</p>
<p>Scrapy supports this functionality out of the box by providing the following
facilities:</p>
<ul class="simple">
<li>a scheduler that persists scheduled requests on disk</li>
<li>a duplicates filter that persists visited requests on disk</li>
<li>an extension that keeps some spider state (key/value pairs) persistent
between batches</li>
</ul>
<div class="section" id="job-directory">
<h4>Job directory<a class="headerlink" href="#job-directory" title="Permalink to this headline">¶</a></h4>
<p>To enable persistence support you just need to define a <em>job directory</em> through
the <tt class="docutils literal"><span class="pre">JOBDIR</span></tt> setting. This directory will be for storing all required data to
keep the state of a single job (ie. a spider run).  It&#8217;s important to note that
this directory must not be shared by different spiders, or even different
jobs/runs of the same spider, as it&#8217;s meant to be used for storing the state of
a <em>single</em> job.</p>
</div>
<div class="section" id="how-to-use-it">
<h4>How to use it<a class="headerlink" href="#how-to-use-it" title="Permalink to this headline">¶</a></h4>
<p>To start a spider with persistence supported enabled, run it like this:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy crawl somespider -s JOBDIR=crawls/somespider-1
</pre></div>
</div>
<p>Then, you can stop the spider safely at any time (by pressing Ctrl-C or sending
a signal), and resume it later by issuing the same command:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy crawl somespider -s JOBDIR=crawls/somespider-1
</pre></div>
</div>
</div>
<div class="section" id="keeping-persistent-state-between-batches">
<h4>Keeping persistent state between batches<a class="headerlink" href="#keeping-persistent-state-between-batches" title="Permalink to this headline">¶</a></h4>
<p>Sometimes you&#8217;ll want to keep some persistent spider state between pause/resume
batches. You can use the <tt class="docutils literal"><span class="pre">spider.state</span></tt> attribute for that, which should be a
dict. There&#8217;s a built-in extension that takes care of serializing, storing and
loading that attribute from the job directory, when the spider starts and
stops.</p>
<p>Here&#8217;s an example of a callback that uses the spider state (other spider code
is omitted for brevity):</p>
<div class="highlight-none"><div class="highlight"><pre>def parse_item(self, response):
    # parse item here
    self.state[&#39;items_count&#39;] = self.state.get(&#39;items_count&#39;, 0) + 1
</pre></div>
</div>
</div>
<div class="section" id="persistence-gotchas">
<h4>Persistence gotchas<a class="headerlink" href="#persistence-gotchas" title="Permalink to this headline">¶</a></h4>
<p>There are a few things to keep in mind if you want to be able to use the Scrapy
persistence support:</p>
<div class="section" id="cookies-expiration">
<h5>Cookies expiration<a class="headerlink" href="#cookies-expiration" title="Permalink to this headline">¶</a></h5>
<p>Cookies may expire. So, if you don&#8217;t resume your spider quickly the requests
scheduled may no longer work. This won&#8217;t be an issue if you spider doesn&#8217;t rely
on cookies.</p>
</div>
<div class="section" id="request-serialization">
<h5>Request serialization<a class="headerlink" href="#request-serialization" title="Permalink to this headline">¶</a></h5>
<p>Requests must be serializable by the <cite>pickle</cite> module, in order for persistence
to work, so you should make sure that your requests are serializable.</p>
<p>The most common issue here is to use <tt class="docutils literal"><span class="pre">lambda</span></tt> functions on request callbacks that
can&#8217;t be persisted.</p>
<p>So, for example, this won&#8217;t work:</p>
<div class="highlight-none"><div class="highlight"><pre>def some_callback(self, response):
    somearg = &#39;test&#39;
    return scrapy.Request(&#39;http://www.example.com&#39;, callback=lambda r: self.other_callback(r, somearg))

def other_callback(self, response, somearg):
    print &quot;the argument passed is:&quot;, somearg
</pre></div>
</div>
<p>But this will:</p>
<div class="highlight-none"><div class="highlight"><pre>def some_callback(self, response):
    somearg = &#39;test&#39;
    return scrapy.Request(&#39;http://www.example.com&#39;, meta={&#39;somearg&#39;: somearg})

def other_callback(self, response):
    somearg = response.meta[&#39;somearg&#39;]
    print &quot;the argument passed is:&quot;, somearg
</pre></div>
</div>
</div>
</div>
</div>
<span id="document-topics/djangoitem"></span><div class="section" id="djangoitem">
<span id="topics-djangoitem"></span><h3>DjangoItem<a class="headerlink" href="#djangoitem" title="Permalink to this headline">¶</a></h3>
<p><tt class="xref py py-class docutils literal"><span class="pre">DjangoItem</span></tt> is a class of item that gets its fields definition from a
Django model, you simply create a <tt class="xref py py-class docutils literal"><span class="pre">DjangoItem</span></tt> and specify what Django
model it relates to.</p>
<p>Besides of getting the model fields defined on your item, <tt class="xref py py-class docutils literal"><span class="pre">DjangoItem</span></tt>
provides a method to create and populate a Django model instance with the item
data.</p>
<div class="section" id="using-djangoitem">
<h4>Using DjangoItem<a class="headerlink" href="#using-djangoitem" title="Permalink to this headline">¶</a></h4>
<p><tt class="xref py py-class docutils literal"><span class="pre">DjangoItem</span></tt> works much like ModelForms in Django, you create a subclass
and define its <tt class="docutils literal"><span class="pre">django_model</span></tt> attribute to be a valid Django model. With this
you will get an item with a field for each Django model field.</p>
<p>In addition, you can define fields that aren&#8217;t present in the model and even
override fields that are present in the model defining them in the item.</p>
<p>Let&#8217;s see some examples:</p>
<p>Creating a Django model for the examples:</p>
<div class="highlight-none"><div class="highlight"><pre>from django.db import models

class Person(models.Model):
    name = models.CharField(max_length=255)
    age = models.IntegerField()
</pre></div>
</div>
<p>Defining a basic <tt class="xref py py-class docutils literal"><span class="pre">DjangoItem</span></tt>:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.contrib.djangoitem import DjangoItem

class PersonItem(DjangoItem):
    django_model = Person
</pre></div>
</div>
<p><tt class="xref py py-class docutils literal"><span class="pre">DjangoItem</span></tt> work just like <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a>:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; p = PersonItem()
&gt;&gt;&gt; p[&#39;name&#39;] = &#39;John&#39;
&gt;&gt;&gt; p[&#39;age&#39;] = &#39;22&#39;
</pre></div>
</div>
<p>To obtain the Django model from the item, we call the extra method
<tt class="xref py py-meth docutils literal"><span class="pre">save()</span></tt> of the <tt class="xref py py-class docutils literal"><span class="pre">DjangoItem</span></tt>:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; person = p.save()
&gt;&gt;&gt; person.name
&#39;John&#39;
&gt;&gt;&gt; person.age
&#39;22&#39;
&gt;&gt;&gt; person.id
1
</pre></div>
</div>
<p>The model is already saved when we call <tt class="xref py py-meth docutils literal"><span class="pre">save()</span></tt>, we
can prevent this by calling it with <tt class="docutils literal"><span class="pre">commit=False</span></tt>. We can use
<tt class="docutils literal"><span class="pre">commit=False</span></tt> in <tt class="xref py py-meth docutils literal"><span class="pre">save()</span></tt> method to obtain an unsaved model:</p>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; person = p.save(commit=False)
&gt;&gt;&gt; person.name
&#39;John&#39;
&gt;&gt;&gt; person.age
&#39;22&#39;
&gt;&gt;&gt; person.id
None
</pre></div>
</div>
<p>As said before, we can add other fields to the item:</p>
<div class="highlight-none"><div class="highlight"><pre>import scrapy
from scrapy.contrib.djangoitem import DjangoItem

class PersonItem(DjangoItem):
    django_model = Person
    sex = scrapy.Field()
</pre></div>
</div>
<div class="highlight-none"><div class="highlight"><pre>&gt;&gt;&gt; p = PersonItem()
&gt;&gt;&gt; p[&#39;name&#39;] = &#39;John&#39;
&gt;&gt;&gt; p[&#39;age&#39;] = &#39;22&#39;
&gt;&gt;&gt; p[&#39;sex&#39;] = &#39;M&#39;
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">fields added to the item won&#8217;t be taken into account when doing a <tt class="xref py py-meth docutils literal"><span class="pre">save()</span></tt></p>
</div>
<p>And we can override the fields of the model with your own:</p>
<div class="highlight-none"><div class="highlight"><pre>class PersonItem(DjangoItem):
    django_model = Person
    name = scrapy.Field(default=&#39;No Name&#39;)
</pre></div>
</div>
<p>This is useful to provide properties to the field, like a default or any other
property that your project uses.</p>
</div>
<div class="section" id="djangoitem-caveats">
<h4>DjangoItem caveats<a class="headerlink" href="#djangoitem-caveats" title="Permalink to this headline">¶</a></h4>
<p>DjangoItem is a rather convenient way to integrate Scrapy projects with Django
models, but bear in mind that Django ORM may not scale well if you scrape a lot
of items (ie. millions) with Scrapy. This is because a relational backend is
often not a good choice for a write intensive application (such as a web
crawler), specially if the database is highly normalized and with many indices.</p>
</div>
<div class="section" id="django-settings-set-up">
<h4>Django settings set up<a class="headerlink" href="#django-settings-set-up" title="Permalink to this headline">¶</a></h4>
<p>To use the Django models outside the Django application you need to set up the
<tt class="docutils literal"><span class="pre">DJANGO_SETTINGS_MODULE</span></tt> environment variable and &#8211;in most cases&#8211; modify
the <tt class="docutils literal"><span class="pre">PYTHONPATH</span></tt> environment variable to be able to import the settings
module.</p>
<p>There are many ways to do this depending on your use case and preferences.
Below is detailed one of the simplest ways to do it.</p>
<p>Suppose your Django project is named <tt class="docutils literal"><span class="pre">mysite</span></tt>, is located in the path
<tt class="docutils literal"><span class="pre">/home/projects/mysite</span></tt> and you have created an app <tt class="docutils literal"><span class="pre">myapp</span></tt> with the model
<tt class="docutils literal"><span class="pre">Person</span></tt>. That means your directory structure is something like this:</p>
<div class="highlight-none"><div class="highlight"><pre>/home/projects/mysite
├── manage.py
├── myapp
│   ├── __init__.py
│   ├── models.py
│   ├── tests.py
│   └── views.py
└── mysite
    ├── __init__.py
    ├── settings.py
    ├── urls.py
    └── wsgi.py
</pre></div>
</div>
<p>Then you need to add <tt class="docutils literal"><span class="pre">/home/projects/mysite</span></tt> to the <tt class="docutils literal"><span class="pre">PYTHONPATH</span></tt>
environment variable and set up the environment variable
<tt class="docutils literal"><span class="pre">DJANGO_SETTINGS_MODULE</span></tt> to <tt class="docutils literal"><span class="pre">mysite.settings</span></tt>. That can be done in your
Scrapy&#8217;s settings file by adding the lines below:</p>
<div class="highlight-none"><div class="highlight"><pre>import sys
sys.path.append(&#39;/home/projects/mysite&#39;)

import os
os.environ[&#39;DJANGO_SETTINGS_MODULE&#39;] = &#39;mysite.settings&#39;
</pre></div>
</div>
<p>Notice that we modify the <tt class="docutils literal"><span class="pre">sys.path</span></tt> variable instead the <tt class="docutils literal"><span class="pre">PYTHONPATH</span></tt>
environment variable as we are already within the python runtime. If everything
is right, you should be able to start the <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">shell</span></tt> command and import
the model <tt class="docutils literal"><span class="pre">Person</span></tt> (i.e. <tt class="docutils literal"><span class="pre">from</span> <span class="pre">myapp.models</span> <span class="pre">import</span> <span class="pre">Person</span></tt>).</p>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-faq"><em>Frequently Asked Questions</em></a></dt>
<dd>Get answers to most frequently asked questions.</dd>
<dt><a class="reference internal" href="index.html#document-topics/debug"><em>Debugging Spiders</em></a></dt>
<dd>Learn how to debug common problems of your scrapy spider.</dd>
<dt><a class="reference internal" href="index.html#document-topics/contracts"><em>Spiders Contracts</em></a></dt>
<dd>Learn how to use contracts for testing your spiders.</dd>
<dt><a class="reference internal" href="index.html#document-topics/practices"><em>Common Practices</em></a></dt>
<dd>Get familiar with some Scrapy common practices.</dd>
<dt><a class="reference internal" href="index.html#document-topics/broad-crawls"><em>Broad Crawls</em></a></dt>
<dd>Tune Scrapy for crawling a lot domains in parallel.</dd>
<dt><a class="reference internal" href="index.html#document-topics/firefox"><em>Using Firefox for scraping</em></a></dt>
<dd>Learn how to scrape with Firefox and some useful add-ons.</dd>
<dt><a class="reference internal" href="index.html#document-topics/firebug"><em>Using Firebug for scraping</em></a></dt>
<dd>Learn how to scrape efficiently using Firebug.</dd>
<dt><a class="reference internal" href="index.html#document-topics/leaks"><em>Debugging memory leaks</em></a></dt>
<dd>Learn how to find and get rid of memory leaks in your crawler.</dd>
<dt><a class="reference internal" href="index.html#document-topics/images"><em>Downloading Item Images</em></a></dt>
<dd>Download static images associated with your scraped items.</dd>
<dt><a class="reference internal" href="index.html#document-topics/ubuntu"><em>Ubuntu packages</em></a></dt>
<dd>Install latest Scrapy packages easily on Ubuntu</dd>
<dt><a class="reference internal" href="index.html#document-topics/scrapyd"><em>Scrapyd</em></a></dt>
<dd>Deploying your Scrapy project in production.</dd>
<dt><a class="reference internal" href="index.html#document-topics/autothrottle"><em>AutoThrottle extension</em></a></dt>
<dd>Adjust crawl rate dynamically based on load.</dd>
<dt><a class="reference internal" href="index.html#document-topics/benchmarking"><em>Benchmarking</em></a></dt>
<dd>Check how Scrapy performs on your hardware.</dd>
<dt><a class="reference internal" href="index.html#document-topics/jobs"><em>Jobs: pausing and resuming crawls</em></a></dt>
<dd>Learn how to pause and resume crawls for large spiders.</dd>
<dt><a class="reference internal" href="index.html#document-topics/djangoitem"><em>DjangoItem</em></a></dt>
<dd>Write scraped items using Django models.</dd>
</dl>
</div>
<div class="section" id="extending-scrapy">
<span id="id1"></span><h2>Extending Scrapy<a class="headerlink" href="#extending-scrapy" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-topics/architecture"></span><div class="section" id="architecture-overview">
<span id="topics-architecture"></span><h3>Architecture overview<a class="headerlink" href="#architecture-overview" title="Permalink to this headline">¶</a></h3>
<p>This document describes the architecture of Scrapy and how its components
interact.</p>
<div class="section" id="overview">
<h4>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h4>
<p>The following diagram shows an overview of the Scrapy architecture with its
components and an outline of the data flow that takes place inside the system
(shown by the green arrows). A brief description of the components is included
below with links for more detailed information about them. The data flow is
also described below.</p>
<a class="reference internal image-reference" href="_images/scrapy_architecture.png"><img alt="Scrapy architecture" src="_images/scrapy_architecture.png" style="width: 700px; height: 494px;" /></a>
</div>
<div class="section" id="components">
<h4>Components<a class="headerlink" href="#components" title="Permalink to this headline">¶</a></h4>
<div class="section" id="scrapy-engine">
<h5>Scrapy Engine<a class="headerlink" href="#scrapy-engine" title="Permalink to this headline">¶</a></h5>
<p>The engine is responsible for controlling the data flow between all components
of the system, and triggering events when certain actions occur. See the Data
Flow section below for more details.</p>
</div>
<div class="section" id="scheduler">
<h5>Scheduler<a class="headerlink" href="#scheduler" title="Permalink to this headline">¶</a></h5>
<p>The Scheduler receives requests from the engine and enqueues them for feeding
them later (also to the engine) when the engine requests them.</p>
</div>
<div class="section" id="downloader">
<h5>Downloader<a class="headerlink" href="#downloader" title="Permalink to this headline">¶</a></h5>
<p>The Downloader is responsible for fetching web pages and feeding them to the
engine which, in turn, feeds them to the spiders.</p>
</div>
<div class="section" id="spiders">
<h5>Spiders<a class="headerlink" href="#spiders" title="Permalink to this headline">¶</a></h5>
<p>Spiders are custom classes written by Scrapy users to parse responses and
extract items (aka scraped items) from them or additional URLs (requests) to
follow. Each spider is able to handle a specific domain (or group of domains).
For more information see <a class="reference internal" href="index.html#topics-spiders"><em>Spiders</em></a>.</p>
</div>
<div class="section" id="item-pipeline">
<h5>Item Pipeline<a class="headerlink" href="#item-pipeline" title="Permalink to this headline">¶</a></h5>
<p>The Item Pipeline is responsible for processing the items once they have been
extracted (or scraped) by the spiders. Typical tasks include cleansing,
validation and persistence (like storing the item in a database). For more
information see <a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a>.</p>
</div>
<div class="section" id="downloader-middlewares">
<h5>Downloader middlewares<a class="headerlink" href="#downloader-middlewares" title="Permalink to this headline">¶</a></h5>
<p>Downloader middlewares are specific hooks that sit between the Engine and the
Downloader and process requests when they pass from the Engine to the
Downloader, and responses that pass from Downloader to the Engine. They provide
a convenient mechanism for extending Scrapy functionality by plugging custom
code. For more information see <a class="reference internal" href="index.html#topics-downloader-middleware"><em>Downloader Middleware</em></a>.</p>
</div>
<div class="section" id="spider-middlewares">
<h5>Spider middlewares<a class="headerlink" href="#spider-middlewares" title="Permalink to this headline">¶</a></h5>
<p>Spider middlewares are specific hooks that sit between the Engine and the
Spiders and are able to process spider input (responses) and output (items and
requests). They provide a convenient mechanism for extending Scrapy
functionality by plugging custom code. For more information see
<a class="reference internal" href="index.html#topics-spider-middleware"><em>Spider Middleware</em></a>.</p>
</div>
</div>
<div class="section" id="data-flow">
<h4>Data flow<a class="headerlink" href="#data-flow" title="Permalink to this headline">¶</a></h4>
<p>The data flow in Scrapy is controlled by the execution engine, and goes like
this:</p>
<ol class="arabic simple">
<li>The Engine opens a domain, locates the Spider that handles that domain, and
asks the spider for the first URLs to crawl.</li>
<li>The Engine gets the first URLs to crawl from the Spider and schedules them
in the Scheduler, as Requests.</li>
<li>The Engine asks the Scheduler for the next URLs to crawl.</li>
<li>The Scheduler returns the next URLs to crawl to the Engine and the Engine
sends them to the Downloader, passing through the Downloader Middleware
(request direction).</li>
<li>Once the page finishes downloading the Downloader generates a Response (with
that page) and sends it to the Engine, passing through the Downloader
Middleware (response direction).</li>
<li>The Engine receives the Response from the Downloader and sends it to the
Spider for processing, passing through the Spider Middleware (input direction).</li>
<li>The Spider processes the Response and returns scraped Items and new Requests
(to follow) to the Engine.</li>
<li>The Engine sends scraped Items (returned by the Spider) to the Item Pipeline
and Requests (returned by spider) to the Scheduler</li>
<li>The process repeats (from step 2) until there are no more requests from the
Scheduler, and the Engine closes the domain.</li>
</ol>
</div>
<div class="section" id="event-driven-networking">
<h4>Event-driven networking<a class="headerlink" href="#event-driven-networking" title="Permalink to this headline">¶</a></h4>
<p>Scrapy is written with <a class="reference external" href="http://twistedmatrix.com/trac/">Twisted</a>, a popular event-driven networking framework
for Python. Thus, it&#8217;s implemented using a non-blocking (aka asynchronous) code
for concurrency.</p>
<p>For more information about asynchronous programming and Twisted see these
links:</p>
<ul class="simple">
<li><a class="reference external" href="http://twistedmatrix.com/documents/current/core/howto/defer-intro.html">Introduction to Deferreds in Twisted</a></li>
<li><a class="reference external" href="http://jessenoller.com/2009/02/11/twisted-hello-asynchronous-programming/">Twisted - hello, asynchronous programming</a></li>
</ul>
</div>
</div>
<span id="document-topics/downloader-middleware"></span><div class="section" id="downloader-middleware">
<span id="topics-downloader-middleware"></span><h3>Downloader Middleware<a class="headerlink" href="#downloader-middleware" title="Permalink to this headline">¶</a></h3>
<p>The downloader middleware is a framework of hooks into Scrapy&#8217;s
request/response processing.  It&#8217;s a light, low-level system for globally
altering Scrapy&#8217;s requests and responses.</p>
<div class="section" id="activating-a-downloader-middleware">
<span id="topics-downloader-middleware-setting"></span><h4>Activating a downloader middleware<a class="headerlink" href="#activating-a-downloader-middleware" title="Permalink to this headline">¶</a></h4>
<p>To activate a downloader middleware component, add it to the
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></tt></a> setting, which is a dict whose keys are the
middleware class paths and their values are the middleware orders.</p>
<p>Here&#8217;s an example:</p>
<div class="highlight-none"><div class="highlight"><pre>DOWNLOADER_MIDDLEWARES = {
    &#39;myproject.middlewares.CustomDownloaderMiddleware&#39;: 543,
}
</pre></div>
</div>
<p>The <a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></tt></a> setting is merged with the
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></tt></a> setting defined in Scrapy (and not meant to
be overridden) and then sorted by order to get the final sorted list of enabled
middlewares: the first middleware is the one closer to the engine and the last
is the one closer to the downloader.</p>
<p>To decide which order to assign to your middleware see the
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></tt></a> setting and pick a value according to
where you want to insert the middleware. The order does matter because each
middleware performs a different action and your middleware could depend on some
previous (or subsequent) middleware being applied.</p>
<p>If you want to disable a built-in middleware (the ones defined in
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></tt></a> and enabled by default) you must define it
in your project&#8217;s <a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></tt></a> setting and assign <cite>None</cite>
as its value.  For example, if you want to disable the user-agent middleware:</p>
<div class="highlight-none"><div class="highlight"><pre>DOWNLOADER_MIDDLEWARES = {
    &#39;myproject.middlewares.CustomDownloaderMiddleware&#39;: 543,
    &#39;scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware&#39;: None,
}
</pre></div>
</div>
<p>Finally, keep in mind that some middlewares may need to be enabled through a
particular setting. See each middleware documentation for more info.</p>
</div>
<div class="section" id="writing-your-own-downloader-middleware">
<h4>Writing your own downloader middleware<a class="headerlink" href="#writing-your-own-downloader-middleware" title="Permalink to this headline">¶</a></h4>
<p>Writing your own downloader middleware is easy. Each middleware component is a
single Python class that defines one or more of the following methods:</p>
<span class="target" id="module-scrapy.contrib.downloadermiddleware"></span><dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.DownloaderMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.</tt><tt class="descname">DownloaderMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request">
<tt class="descname">process_request</tt><big>(</big><em>request</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for each request that goes through the download
middleware.</p>
<p><a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request"><tt class="xref py py-meth docutils literal"><span class="pre">process_request()</span></tt></a> should either: return <tt class="docutils literal"><span class="pre">None</span></tt>, return a
<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object, return a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a>
object, or raise <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><tt class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></tt></a>.</p>
<p>If it returns <tt class="docutils literal"><span class="pre">None</span></tt>, Scrapy will continue processing this request, executing all
other middlewares until, finally, the appropriate downloader handler is called
the request performed (and its response downloaded).</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object, Scrapy won&#8217;t bother
calling <em>any</em> other <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request"><tt class="xref py py-meth docutils literal"><span class="pre">process_request()</span></tt></a> or <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> methods,
or the appropriate download function; it&#8217;ll return that response. The <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response"><tt class="xref py py-meth docutils literal"><span class="pre">process_response()</span></tt></a>
methods of installed middleware is always called on every response.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object, Scrapy will stop calling
process_request methods and reschedule the returned request. Once the newly returned
request is performed, the appropriate middleware chain will be called on
the downloaded response.</p>
<p>If it raises an <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><tt class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></tt></a> exception, the
<a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> methods of installed downloader middleware will be called.
If none of them handle the exception, the errback function of the request
(<tt class="docutils literal"><span class="pre">Request.errback</span></tt>) is called. If no code handles the raised exception, it is
ignored and not logged (unlike other exceptions).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object) &#8211; the request being processed</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider for which this request is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response">
<tt class="descname">process_response</tt><big>(</big><em>request</em>, <em>response</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response"><tt class="xref py py-meth docutils literal"><span class="pre">process_response()</span></tt></a> should either: return a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a>
object, return a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object or
raise a <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><tt class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></tt></a> exception.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> (it could be the same given
response, or a brand-new one), that response will continue to be processed
with the <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response"><tt class="xref py py-meth docutils literal"><span class="pre">process_response()</span></tt></a> of the next middleware in the chain.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object, the middleware chain is
halted and the returned request is rescheduled to be downloaded in the future.
This is the same behavior as if a request is returned from <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request"><tt class="xref py py-meth docutils literal"><span class="pre">process_request()</span></tt></a>.</p>
<p>If it raises an <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><tt class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></tt></a> exception, the errback
function of the request (<tt class="docutils literal"><span class="pre">Request.errback</span></tt>) is called. If no code handles the raised
exception, it is ignored and not logged (unlike other exceptions).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (is a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object) &#8211; the request that originated the response</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object) &#8211; the response being processed</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider for which this response is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception">
<tt class="descname">process_exception</tt><big>(</big><em>request</em>, <em>exception</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="Permalink to this definition">¶</a></dt>
<dd><p>Scrapy calls <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> when a download handler
or a <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request"><tt class="xref py py-meth docutils literal"><span class="pre">process_request()</span></tt></a> (from a downloader middleware) raises an
exception (including an <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><tt class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></tt></a> exception)</p>
<p><a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> should return: either <tt class="docutils literal"><span class="pre">None</span></tt>,
a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object, or a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object.</p>
<p>If it returns <tt class="docutils literal"><span class="pre">None</span></tt>, Scrapy will continue processing this exception,
executing any other <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> methods of installed middleware,
until no middleware is left and the default exception handling kicks in.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object, the <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response"><tt class="xref py py-meth docutils literal"><span class="pre">process_response()</span></tt></a>
method chain of installed middleware is started, and Scrapy won&#8217;t bother calling
any other <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> methods of middleware.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object, the returned request is
rescheduled to be downloaded in the future. This stops the execution of
<a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> methods of the middleware the same as returning a
response would.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (is a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object) &#8211; the request that generated the exception</li>
<li><strong>exception</strong> (an <tt class="docutils literal"><span class="pre">Exception</span></tt> object) &#8211; the raised exception</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider for which this request is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="built-in-downloader-middleware-reference">
<span id="topics-downloader-middleware-ref"></span><h4>Built-in downloader middleware reference<a class="headerlink" href="#built-in-downloader-middleware-reference" title="Permalink to this headline">¶</a></h4>
<p>This page describes all downloader middleware components that come with
Scrapy. For information on how to use them and how to write your own downloader
middleware, see the <a class="reference internal" href="index.html#topics-downloader-middleware"><em>downloader middleware usage guide</em></a>.</p>
<p>For a list of the components enabled by default (and their orders) see the
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></tt></a> setting.</p>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.cookies">
<span id="cookiesmiddleware"></span><span id="cookies-mw"></span><h5>CookiesMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.cookies" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.cookies.</tt><tt class="descname">CookiesMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware enables working with sites that require cookies, such as
those that use sessions. It keeps track of cookies sent by web servers, and
send them back on subsequent requests (from that spider), just like web
browsers do.</p>
</dd></dl>

<p>The following settings can be used to configure the cookie middleware:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-COOKIES_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">COOKIES_ENABLED</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-COOKIES_DEBUG"><tt class="xref std std-setting docutils literal"><span class="pre">COOKIES_DEBUG</span></tt></a></li>
</ul>
<div class="section" id="multiple-cookie-sessions-per-spider">
<span id="std:reqmeta-cookiejar"></span><h6>Multiple cookie sessions per spider<a class="headerlink" href="#multiple-cookie-sessions-per-spider" title="Permalink to this headline">¶</a></h6>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<p>There is support for keeping multiple cookie sessions per spider by using the
<a class="reference internal" href="index.html#std:reqmeta-cookiejar"><tt class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></tt></a> Request meta key. By default it uses a single cookie jar
(session), but you can pass an identifier to use different ones.</p>
<p>For example:</p>
<div class="highlight-none"><div class="highlight"><pre>for i, url in enumerate(urls):
    yield scrapy.Request(&quot;http://www.example.com&quot;, meta={&#39;cookiejar&#39;: i},
        callback=self.parse_page)
</pre></div>
</div>
<p>Keep in mind that the <a class="reference internal" href="index.html#std:reqmeta-cookiejar"><tt class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></tt></a> meta key is not &#8220;sticky&#8221;. You need to keep
passing it along on subsequent requests. For example:</p>
<div class="highlight-none"><div class="highlight"><pre>def parse_page(self, response):
    # do some processing
    return scrapy.Request(&quot;http://www.example.com/otherpage&quot;,
        meta={&#39;cookiejar&#39;: response.meta[&#39;cookiejar&#39;]},
        callback=self.parse_other_page)
</pre></div>
</div>
</div>
<div class="section" id="cookies-enabled">
<span id="std:setting-COOKIES_ENABLED"></span><h6>COOKIES_ENABLED<a class="headerlink" href="#cookies-enabled" title="Permalink to this headline">¶</a></h6>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Whether to enable the cookies middleware. If disabled, no cookies will be sent
to web servers.</p>
</div>
<div class="section" id="cookies-debug">
<span id="std:setting-COOKIES_DEBUG"></span><h6>COOKIES_DEBUG<a class="headerlink" href="#cookies-debug" title="Permalink to this headline">¶</a></h6>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>If enabled, Scrapy will log all cookies sent in requests (ie. <tt class="docutils literal"><span class="pre">Cookie</span></tt>
header) and all cookies received in responses (ie. <tt class="docutils literal"><span class="pre">Set-Cookie</span></tt> header).</p>
<p>Here&#8217;s an example of a log with <a class="reference internal" href="index.html#std:setting-COOKIES_DEBUG"><tt class="xref std std-setting docutils literal"><span class="pre">COOKIES_DEBUG</span></tt></a> enabled:</p>
<div class="highlight-none"><div class="highlight"><pre>2011-04-06 14:35:10-0300 [diningcity] INFO: Spider opened
2011-04-06 14:35:10-0300 [diningcity] DEBUG: Sending cookies to: &lt;GET http://www.diningcity.com/netherlands/index.html&gt;
        Cookie: clientlanguage_nl=en_EN
2011-04-06 14:35:14-0300 [diningcity] DEBUG: Received cookies from: &lt;200 http://www.diningcity.com/netherlands/index.html&gt;
        Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/
        Set-Cookie: ip_isocode=US
        Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/
2011-04-06 14:49:50-0300 [diningcity] DEBUG: Crawled (200) &lt;GET http://www.diningcity.com/netherlands/index.html&gt; (referer: None)
[...]
</pre></div>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.defaultheaders">
<span id="defaultheadersmiddleware"></span><h5>DefaultHeadersMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.defaultheaders" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.defaultheaders.</tt><tt class="descname">DefaultHeadersMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets all default requests headers specified in the
<a class="reference internal" href="index.html#std:setting-DEFAULT_REQUEST_HEADERS"><tt class="xref std std-setting docutils literal"><span class="pre">DEFAULT_REQUEST_HEADERS</span></tt></a> setting.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.downloadtimeout">
<span id="downloadtimeoutmiddleware"></span><h5>DownloadTimeoutMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.downloadtimeout" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.downloadtimeout.</tt><tt class="descname">DownloadTimeoutMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets the download timeout for requests specified in the
<a class="reference internal" href="index.html#std:setting-DOWNLOAD_TIMEOUT"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_TIMEOUT</span></tt></a> setting.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.httpauth">
<span id="httpauthmiddleware"></span><h5>HttpAuthMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.httpauth" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.httpauth.</tt><tt class="descname">HttpAuthMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware authenticates all requests generated from certain spiders
using <a class="reference external" href="http://en.wikipedia.org/wiki/Basic_access_authentication">Basic access authentication</a> (aka. HTTP auth).</p>
<p>To enable HTTP authentication from certain spiders, set the <tt class="docutils literal"><span class="pre">http_user</span></tt>
and <tt class="docutils literal"><span class="pre">http_pass</span></tt> attributes of those spiders.</p>
<p>Example:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy.contrib.spiders import CrawlSpider

class SomeIntranetSiteSpider(CrawlSpider):

    http_user = &#39;someuser&#39;
    http_pass = &#39;somepass&#39;
    name = &#39;intranet.example.com&#39;

    # .. rest of the spider code omitted ...
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.httpcache">
<span id="httpcachemiddleware"></span><h5>HttpCacheMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.httpcache" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.httpcache.</tt><tt class="descname">HttpCacheMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware provides low-level cache to all HTTP requests and responses.
It has to be combined with a cache storage backend as well as a cache policy.</p>
<p>Scrapy ships with two HTTP cache storage backends:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#httpcache-storage-fs"><em>Filesystem storage backend (default)</em></a></li>
<li><a class="reference internal" href="index.html#httpcache-storage-dbm"><em>DBM storage backend</em></a></li>
</ul>
</div></blockquote>
<p>You can change the HTTP cache storage backend with the <a class="reference internal" href="index.html#std:setting-HTTPCACHE_STORAGE"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></tt></a>
setting. Or you can also implement your own storage backend.</p>
<p>Scrapy ships with two HTTP cache policies:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#httpcache-policy-rfc2616"><em>RFC2616 policy</em></a></li>
<li><a class="reference internal" href="index.html#httpcache-policy-dummy"><em>Dummy policy (default)</em></a></li>
</ul>
</div></blockquote>
<p>You can change the HTTP cache policy with the <a class="reference internal" href="index.html#std:setting-HTTPCACHE_POLICY"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_POLICY</span></tt></a>
setting. Or you can also implement your own policy.</p>
</dd></dl>

<div class="section" id="dummy-policy-default">
<span id="httpcache-policy-dummy"></span><h6>Dummy policy (default)<a class="headerlink" href="#dummy-policy-default" title="Permalink to this headline">¶</a></h6>
<p>This policy has no awareness of any HTTP Cache-Control directives.
Every request and its corresponding response are cached.  When the same
request is seen again, the response is returned without transferring
anything from the Internet.</p>
<p>The Dummy policy is useful for testing spiders faster (without having
to wait for downloads every time) and for trying your spider offline,
when an Internet connection is not available. The goal is to be able to
&#8220;replay&#8221; a spider run <em>exactly as it ran before</em>.</p>
<p>In order to use this policy, set:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_POLICY"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_POLICY</span></tt></a> to <tt class="docutils literal"><span class="pre">scrapy.contrib.httpcache.DummyPolicy</span></tt></li>
</ul>
</div>
<div class="section" id="rfc2616-policy">
<span id="httpcache-policy-rfc2616"></span><h6>RFC2616 policy<a class="headerlink" href="#rfc2616-policy" title="Permalink to this headline">¶</a></h6>
<p>This policy provides a RFC2616 compliant HTTP cache, i.e. with HTTP
Cache-Control awareness, aimed at production and used in continuous
runs to avoid downloading unmodified data (to save bandwidth and speed up crawls).</p>
<p>what is implemented:</p>
<ul class="simple">
<li>Do not attempt to store responses/requests with <cite>no-store</cite> cache-control directive set</li>
<li>Do not serve responses from cache if <cite>no-cache</cite> cache-control directive is set even for fresh responses</li>
<li>Compute freshness lifetime from <cite>max-age</cite> cache-control directive</li>
<li>Compute freshness lifetime from <cite>Expires</cite> response header</li>
<li>Compute freshness lifetime from <cite>Last-Modified</cite> response header (heuristic used by Firefox)</li>
<li>Compute current age from <cite>Age</cite> response header</li>
<li>Compute current age from <cite>Date</cite> header</li>
<li>Revalidate stale responses based on <cite>Last-Modified</cite> response header</li>
<li>Revalidate stale responses based on <cite>ETag</cite> response header</li>
<li>Set <cite>Date</cite> header for any received response missing it</li>
</ul>
<p>what is missing:</p>
<ul class="simple">
<li><cite>Pragma: no-cache</cite> support <a class="reference external" href="http://www.mnot.net/cache_docs/#PRAGMA">http://www.mnot.net/cache_docs/#PRAGMA</a></li>
<li><cite>Vary</cite> header support <a class="reference external" href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6">http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6</a></li>
<li>Invalidation after updates or deletes <a class="reference external" href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10">http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10</a></li>
<li>... probably others ..</li>
</ul>
<p>In order to use this policy, set:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_POLICY"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_POLICY</span></tt></a> to <tt class="docutils literal"><span class="pre">scrapy.contrib.httpcache.RFC2616Policy</span></tt></li>
</ul>
</div>
<div class="section" id="filesystem-storage-backend-default">
<span id="httpcache-storage-fs"></span><h6>Filesystem storage backend (default)<a class="headerlink" href="#filesystem-storage-backend-default" title="Permalink to this headline">¶</a></h6>
<p>File system storage backend is available for the HTTP cache middleware.</p>
<p>In order to use this storage backend, set:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_STORAGE"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></tt></a> to <tt class="docutils literal"><span class="pre">scrapy.contrib.httpcache.FilesystemCacheStorage</span></tt></li>
</ul>
<p>Each request/response pair is stored in a different directory containing
the following files:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">request_body</span></tt> - the plain request body</li>
<li><tt class="docutils literal"><span class="pre">request_headers</span></tt> - the request headers (in raw HTTP format)</li>
<li><tt class="docutils literal"><span class="pre">response_body</span></tt> - the plain response body</li>
<li><tt class="docutils literal"><span class="pre">response_headers</span></tt> - the request headers (in raw HTTP format)</li>
<li><tt class="docutils literal"><span class="pre">meta</span></tt> - some metadata of this cache resource in Python <tt class="docutils literal"><span class="pre">repr()</span></tt> format
(grep-friendly format)</li>
<li><tt class="docutils literal"><span class="pre">pickled_meta</span></tt> - the same metadata in <tt class="docutils literal"><span class="pre">meta</span></tt> but pickled for more
efficient deserialization</li>
</ul>
</div></blockquote>
<p>The directory name is made from the request fingerprint (see
<tt class="docutils literal"><span class="pre">scrapy.utils.request.fingerprint</span></tt>), and one level of subdirectories is
used to avoid creating too many files into the same directory (which is
inefficient in many file systems). An example directory could be:</p>
<div class="highlight-none"><div class="highlight"><pre>/path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7
</pre></div>
</div>
</div>
<div class="section" id="dbm-storage-backend">
<span id="httpcache-storage-dbm"></span><h6>DBM storage backend<a class="headerlink" href="#dbm-storage-backend" title="Permalink to this headline">¶</a></h6>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>A <a class="reference external" href="http://en.wikipedia.org/wiki/Dbm">DBM</a> storage backend is also available for the HTTP cache middleware.</p>
<p>By default, it uses the <a class="reference external" href="http://docs.python.org/library/anydbm.html">anydbm</a> module, but you can change it with the
<a class="reference internal" href="index.html#std:setting-HTTPCACHE_DBM_MODULE"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_DBM_MODULE</span></tt></a> setting.</p>
<p>In order to use this storage backend, set:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_STORAGE"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></tt></a> to <tt class="docutils literal"><span class="pre">scrapy.contrib.httpcache.DbmCacheStorage</span></tt></li>
</ul>
</div>
<div class="section" id="leveldb-storage-backend">
<span id="httpcache-storage-leveldb"></span><h6>LevelDB storage backend<a class="headerlink" href="#leveldb-storage-backend" title="Permalink to this headline">¶</a></h6>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.23.</span></p>
</div>
<p>A <a class="reference external" href="http://code.google.com/p/leveldb/">LevelDB</a> storage backend is also available for the HTTP cache middleware.</p>
<p>This backend is not recommended for development because only one process can
access LevelDB databases at the same time, so you can&#8217;t run a crawl and open
the scrapy shell in parallel for the same spider.</p>
<p>In order to use this storage backend:</p>
<ul class="simple">
<li>set <a class="reference internal" href="index.html#std:setting-HTTPCACHE_STORAGE"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></tt></a> to <tt class="docutils literal"><span class="pre">scrapy.contrib.httpcache.LeveldbCacheStorage</span></tt></li>
<li>install <a class="reference external" href="http://pypi.python.org/pypi/leveldb">LevelDB python bindings</a> like <tt class="docutils literal"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">leveldb</span></tt></li>
</ul>
</div>
<div class="section" id="httpcache-middleware-settings">
<h6>HTTPCache middleware settings<a class="headerlink" href="#httpcache-middleware-settings" title="Permalink to this headline">¶</a></h6>
<p>The <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware" title="scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">HttpCacheMiddleware</span></tt></a> can be configured through the following
settings:</p>
<div class="section" id="httpcache-enabled">
<span id="std:setting-HTTPCACHE_ENABLED"></span><h7>HTTPCACHE_ENABLED<a class="headerlink" href="#httpcache-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.11.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Whether the HTTP cache will be enabled.</p>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 0.11: </span>Before 0.11, <a class="reference internal" href="index.html#std:setting-HTTPCACHE_DIR"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_DIR</span></tt></a> was used to enable cache.</p>
</div>
</div>
<div class="section" id="httpcache-expiration-secs">
<span id="std:setting-HTTPCACHE_EXPIRATION_SECS"></span><h7>HTTPCACHE_EXPIRATION_SECS<a class="headerlink" href="#httpcache-expiration-secs" title="Permalink to this headline">¶</a></h7>
<p>Default: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>Expiration time for cached requests, in seconds.</p>
<p>Cached requests older than this time will be re-downloaded. If zero, cached
requests will never expire.</p>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 0.11: </span>Before 0.11, zero meant cached requests always expire.</p>
</div>
</div>
<div class="section" id="httpcache-dir">
<span id="std:setting-HTTPCACHE_DIR"></span><h7>HTTPCACHE_DIR<a class="headerlink" href="#httpcache-dir" title="Permalink to this headline">¶</a></h7>
<p>Default: <tt class="docutils literal"><span class="pre">'httpcache'</span></tt></p>
<p>The directory to use for storing the (low-level) HTTP cache. If empty, the HTTP
cache will be disabled. If a relative path is given, is taken relative to the
project data dir. For more info see: <a class="reference internal" href="index.html#topics-project-structure"><em>Default structure of Scrapy projects</em></a>.</p>
</div>
<div class="section" id="httpcache-ignore-http-codes">
<span id="std:setting-HTTPCACHE_IGNORE_HTTP_CODES"></span><h7>HTTPCACHE_IGNORE_HTTP_CODES<a class="headerlink" href="#httpcache-ignore-http-codes" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">[]</span></tt></p>
<p>Don&#8217;t cache response with these HTTP codes.</p>
</div>
<div class="section" id="httpcache-ignore-missing">
<span id="std:setting-HTTPCACHE_IGNORE_MISSING"></span><h7>HTTPCACHE_IGNORE_MISSING<a class="headerlink" href="#httpcache-ignore-missing" title="Permalink to this headline">¶</a></h7>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>If enabled, requests not found in the cache will be ignored instead of downloaded.</p>
</div>
<div class="section" id="httpcache-ignore-schemes">
<span id="std:setting-HTTPCACHE_IGNORE_SCHEMES"></span><h7>HTTPCACHE_IGNORE_SCHEMES<a class="headerlink" href="#httpcache-ignore-schemes" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">['file']</span></tt></p>
<p>Don&#8217;t cache responses with these URI schemes.</p>
</div>
<div class="section" id="httpcache-storage">
<span id="std:setting-HTTPCACHE_STORAGE"></span><h7>HTTPCACHE_STORAGE<a class="headerlink" href="#httpcache-storage" title="Permalink to this headline">¶</a></h7>
<p>Default: <tt class="docutils literal"><span class="pre">'scrapy.contrib.httpcache.FilesystemCacheStorage'</span></tt></p>
<p>The class which implements the cache storage backend.</p>
</div>
<div class="section" id="httpcache-dbm-module">
<span id="std:setting-HTTPCACHE_DBM_MODULE"></span><h7>HTTPCACHE_DBM_MODULE<a class="headerlink" href="#httpcache-dbm-module" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">'anydbm'</span></tt></p>
<p>The database module to use in the <a class="reference internal" href="index.html#httpcache-storage-dbm"><em>DBM storage backend</em></a>. This setting is specific to the DBM backend.</p>
</div>
<div class="section" id="httpcache-policy">
<span id="std:setting-HTTPCACHE_POLICY"></span><h7>HTTPCACHE_POLICY<a class="headerlink" href="#httpcache-policy" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">'scrapy.contrib.httpcache.DummyPolicy'</span></tt></p>
<p>The class which implements the cache policy.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.httpcompression">
<span id="httpcompressionmiddleware"></span><h5>HttpCompressionMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.httpcompression" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.httpcompression.</tt><tt class="descname">HttpCompressionMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware allows compressed (gzip, deflate) traffic to be
sent/received from web sites.</p>
</dd></dl>

<div class="section" id="httpcompressionmiddleware-settings">
<h6>HttpCompressionMiddleware Settings<a class="headerlink" href="#httpcompressionmiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="compression-enabled">
<span id="std:setting-COMPRESSION_ENABLED"></span><h7>COMPRESSION_ENABLED<a class="headerlink" href="#compression-enabled" title="Permalink to this headline">¶</a></h7>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Whether the Compression middleware will be enabled.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.chunked">
<span id="chunkedtransfermiddleware"></span><h5>ChunkedTransferMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.chunked" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.chunked.</tt><tt class="descname">ChunkedTransferMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware adds support for <a class="reference external" href="http://en.wikipedia.org/wiki/Chunked_transfer_encoding">chunked transfer encoding</a></p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.httpproxy">
<span id="httpproxymiddleware"></span><h5>HttpProxyMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.httpproxy" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.8.</span></p>
</div>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.httpproxy.</tt><tt class="descname">HttpProxyMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets the HTTP proxy to use for requests, by setting the
<tt class="docutils literal"><span class="pre">proxy</span></tt> meta value to <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> objects.</p>
<p>Like the Python standard library modules <a class="reference external" href="http://docs.python.org/library/urllib.html">urllib</a> and <a class="reference external" href="http://docs.python.org/library/urllib2.html">urllib2</a>, it obeys
the following environment variables:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">http_proxy</span></tt></li>
<li><tt class="docutils literal"><span class="pre">https_proxy</span></tt></li>
<li><tt class="docutils literal"><span class="pre">no_proxy</span></tt></li>
</ul>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.redirect">
<span id="redirectmiddleware"></span><h5>RedirectMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.redirect" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.redirect.</tt><tt class="descname">RedirectMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware handles redirection of requests based on response status.</p>
</dd></dl>

<p id="std:reqmeta-redirect_urls">The urls which the request goes through (while being redirected) can be found
in the <tt class="docutils literal"><span class="pre">redirect_urls</span></tt> <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> key.</p>
<p>The <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware" title="scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">RedirectMiddleware</span></tt></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-REDIRECT_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">REDIRECT_ENABLED</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-REDIRECT_MAX_TIMES"><tt class="xref std std-setting docutils literal"><span class="pre">REDIRECT_MAX_TIMES</span></tt></a></li>
</ul>
<p id="std:reqmeta-dont_redirect">If <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> contains the
<tt class="docutils literal"><span class="pre">dont_redirect</span></tt> key, the request will be ignored by this middleware.</p>
<div class="section" id="redirectmiddleware-settings">
<h6>RedirectMiddleware settings<a class="headerlink" href="#redirectmiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="redirect-enabled">
<span id="std:setting-REDIRECT_ENABLED"></span><h7>REDIRECT_ENABLED<a class="headerlink" href="#redirect-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Whether the Redirect middleware will be enabled.</p>
</div>
<div class="section" id="redirect-max-times">
<span id="std:setting-REDIRECT_MAX_TIMES"></span><h7>REDIRECT_MAX_TIMES<a class="headerlink" href="#redirect-max-times" title="Permalink to this headline">¶</a></h7>
<p>Default: <tt class="docutils literal"><span class="pre">20</span></tt></p>
<p>The maximum number of redirections that will be follow for a single request.</p>
</div>
</div>
</div>
<div class="section" id="metarefreshmiddleware">
<h5>MetaRefreshMiddleware<a class="headerlink" href="#metarefreshmiddleware" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.redirect.</tt><tt class="descname">MetaRefreshMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware handles redirection of requests based on meta-refresh html tag.</p>
</dd></dl>

<p>The <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware" title="scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">MetaRefreshMiddleware</span></tt></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-METAREFRESH_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">METAREFRESH_ENABLED</span></tt></a></li>
<li><tt class="xref std std-setting docutils literal"><span class="pre">METAREFRESH_MAXDELAY</span></tt></li>
</ul>
<p>This middleware obey <a class="reference internal" href="index.html#std:setting-REDIRECT_MAX_TIMES"><tt class="xref std std-setting docutils literal"><span class="pre">REDIRECT_MAX_TIMES</span></tt></a> setting, <a class="reference internal" href="index.html#std:reqmeta-dont_redirect"><tt class="xref std std-reqmeta docutils literal"><span class="pre">dont_redirect</span></tt></a>
and <a class="reference internal" href="index.html#std:reqmeta-redirect_urls"><tt class="xref std std-reqmeta docutils literal"><span class="pre">redirect_urls</span></tt></a> request meta keys as described for <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware" title="scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">RedirectMiddleware</span></tt></a></p>
<div class="section" id="metarefreshmiddleware-settings">
<h6>MetaRefreshMiddleware settings<a class="headerlink" href="#metarefreshmiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="metarefresh-enabled">
<span id="std:setting-METAREFRESH_ENABLED"></span><h7>METAREFRESH_ENABLED<a class="headerlink" href="#metarefresh-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Whether the Meta Refresh middleware will be enabled.</p>
</div>
<div class="section" id="redirect-max-metarefresh-delay">
<span id="std:setting-REDIRECT_MAX_METAREFRESH_DELAY"></span><h7>REDIRECT_MAX_METAREFRESH_DELAY<a class="headerlink" href="#redirect-max-metarefresh-delay" title="Permalink to this headline">¶</a></h7>
<p>Default: <tt class="docutils literal"><span class="pre">100</span></tt></p>
<p>The maximum meta-refresh delay (in seconds) to follow the redirection.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.retry">
<span id="retrymiddleware"></span><h5>RetryMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.retry" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.retry.RetryMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.retry.</tt><tt class="descname">RetryMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.retry.RetryMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>A middlware to retry failed requests that are potentially caused by
temporary problems such as a connection timeout or HTTP 500 error.</p>
</dd></dl>

<p>Failed pages are collected on the scraping process and rescheduled at the
end, once the spider has finished crawling all regular (non failed) pages.
Once there are no more failed pages to retry, this middleware sends a signal
(retry_complete), so other extensions could connect to that signal.</p>
<p>The <a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.retry.RetryMiddleware" title="scrapy.contrib.downloadermiddleware.retry.RetryMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">RetryMiddleware</span></tt></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-RETRY_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">RETRY_ENABLED</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-RETRY_TIMES"><tt class="xref std std-setting docutils literal"><span class="pre">RETRY_TIMES</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-RETRY_HTTP_CODES"><tt class="xref std std-setting docutils literal"><span class="pre">RETRY_HTTP_CODES</span></tt></a></li>
</ul>
<p>About HTTP errors to consider:</p>
<p>You may want to remove 400 from <a class="reference internal" href="index.html#std:setting-RETRY_HTTP_CODES"><tt class="xref std std-setting docutils literal"><span class="pre">RETRY_HTTP_CODES</span></tt></a>, if you stick to the
HTTP protocol. It&#8217;s included by default because it&#8217;s a common code used
to indicate server overload, which would be something we want to retry.</p>
<p id="std:reqmeta-dont_retry">If <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> contains the <tt class="docutils literal"><span class="pre">dont_retry</span></tt>
key, the request will be ignored by this middleware.</p>
<div class="section" id="retrymiddleware-settings">
<h6>RetryMiddleware Settings<a class="headerlink" href="#retrymiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="retry-enabled">
<span id="std:setting-RETRY_ENABLED"></span><h7>RETRY_ENABLED<a class="headerlink" href="#retry-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Whether the Retry middleware will be enabled.</p>
</div>
<div class="section" id="retry-times">
<span id="std:setting-RETRY_TIMES"></span><h7>RETRY_TIMES<a class="headerlink" href="#retry-times" title="Permalink to this headline">¶</a></h7>
<p>Default: <tt class="docutils literal"><span class="pre">2</span></tt></p>
<p>Maximum number of times to retry, in addition to the first download.</p>
</div>
<div class="section" id="retry-http-codes">
<span id="std:setting-RETRY_HTTP_CODES"></span><h7>RETRY_HTTP_CODES<a class="headerlink" href="#retry-http-codes" title="Permalink to this headline">¶</a></h7>
<p>Default: <tt class="docutils literal"><span class="pre">[500,</span> <span class="pre">502,</span> <span class="pre">503,</span> <span class="pre">504,</span> <span class="pre">400,</span> <span class="pre">408]</span></tt></p>
<p>Which HTTP response codes to retry. Other errors (DNS lookup issues,
connections lost, etc) are always retried.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.robotstxt">
<span id="robotstxtmiddleware"></span><span id="topics-dlmw-robots"></span><h5>RobotsTxtMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.robotstxt" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.robotstxt.</tt><tt class="descname">RobotsTxtMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware filters out requests forbidden by the robots.txt exclusion
standard.</p>
<p>To make sure Scrapy respects robots.txt make sure the middleware is enabled
and the <a class="reference internal" href="index.html#std:setting-ROBOTSTXT_OBEY"><tt class="xref std std-setting docutils literal"><span class="pre">ROBOTSTXT_OBEY</span></tt></a> setting is enabled.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Keep in mind that, if you crawl using multiple concurrent
requests per domain, Scrapy could still  download some forbidden pages
if they were requested before the robots.txt file was downloaded. This
is a known limitation of the current robots.txt middleware and will
be fixed in the future.</p>
</div>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.stats">
<span id="downloaderstats"></span><h5>DownloaderStats<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.stats" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.stats.DownloaderStats">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.stats.</tt><tt class="descname">DownloaderStats</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.stats.DownloaderStats" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that stores stats of all requests, responses and exceptions that
pass through it.</p>
<p>To use this middleware you must enable the <a class="reference internal" href="index.html#std:setting-DOWNLOADER_STATS"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_STATS</span></tt></a>
setting.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.useragent">
<span id="useragentmiddleware"></span><h5>UserAgentMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.useragent" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.useragent.</tt><tt class="descname">UserAgentMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that allows spiders to override the default user agent.</p>
<p>In order for a spider to override the default user agent, its <cite>user_agent</cite>
attribute must be set.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.ajaxcrawl">
<span id="ajaxcrawlmiddleware"></span><span id="ajaxcrawl-middleware"></span><h5>AjaxCrawlMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.ajaxcrawl" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.ajaxcrawl.AjaxCrawlMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.ajaxcrawl.</tt><tt class="descname">AjaxCrawlMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.ajaxcrawl.AjaxCrawlMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that finds &#8216;AJAX crawlable&#8217; page variants based
on meta-fragment html tag. See
<a class="reference external" href="https://developers.google.com/webmasters/ajax-crawling/docs/getting-started">https://developers.google.com/webmasters/ajax-crawling/docs/getting-started</a>
for more info.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Scrapy finds &#8216;AJAX crawlable&#8217; pages for URLs like
<tt class="docutils literal"><span class="pre">'http://example.com/!#foo=bar'</span></tt> even without this middleware.
AjaxCrawlMiddleware is necessary when URL doesn&#8217;t contain <tt class="docutils literal"><span class="pre">'!#'</span></tt>.
This is often a case for &#8216;index&#8217; or &#8216;main&#8217; website pages.</p>
</div>
</dd></dl>

<div class="section" id="ajaxcrawlmiddleware-settings">
<h6>AjaxCrawlMiddleware Settings<a class="headerlink" href="#ajaxcrawlmiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="ajaxcrawl-enabled">
<span id="std:setting-AJAXCRAWL_ENABLED"></span><h7>AJAXCRAWL_ENABLED<a class="headerlink" href="#ajaxcrawl-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.21.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Whether the AjaxCrawlMiddleware will be enabled. You may want to
enable it for <a class="reference internal" href="index.html#topics-broad-crawls"><em>broad crawls</em></a>.</p>
</div>
</div>
</div>
</div>
</div>
<span id="document-topics/spider-middleware"></span><div class="section" id="spider-middleware">
<span id="topics-spider-middleware"></span><h3>Spider Middleware<a class="headerlink" href="#spider-middleware" title="Permalink to this headline">¶</a></h3>
<p>The spider middleware is a framework of hooks into Scrapy&#8217;s spider processing
mechanism where you can plug custom functionality to process the responses that
are sent to <a class="reference internal" href="index.html#topics-spiders"><em>Spiders</em></a> for processing and to process the requests
and items that are generated from spiders.</p>
<div class="section" id="activating-a-spider-middleware">
<span id="topics-spider-middleware-setting"></span><h4>Activating a spider middleware<a class="headerlink" href="#activating-a-spider-middleware" title="Permalink to this headline">¶</a></h4>
<p>To activate a spider middleware component, add it to the
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES</span></tt></a> setting, which is a dict whose keys are the
middleware class path and their values are the middleware orders.</p>
<p>Here&#8217;s an example:</p>
<div class="highlight-none"><div class="highlight"><pre>SPIDER_MIDDLEWARES = {
    &#39;myproject.middlewares.CustomSpiderMiddleware&#39;: 543,
}
</pre></div>
</div>
<p>The <a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES</span></tt></a> setting is merged with the
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></tt></a> setting defined in Scrapy (and not meant to
be overridden) and then sorted by order to get the final sorted list of enabled
middlewares: the first middleware is the one closer to the engine and the last
is the one closer to the spider.</p>
<p>To decide which order to assign to your middleware see the
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></tt></a> setting and pick a value according to where
you want to insert the middleware. The order does matter because each
middleware performs a different action and your middleware could depend on some
previous (or subsequent) middleware being applied.</p>
<p>If you want to disable a builtin middleware (the ones defined in
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></tt></a>, and enabled by default) you must define it
in your project <a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES</span></tt></a> setting and assign <cite>None</cite> as its
value.  For example, if you want to disable the off-site middleware:</p>
<div class="highlight-none"><div class="highlight"><pre>SPIDER_MIDDLEWARES = {
    &#39;myproject.middlewares.CustomSpiderMiddleware&#39;: 543,
    &#39;scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware&#39;: None,
}
</pre></div>
</div>
<p>Finally, keep in mind that some middlewares may need to be enabled through a
particular setting. See each middleware documentation for more info.</p>
</div>
<div class="section" id="writing-your-own-spider-middleware">
<h4>Writing your own spider middleware<a class="headerlink" href="#writing-your-own-spider-middleware" title="Permalink to this headline">¶</a></h4>
<p>Writing your own spider middleware is easy. Each middleware component is a
single Python class that defines one or more of the following methods:</p>
<span class="target" id="module-scrapy.contrib.spidermiddleware"></span><dl class="class">
<dt id="scrapy.contrib.spidermiddleware.SpiderMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spidermiddleware.</tt><tt class="descname">SpiderMiddleware</tt><a class="headerlink" href="#scrapy.contrib.spidermiddleware.SpiderMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_input">
<tt class="descname">process_spider_input</tt><big>(</big><em>response</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_input" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for each response that goes through the spider
middleware and into the spider, for processing.</p>
<p><a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_input" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_input"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_input()</span></tt></a> should return <tt class="docutils literal"><span class="pre">None</span></tt> or raise an
exception.</p>
<p>If it returns <tt class="docutils literal"><span class="pre">None</span></tt>, Scrapy will continue processing this response,
executing all other middlewares until, finally, the response is handed
to the spider for processing.</p>
<p>If it raises an exception, Scrapy won&#8217;t bother calling any other spider
middleware <a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_input" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_input"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_input()</span></tt></a> and will call the request
errback.  The output of the errback is chained back in the other
direction for <a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></tt></a> to process it, or
<a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_exception()</span></tt></a> if it raised an exception.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object) &#8211; the response being processed</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider for which this response is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output">
<tt class="descname">process_spider_output</tt><big>(</big><em>response</em>, <em>result</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called with the results returned from the Spider, after
it has processed the response.</p>
<p><a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></tt></a> must return an iterable of
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> or <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> objects.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (class:<cite>~scrapy.http.Response</cite> object) &#8211; the response which generated this output from the
spider</li>
<li><strong>result</strong> (an iterable of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> or
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> objects) &#8211; the result returned by the spider</li>
<li><strong>spider</strong> (<tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt> object) &#8211; the spider whose result is being processed</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception">
<tt class="descname">process_spider_exception</tt><big>(</big><em>response</em>, <em>exception</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called when when a spider or <a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_input" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_input"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_input()</span></tt></a>
method (from other spider middleware) raises an exception.</p>
<p><a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_exception()</span></tt></a> should return either <tt class="docutils literal"><span class="pre">None</span></tt> or an
iterable of <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> or
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> objects.</p>
<p>If it returns <tt class="docutils literal"><span class="pre">None</span></tt>, Scrapy will continue processing this exception,
executing any other <a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_exception()</span></tt></a> in the following
middleware components, until no middleware components are left and the
exception reaches the engine (where it&#8217;s logged and discarded).</p>
<p>If it returns an iterable the <a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></tt></a> pipeline
kicks in, and no other <a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_exception()</span></tt></a> will be called.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object) &#8211; the response being processed when the exception was
raised</li>
<li><strong>exception</strong> (<a class="reference external" href="http://docs.python.org/library/exceptions.html#exceptions.Exception">Exception</a> object) &#8211; the exception raised</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider which raised the exception</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_start_requests">
<tt class="descname">process_start_requests</tt><big>(</big><em>start_requests</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_start_requests" title="Permalink to this definition">¶</a></dt>
<dd><div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<p>This method is called with the start requests of the spider, and works
similarly to the <a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_spider_output"><tt class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></tt></a> method, except that it
doesn&#8217;t have a response associated and must return only requests (not
items).</p>
<p>It receives an iterable (in the <tt class="docutils literal"><span class="pre">start_requests</span></tt> parameter) and must
return another iterable of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> objects.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When implementing this method in your spider middleware, you
should always return an iterable (that follows the input one) and
not consume all <tt class="docutils literal"><span class="pre">start_requests</span></tt> iterator because it can be very
large (or even unbounded) and cause a memory overflow. The Scrapy
engine is designed to pull start requests while it has capacity to
process them, so the start requests iterator can be effectively
endless where there is some other condition for stopping the spider
(like a time limit or item/page count).</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>start_requests</strong> (an iterable of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a>) &#8211; the start requests</li>
<li><strong>spider</strong> (<tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt> object) &#8211; the spider to whom the start requests belong</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="built-in-spider-middleware-reference">
<span id="topics-spider-middleware-ref"></span><h4>Built-in spider middleware reference<a class="headerlink" href="#built-in-spider-middleware-reference" title="Permalink to this headline">¶</a></h4>
<p>This page describes all spider middleware components that come with Scrapy. For
information on how to use them and how to write your own spider middleware, see
the <a class="reference internal" href="index.html#topics-spider-middleware"><em>spider middleware usage guide</em></a>.</p>
<p>For a list of the components enabled by default (and their orders) see the
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></tt></a> setting.</p>
<div class="section" id="module-scrapy.contrib.spidermiddleware.depth">
<span id="depthmiddleware"></span><h5>DepthMiddleware<a class="headerlink" href="#module-scrapy.contrib.spidermiddleware.depth" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spidermiddleware.depth.DepthMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spidermiddleware.depth.</tt><tt class="descname">DepthMiddleware</tt><a class="headerlink" href="#scrapy.contrib.spidermiddleware.depth.DepthMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>DepthMiddleware is a scrape middleware used for tracking the depth of each
Request inside the site being scraped. It can be used to limit the maximum
depth to scrape or things like that.</p>
<p>The <a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.depth.DepthMiddleware" title="scrapy.contrib.spidermiddleware.depth.DepthMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">DepthMiddleware</span></tt></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-DEPTH_LIMIT"><tt class="xref std std-setting docutils literal"><span class="pre">DEPTH_LIMIT</span></tt></a> - The maximum depth that will be allowed to
crawl for any site. If zero, no limit will be imposed.</li>
<li><a class="reference internal" href="index.html#std:setting-DEPTH_STATS"><tt class="xref std std-setting docutils literal"><span class="pre">DEPTH_STATS</span></tt></a> - Whether to collect depth stats.</li>
<li><a class="reference internal" href="index.html#std:setting-DEPTH_PRIORITY"><tt class="xref std std-setting docutils literal"><span class="pre">DEPTH_PRIORITY</span></tt></a> - Whether to prioritize the requests based on
their depth.</li>
</ul>
</div></blockquote>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.spidermiddleware.httperror">
<span id="httperrormiddleware"></span><h5>HttpErrorMiddleware<a class="headerlink" href="#module-scrapy.contrib.spidermiddleware.httperror" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spidermiddleware.httperror.</tt><tt class="descname">HttpErrorMiddleware</tt><a class="headerlink" href="#scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter out unsuccessful (erroneous) HTTP responses so that spiders don&#8217;t
have to deal with them, which (most of the time) imposes an overhead,
consumes more resources, and makes the spider logic more complex.</p>
</dd></dl>

<p>According to the <a class="reference external" href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP standard</a>, successful responses are those whose
status codes are in the 200-300 range.</p>
<p>If you still want to process response codes outside that range, you can
specify which response codes the spider is able to handle using the
<tt class="docutils literal"><span class="pre">handle_httpstatus_list</span></tt> spider attribute or
<a class="reference internal" href="index.html#std:setting-HTTPERROR_ALLOWED_CODES"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPERROR_ALLOWED_CODES</span></tt></a> setting.</p>
<p>For example, if you want your spider to handle 404 responses you can do
this:</p>
<div class="highlight-none"><div class="highlight"><pre>class MySpider(CrawlSpider):
    handle_httpstatus_list = [404]
</pre></div>
</div>
<p id="std:reqmeta-handle_httpstatus_list">The <tt class="docutils literal"><span class="pre">handle_httpstatus_list</span></tt> key of <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> can also be used to specify which response codes to
allow on a per-request basis.</p>
<p>Keep in mind, however, that it&#8217;s usually a bad idea to handle non-200
responses, unless you really know what you&#8217;re doing.</p>
<p>For more information see: <a class="reference external" href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP Status Code Definitions</a>.</p>
<div class="section" id="httperrormiddleware-settings">
<h6>HttpErrorMiddleware settings<a class="headerlink" href="#httperrormiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="httperror-allowed-codes">
<span id="std:setting-HTTPERROR_ALLOWED_CODES"></span><h7>HTTPERROR_ALLOWED_CODES<a class="headerlink" href="#httperror-allowed-codes" title="Permalink to this headline">¶</a></h7>
<p>Default: <tt class="docutils literal"><span class="pre">[]</span></tt></p>
<p>Pass all responses with non-200 status codes contained in this list.</p>
</div>
<div class="section" id="httperror-allow-all">
<span id="std:setting-HTTPERROR_ALLOW_ALL"></span><h7>HTTPERROR_ALLOW_ALL<a class="headerlink" href="#httperror-allow-all" title="Permalink to this headline">¶</a></h7>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Pass all responses, regardless of its status code.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.spidermiddleware.offsite">
<span id="offsitemiddleware"></span><h5>OffsiteMiddleware<a class="headerlink" href="#module-scrapy.contrib.spidermiddleware.offsite" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spidermiddleware.offsite.</tt><tt class="descname">OffsiteMiddleware</tt><a class="headerlink" href="#scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Filters out Requests for URLs outside the domains covered by the spider.</p>
<p>This middleware filters out every request whose host names aren&#8217;t in the
spider&#8217;s <a class="reference internal" href="index.html#scrapy.spider.Spider.allowed_domains" title="scrapy.spider.Spider.allowed_domains"><tt class="xref py py-attr docutils literal"><span class="pre">allowed_domains</span></tt></a> attribute.</p>
<p>When your spider returns a request for a domain not belonging to those
covered by the spider, this middleware will log a debug message similar to
this one:</p>
<div class="highlight-none"><div class="highlight"><pre>DEBUG: Filtered offsite request to &#39;www.othersite.com&#39;: &lt;GET http://www.othersite.com/some/page.html&gt;
</pre></div>
</div>
<p>To avoid filling the log with too much noise, it will only print one of
these messages for each new domain filtered. So, for example, if another
request for <tt class="docutils literal"><span class="pre">www.othersite.com</span></tt> is filtered, no log message will be
printed. But if a request for <tt class="docutils literal"><span class="pre">someothersite.com</span></tt> is filtered, a message
will be printed (but only for the first request filtered).</p>
<p>If the spider doesn&#8217;t define an
<a class="reference internal" href="index.html#scrapy.spider.Spider.allowed_domains" title="scrapy.spider.Spider.allowed_domains"><tt class="xref py py-attr docutils literal"><span class="pre">allowed_domains</span></tt></a> attribute, or the
attribute is empty, the offsite middleware will allow all requests.</p>
<p>If the request has the <tt class="xref py py-attr docutils literal"><span class="pre">dont_filter</span></tt> attribute
set, the offsite middleware will allow the request even if its domain is not
listed in allowed domains.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.spidermiddleware.referer">
<span id="referermiddleware"></span><h5>RefererMiddleware<a class="headerlink" href="#module-scrapy.contrib.spidermiddleware.referer" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spidermiddleware.referer.RefererMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spidermiddleware.referer.</tt><tt class="descname">RefererMiddleware</tt><a class="headerlink" href="#scrapy.contrib.spidermiddleware.referer.RefererMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Populates Request <tt class="docutils literal"><span class="pre">Referer</span></tt> header, based on the URL of the Response which
generated it.</p>
</dd></dl>

<div class="section" id="referermiddleware-settings">
<h6>RefererMiddleware settings<a class="headerlink" href="#referermiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="referer-enabled">
<span id="std:setting-REFERER_ENABLED"></span><h7>REFERER_ENABLED<a class="headerlink" href="#referer-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Whether to enable referer middleware.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.spidermiddleware.urllength">
<span id="urllengthmiddleware"></span><h5>UrlLengthMiddleware<a class="headerlink" href="#module-scrapy.contrib.spidermiddleware.urllength" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.spidermiddleware.urllength.</tt><tt class="descname">UrlLengthMiddleware</tt><a class="headerlink" href="#scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Filters out requests with URLs longer than URLLENGTH_LIMIT</p>
<p>The <a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware" title="scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">UrlLengthMiddleware</span></tt></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-URLLENGTH_LIMIT"><tt class="xref std std-setting docutils literal"><span class="pre">URLLENGTH_LIMIT</span></tt></a> - The maximum URL length to allow for crawled URLs.</li>
</ul>
</div></blockquote>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/extensions"></span><div class="section" id="extensions">
<span id="topics-extensions"></span><h3>Extensions<a class="headerlink" href="#extensions" title="Permalink to this headline">¶</a></h3>
<p>The extensions framework provides a mechanism for inserting your own
custom functionality into Scrapy.</p>
<p>Extensions are just regular classes that are instantiated at Scrapy startup,
when extensions are initialized.</p>
<div class="section" id="extension-settings">
<h4>Extension settings<a class="headerlink" href="#extension-settings" title="Permalink to this headline">¶</a></h4>
<p>Extensions use the <a class="reference internal" href="index.html#topics-settings"><em>Scrapy settings</em></a> to manage their
settings, just like any other Scrapy code.</p>
<p>It is customary for extensions to prefix their settings with their own name, to
avoid collision with existing (and future) extensions. For example, an
hypothetic extension to handle <a class="reference external" href="http://en.wikipedia.org/wiki/Sitemaps">Google Sitemaps</a> would use settings like
<cite>GOOGLESITEMAP_ENABLED</cite>, <cite>GOOGLESITEMAP_DEPTH</cite>, and so on.</p>
</div>
<div class="section" id="loading-activating-extensions">
<h4>Loading &amp; activating extensions<a class="headerlink" href="#loading-activating-extensions" title="Permalink to this headline">¶</a></h4>
<p>Extensions are loaded and activated at startup by instantiating a single
instance of the extension class. Therefore, all the extension initialization
code must be performed in the class constructor (<tt class="docutils literal"><span class="pre">__init__</span></tt> method).</p>
<p>To make an extension available, add it to the <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><tt class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></tt></a> setting in
your Scrapy settings. In <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><tt class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></tt></a>, each extension is represented
by a string: the full Python path to the extension&#8217;s class name. For example:</p>
<div class="highlight-none"><div class="highlight"><pre>EXTENSIONS = {
    &#39;scrapy.contrib.corestats.CoreStats&#39;: 500,
    &#39;scrapy.webservice.WebService&#39;: 500,
    &#39;scrapy.telnet.TelnetConsole&#39;: 500,
}
</pre></div>
</div>
<p>As you can see, the <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><tt class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></tt></a> setting is a dict where the keys are
the extension paths, and their values are the orders, which define the
extension <em>loading</em> order. Extensions orders are not as important as middleware
orders though, and they are typically irrelevant, ie. it doesn&#8217;t matter in
which order the extensions are loaded because they don&#8217;t depend on each other
[1].</p>
<p>However, this feature can be exploited if you need to add an extension which
depends on other extensions already loaded.</p>
<p>[1] This is is why the <a class="reference internal" href="index.html#std:setting-EXTENSIONS_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS_BASE</span></tt></a> setting in Scrapy (which
contains all built-in extensions enabled by default) defines all the extensions
with the same order (<tt class="docutils literal"><span class="pre">500</span></tt>).</p>
</div>
<div class="section" id="available-enabled-and-disabled-extensions">
<h4>Available, enabled and disabled extensions<a class="headerlink" href="#available-enabled-and-disabled-extensions" title="Permalink to this headline">¶</a></h4>
<p>Not all available extensions will be enabled. Some of them usually depend on a
particular setting. For example, the HTTP Cache extension is available by default
but disabled unless the <a class="reference internal" href="index.html#std:setting-HTTPCACHE_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_ENABLED</span></tt></a> setting is set.</p>
</div>
<div class="section" id="disabling-an-extension">
<h4>Disabling an extension<a class="headerlink" href="#disabling-an-extension" title="Permalink to this headline">¶</a></h4>
<p>In order to disable an extension that comes enabled by default (ie. those
included in the <a class="reference internal" href="index.html#std:setting-EXTENSIONS_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS_BASE</span></tt></a> setting) you must set its order to
<tt class="docutils literal"><span class="pre">None</span></tt>. For example:</p>
<div class="highlight-none"><div class="highlight"><pre>EXTENSIONS = {
    &#39;scrapy.contrib.corestats.CoreStats&#39;: None,
}
</pre></div>
</div>
</div>
<div class="section" id="writing-your-own-extension">
<h4>Writing your own extension<a class="headerlink" href="#writing-your-own-extension" title="Permalink to this headline">¶</a></h4>
<p>Writing your own extension is easy. Each extension is a single Python class
which doesn&#8217;t need to implement any particular method.</p>
<p>The main entry point for a Scrapy extension (this also includes middlewares and
pipelines) is the <tt class="docutils literal"><span class="pre">from_crawler</span></tt> class method which receives a
<tt class="docutils literal"><span class="pre">Crawler</span></tt> instance which is the main object controlling the Scrapy crawler.
Through that object you can access settings, signals, stats, and also control
the crawler behaviour, if your extension needs to such thing.</p>
<p>Typically, extensions connect to <a class="reference internal" href="index.html#topics-signals"><em>signals</em></a> and perform
tasks triggered by them.</p>
<p>Finally, if the <tt class="docutils literal"><span class="pre">from_crawler</span></tt> method raises the
<a class="reference internal" href="index.html#scrapy.exceptions.NotConfigured" title="scrapy.exceptions.NotConfigured"><tt class="xref py py-exc docutils literal"><span class="pre">NotConfigured</span></tt></a> exception, the extension will be
disabled. Otherwise, the extension will be enabled.</p>
<div class="section" id="sample-extension">
<h5>Sample extension<a class="headerlink" href="#sample-extension" title="Permalink to this headline">¶</a></h5>
<p>Here we will implement a simple extension to illustrate the concepts described
in the previous section. This extension will log a message every time:</p>
<ul class="simple">
<li>a spider is opened</li>
<li>a spider is closed</li>
<li>a specific number of items are scraped</li>
</ul>
<p>The extension will be enabled through the <tt class="docutils literal"><span class="pre">MYEXT_ENABLED</span></tt> setting and the
number of items will be specified through the <tt class="docutils literal"><span class="pre">MYEXT_ITEMCOUNT</span></tt> setting.</p>
<p>Here is the code of such extension:</p>
<div class="highlight-none"><div class="highlight"><pre>from scrapy import signals
from scrapy.exceptions import NotConfigured

class SpiderOpenCloseLogging(object):

    def __init__(self, item_count):
        self.item_count = item_count
        self.items_scraped = 0

    @classmethod
    def from_crawler(cls, crawler):
        # first check if the extension should be enabled and raise
        # NotConfigured otherwise
        if not crawler.settings.getbool(&#39;MYEXT_ENABLED&#39;):
            raise NotConfigured

        # get the number of items from settings
        item_count = crawler.settings.getint(&#39;MYEXT_ITEMCOUNT&#39;, 1000)

        # instantiate the extension object
        ext = cls(item_count)

        # connect the extension object to signals
        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)
        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)

        # return the extension object
        return ext

    def spider_opened(self, spider):
        spider.log(&quot;opened spider %s&quot; % spider.name)

    def spider_closed(self, spider):
        spider.log(&quot;closed spider %s&quot; % spider.name)

    def item_scraped(self, item, spider):
        self.items_scraped += 1
        if self.items_scraped == self.item_count:
            spider.log(&quot;scraped %d items, resetting counter&quot; % self.items_scraped)
            self.item_count = 0
</pre></div>
</div>
</div>
</div>
<div class="section" id="built-in-extensions-reference">
<span id="topics-extensions-ref"></span><h4>Built-in extensions reference<a class="headerlink" href="#built-in-extensions-reference" title="Permalink to this headline">¶</a></h4>
<div class="section" id="general-purpose-extensions">
<h5>General purpose extensions<a class="headerlink" href="#general-purpose-extensions" title="Permalink to this headline">¶</a></h5>
<div class="section" id="module-scrapy.contrib.logstats">
<span id="log-stats-extension"></span><h6>Log Stats extension<a class="headerlink" href="#module-scrapy.contrib.logstats" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.logstats.LogStats">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.logstats.</tt><tt class="descname">LogStats</tt><a class="headerlink" href="#scrapy.contrib.logstats.LogStats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Log basic stats like crawled pages and scraped items.</p>
</div>
<div class="section" id="module-scrapy.contrib.corestats">
<span id="core-stats-extension"></span><h6>Core Stats extension<a class="headerlink" href="#module-scrapy.contrib.corestats" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.corestats.CoreStats">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.corestats.</tt><tt class="descname">CoreStats</tt><a class="headerlink" href="#scrapy.contrib.corestats.CoreStats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Enable the collection of core statistics, provided the stats collection is
enabled (see <a class="reference internal" href="index.html#topics-stats"><em>Stats Collection</em></a>).</p>
</div>
<div class="section" id="module-scrapy.webservice">
<span id="web-service-extension"></span><span id="topics-extensions-ref-webservice"></span><h6>Web service extension<a class="headerlink" href="#module-scrapy.webservice" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.webservice.scrapy.webservice.WebService">
<em class="property">class </em><tt class="descclassname">scrapy.webservice.</tt><tt class="descname">WebService</tt><a class="headerlink" href="#scrapy.webservice.scrapy.webservice.WebService" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>See <cite>topics-webservice</cite>.</p>
</div>
<div class="section" id="module-scrapy.telnet">
<span id="telnet-console-extension"></span><span id="topics-extensions-ref-telnetconsole"></span><h6>Telnet console extension<a class="headerlink" href="#module-scrapy.telnet" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.telnet.scrapy.telnet.TelnetConsole">
<em class="property">class </em><tt class="descclassname">scrapy.telnet.</tt><tt class="descname">TelnetConsole</tt><a class="headerlink" href="#scrapy.telnet.scrapy.telnet.TelnetConsole" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Provides a telnet console for getting into a Python interpreter inside the
currently running Scrapy process, which can be very useful for debugging.</p>
<p>The telnet console must be enabled by the <a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">TELNETCONSOLE_ENABLED</span></tt></a>
setting, and the server will listen in the port specified in
<a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_PORT"><tt class="xref std std-setting docutils literal"><span class="pre">TELNETCONSOLE_PORT</span></tt></a>.</p>
</div>
<div class="section" id="module-scrapy.contrib.memusage">
<span id="memory-usage-extension"></span><span id="topics-extensions-ref-memusage"></span><h6>Memory usage extension<a class="headerlink" href="#module-scrapy.contrib.memusage" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.memusage.scrapy.contrib.memusage.MemoryUsage">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.memusage.</tt><tt class="descname">MemoryUsage</tt><a class="headerlink" href="#scrapy.contrib.memusage.scrapy.contrib.memusage.MemoryUsage" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This extension does not work in Windows.</p>
</div>
<p>Monitors the memory used by the Scrapy process that runs the spider and:</p>
<p>1, sends a notification e-mail when it exceeds a certain value
2. closes the spider when it exceeds a certain value</p>
<p>The notification e-mails can be triggered when a certain warning value is
reached (<a class="reference internal" href="index.html#std:setting-MEMUSAGE_WARNING_MB"><tt class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_WARNING_MB</span></tt></a>) and when the maximum value is reached
(<a class="reference internal" href="index.html#std:setting-MEMUSAGE_LIMIT_MB"><tt class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_LIMIT_MB</span></tt></a>) which will also cause the spider to be closed
and the Scrapy process to be terminated.</p>
<p>This extension is enabled by the <a class="reference internal" href="index.html#std:setting-MEMUSAGE_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_ENABLED</span></tt></a> setting and
can be configured with the following settings:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_LIMIT_MB"><tt class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_LIMIT_MB</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_WARNING_MB"><tt class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_WARNING_MB</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_NOTIFY_MAIL"><tt class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_NOTIFY_MAIL</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_REPORT"><tt class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_REPORT</span></tt></a></li>
</ul>
</div>
<div class="section" id="module-scrapy.contrib.memdebug">
<span id="memory-debugger-extension"></span><h6>Memory debugger extension<a class="headerlink" href="#module-scrapy.contrib.memdebug" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.memdebug.scrapy.contrib.memdebug.MemoryDebugger">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.memdebug.</tt><tt class="descname">MemoryDebugger</tt><a class="headerlink" href="#scrapy.contrib.memdebug.scrapy.contrib.memdebug.MemoryDebugger" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>An extension for debugging memory usage. It collects information about:</p>
<ul class="simple">
<li>objects uncollected by the Python garbage collector</li>
<li>objects left alive that shouldn&#8217;t. For more info, see <a class="reference internal" href="index.html#topics-leaks-trackrefs"><em>Debugging memory leaks with trackref</em></a></li>
</ul>
<p>To enable this extension, turn on the <a class="reference internal" href="index.html#std:setting-MEMDEBUG_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">MEMDEBUG_ENABLED</span></tt></a> setting. The
info will be stored in the stats.</p>
</div>
<div class="section" id="module-scrapy.contrib.closespider">
<span id="close-spider-extension"></span><h6>Close spider extension<a class="headerlink" href="#module-scrapy.contrib.closespider" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.closespider.scrapy.contrib.closespider.CloseSpider">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.closespider.</tt><tt class="descname">CloseSpider</tt><a class="headerlink" href="#scrapy.contrib.closespider.scrapy.contrib.closespider.CloseSpider" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Closes a spider automatically when some conditions are met, using a specific
closing reason for each condition.</p>
<p>The conditions for closing a spider can be configured through the following
settings:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-CLOSESPIDER_TIMEOUT"><tt class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_TIMEOUT</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-CLOSESPIDER_ITEMCOUNT"><tt class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_ITEMCOUNT</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-CLOSESPIDER_PAGECOUNT"><tt class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_PAGECOUNT</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:setting-CLOSESPIDER_ERRORCOUNT"><tt class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_ERRORCOUNT</span></tt></a></li>
</ul>
<div class="section" id="closespider-timeout">
<span id="std:setting-CLOSESPIDER_TIMEOUT"></span><h7>CLOSESPIDER_TIMEOUT<a class="headerlink" href="#closespider-timeout" title="Permalink to this headline">¶</a></h7>
<p>Default: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>An integer which specifies a number of seconds. If the spider remains open for
more than that number of second, it will be automatically closed with the
reason <tt class="docutils literal"><span class="pre">closespider_timeout</span></tt>. If zero (or non set), spiders won&#8217;t be closed by
timeout.</p>
</div>
<div class="section" id="closespider-itemcount">
<span id="std:setting-CLOSESPIDER_ITEMCOUNT"></span><h7>CLOSESPIDER_ITEMCOUNT<a class="headerlink" href="#closespider-itemcount" title="Permalink to this headline">¶</a></h7>
<p>Default: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>An integer which specifies a number of items. If the spider scrapes more than
that amount if items and those items are passed by the item pipeline, the
spider will be closed with the reason <tt class="docutils literal"><span class="pre">closespider_itemcount</span></tt>. If zero (or
non set), spiders won&#8217;t be closed by number of passed items.</p>
</div>
<div class="section" id="closespider-pagecount">
<span id="std:setting-CLOSESPIDER_PAGECOUNT"></span><h7>CLOSESPIDER_PAGECOUNT<a class="headerlink" href="#closespider-pagecount" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.11.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>An integer which specifies the maximum number of responses to crawl. If the spider
crawls more than that, the spider will be closed with the reason
<tt class="docutils literal"><span class="pre">closespider_pagecount</span></tt>. If zero (or non set), spiders won&#8217;t be closed by
number of crawled responses.</p>
</div>
<div class="section" id="closespider-errorcount">
<span id="std:setting-CLOSESPIDER_ERRORCOUNT"></span><h7>CLOSESPIDER_ERRORCOUNT<a class="headerlink" href="#closespider-errorcount" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.11.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>An integer which specifies the maximum number of errors to receive before
closing the spider. If the spider generates more than that number of errors,
it will be closed with the reason <tt class="docutils literal"><span class="pre">closespider_errorcount</span></tt>. If zero (or non
set), spiders won&#8217;t be closed by number of errors.</p>
</div>
</div>
<div class="section" id="module-scrapy.contrib.statsmailer">
<span id="statsmailer-extension"></span><h6>StatsMailer extension<a class="headerlink" href="#module-scrapy.contrib.statsmailer" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.statsmailer.scrapy.contrib.statsmailer.StatsMailer">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.statsmailer.</tt><tt class="descname">StatsMailer</tt><a class="headerlink" href="#scrapy.contrib.statsmailer.scrapy.contrib.statsmailer.StatsMailer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This simple extension can be used to send a notification e-mail every time a
domain has finished scraping, including the Scrapy stats collected. The email
will be sent to all recipients specified in the <a class="reference internal" href="index.html#std:setting-STATSMAILER_RCPTS"><tt class="xref std std-setting docutils literal"><span class="pre">STATSMAILER_RCPTS</span></tt></a>
setting.</p>
<span class="target" id="module-scrapy.contrib.debug"></span></div>
</div>
<div class="section" id="debugging-extensions">
<h5>Debugging extensions<a class="headerlink" href="#debugging-extensions" title="Permalink to this headline">¶</a></h5>
<div class="section" id="stack-trace-dump-extension">
<h6>Stack trace dump extension<a class="headerlink" href="#stack-trace-dump-extension" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.debug.scrapy.contrib.debug.StackTraceDump">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.debug.</tt><tt class="descname">StackTraceDump</tt><a class="headerlink" href="#scrapy.contrib.debug.scrapy.contrib.debug.StackTraceDump" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Dumps information about the running process when a <a class="reference external" href="http://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a> or <a class="reference external" href="http://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a>
signal is received. The information dumped is the following:</p>
<ol class="arabic simple">
<li>engine status (using <tt class="docutils literal"><span class="pre">scrapy.utils.engine.get_engine_status()</span></tt>)</li>
<li>live references (see <a class="reference internal" href="index.html#topics-leaks-trackrefs"><em>Debugging memory leaks with trackref</em></a>)</li>
<li>stack trace of all threads</li>
</ol>
<p>After the stack trace and engine status is dumped, the Scrapy process continues
running normally.</p>
<p>This extension only works on POSIX-compliant platforms (ie. not Windows),
because the <a class="reference external" href="http://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a> and <a class="reference external" href="http://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a> signals are not available on Windows.</p>
<p>There are at least two ways to send Scrapy the <a class="reference external" href="http://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a> signal:</p>
<ol class="arabic">
<li><p class="first">By pressing Ctrl-while a Scrapy process is running (Linux only?)</p>
</li>
<li><p class="first">By running this command (assuming <tt class="docutils literal"><span class="pre">&lt;pid&gt;</span></tt> is the process id of the Scrapy
process):</p>
<div class="highlight-none"><div class="highlight"><pre>kill -QUIT &lt;pid&gt;
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="debugger-extension">
<h6>Debugger extension<a class="headerlink" href="#debugger-extension" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.contrib.debug.scrapy.contrib.debug.Debugger">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.debug.</tt><tt class="descname">Debugger</tt><a class="headerlink" href="#scrapy.contrib.debug.scrapy.contrib.debug.Debugger" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Invokes a <a class="reference external" href="http://docs.python.org/library/pdb.html">Python debugger</a> inside a running Scrapy process when a <a class="reference external" href="http://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a>
signal is received. After the debugger is exited, the Scrapy process continues
running normally.</p>
<p>For more info see <cite>Debugging in Python</cite>.</p>
<p>This extension only works on POSIX-compliant platforms (ie. not Windows).</p>
</div>
</div>
</div>
</div>
<span id="document-topics/api"></span><div class="section" id="core-api">
<span id="topics-api"></span><h3>Core API<a class="headerlink" href="#core-api" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<p>This section documents the Scrapy core API, and it&#8217;s intended for developers of
extensions and middlewares.</p>
<div class="section" id="crawler-api">
<span id="topics-api-crawler"></span><h4>Crawler API<a class="headerlink" href="#crawler-api" title="Permalink to this headline">¶</a></h4>
<p>The main entry point to Scrapy API is the <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><tt class="xref py py-class docutils literal"><span class="pre">Crawler</span></tt></a>
object, passed to extensions through the <tt class="docutils literal"><span class="pre">from_crawler</span></tt> class method. This
object provides access to all Scrapy core components, and it&#8217;s the only way for
extensions to access them and hook their functionality into Scrapy.</p>
<span class="target" id="module-scrapy.crawler"></span><p>The Extension Manager is responsible for loading and keeping track of installed
extensions and it&#8217;s configured through the <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><tt class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></tt></a> setting which
contains a dictionary of all available extensions and their order similar to
how you <a class="reference internal" href="index.html#topics-downloader-middleware-setting"><em>configure the downloader middlewares</em></a>.</p>
<dl class="class">
<dt id="scrapy.crawler.Crawler">
<em class="property">class </em><tt class="descclassname">scrapy.crawler.</tt><tt class="descname">Crawler</tt><big>(</big><em>settings</em><big>)</big><a class="headerlink" href="#scrapy.crawler.Crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>The Crawler object must be instantiated with a
<a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><tt class="xref py py-class docutils literal"><span class="pre">scrapy.settings.Settings</span></tt></a> object.</p>
<dl class="attribute">
<dt id="scrapy.crawler.Crawler.settings">
<tt class="descname">settings</tt><a class="headerlink" href="#scrapy.crawler.Crawler.settings" title="Permalink to this definition">¶</a></dt>
<dd><p>The settings manager of this crawler.</p>
<p>This is used by extensions &amp; middlewares to access the Scrapy settings
of this crawler.</p>
<p>For an introduction on Scrapy settings see <a class="reference internal" href="index.html#topics-settings"><em>Settings</em></a>.</p>
<p>For the API see <a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><tt class="xref py py-class docutils literal"><span class="pre">Settings</span></tt></a> class.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.signals">
<tt class="descname">signals</tt><a class="headerlink" href="#scrapy.crawler.Crawler.signals" title="Permalink to this definition">¶</a></dt>
<dd><p>The signals manager of this crawler.</p>
<p>This is used by extensions &amp; middlewares to hook themselves into Scrapy
functionality.</p>
<p>For an introduction on signals see <a class="reference internal" href="index.html#topics-signals"><em>Signals</em></a>.</p>
<p>For the API see <a class="reference internal" href="index.html#scrapy.signalmanager.SignalManager" title="scrapy.signalmanager.SignalManager"><tt class="xref py py-class docutils literal"><span class="pre">SignalManager</span></tt></a> class.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.stats">
<tt class="descname">stats</tt><a class="headerlink" href="#scrapy.crawler.Crawler.stats" title="Permalink to this definition">¶</a></dt>
<dd><p>The stats collector of this crawler.</p>
<p>This is used from extensions &amp; middlewares to record stats of their
behaviour, or access stats collected by other extensions.</p>
<p>For an introduction on stats collection see <a class="reference internal" href="index.html#topics-stats"><em>Stats Collection</em></a>.</p>
<p>For the API see <a class="reference internal" href="index.html#scrapy.statscol.StatsCollector" title="scrapy.statscol.StatsCollector"><tt class="xref py py-class docutils literal"><span class="pre">StatsCollector</span></tt></a> class.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.extensions">
<tt class="descname">extensions</tt><a class="headerlink" href="#scrapy.crawler.Crawler.extensions" title="Permalink to this definition">¶</a></dt>
<dd><p>The extension manager that keeps track of enabled extensions.</p>
<p>Most extensions won&#8217;t need to access this attribute.</p>
<p>For an introduction on extensions and a list of available extensions on
Scrapy see <a class="reference internal" href="index.html#topics-extensions"><em>Extensions</em></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.spiders">
<tt class="descname">spiders</tt><a class="headerlink" href="#scrapy.crawler.Crawler.spiders" title="Permalink to this definition">¶</a></dt>
<dd><p>The spider manager which takes care of loading and instantiating
spiders.</p>
<p>Most extensions won&#8217;t need to access this attribute.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.engine">
<tt class="descname">engine</tt><a class="headerlink" href="#scrapy.crawler.Crawler.engine" title="Permalink to this definition">¶</a></dt>
<dd><p>The execution engine, which coordinates the core crawling logic
between the scheduler, downloader and spiders.</p>
<p>Some extension may want to access the Scrapy engine, to modify inspect
or modify the downloader and scheduler behaviour, although this is an
advanced use and this API is not yet stable.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.Crawler.configure">
<tt class="descname">configure</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.crawler.Crawler.configure" title="Permalink to this definition">¶</a></dt>
<dd><p>Configure the crawler.</p>
<p>This loads extensions, middlewares and spiders, leaving the crawler
ready to be started. It also configures the execution engine.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.Crawler.start">
<tt class="descname">start</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.crawler.Crawler.start" title="Permalink to this definition">¶</a></dt>
<dd><p>Start the crawler. This calls <a class="reference internal" href="index.html#scrapy.crawler.Crawler.configure" title="scrapy.crawler.Crawler.configure"><tt class="xref py py-meth docutils literal"><span class="pre">configure()</span></tt></a> if it hasn&#8217;t been called yet.
Returns a deferred that is fired when the crawl is finished.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-scrapy.settings">
<span id="settings-api"></span><span id="topics-api-settings"></span><h4>Settings API<a class="headerlink" href="#module-scrapy.settings" title="Permalink to this headline">¶</a></h4>
<dl class="attribute">
<dt id="scrapy.settings.SETTINGS_PRIORITIES">
<tt class="descclassname">scrapy.settings.</tt><tt class="descname">SETTINGS_PRIORITIES</tt><a class="headerlink" href="#scrapy.settings.SETTINGS_PRIORITIES" title="Permalink to this definition">¶</a></dt>
<dd><p>Dictionary that sets the key name and priority level of the default
settings priorities used in Scrapy.</p>
<p>Each item defines a settings entry point, giving it a code name for
identification and an integer priority. Greater priorities take more
precedence over lesser ones when setting and retrieving values in the
<a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><tt class="xref py py-class docutils literal"><span class="pre">Settings</span></tt></a> class.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">SETTINGS_PRIORITIES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">&#39;default&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;command&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s">&#39;project&#39;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s">&#39;cmdline&#39;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>For a detailed explanation on each settings sources, see:
<a class="reference internal" href="index.html#topics-settings"><em>Settings</em></a>.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.settings.Settings">
<em class="property">class </em><tt class="descclassname">scrapy.settings.</tt><tt class="descname">Settings</tt><big>(</big><em>values={}</em>, <em>priority='project'</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings" title="Permalink to this definition">¶</a></dt>
<dd><p>This object stores Scrapy settings for the configuration of internal
components, and can be used for any further customization.</p>
<p>After instantiation of this class, the new object will have the global
default settings described on <a class="reference internal" href="index.html#topics-settings-ref"><em>Built-in settings reference</em></a> already
populated.</p>
<p>Additional values can be passed on initialization with the <tt class="docutils literal"><span class="pre">values</span></tt>
argument, and they would take the <tt class="docutils literal"><span class="pre">priority</span></tt> level.  If the latter
argument is a string, the priority name will be looked up in
<a class="reference internal" href="index.html#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><tt class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></tt></a>. Otherwise, a expecific
integer should be provided.</p>
<p>Once the object is created, new settings can be loaded or updated with the
<a class="reference internal" href="index.html#scrapy.settings.Settings.set" title="scrapy.settings.Settings.set"><tt class="xref py py-meth docutils literal"><span class="pre">set()</span></tt></a> method, and can be accessed with the
square bracket notation of dictionaries, or with the
<a class="reference internal" href="index.html#scrapy.settings.Settings.get" title="scrapy.settings.Settings.get"><tt class="xref py py-meth docutils literal"><span class="pre">get()</span></tt></a> method of the instance and its value
conversion variants.  When requesting a stored key, the value with the
highest priority will be retrieved.</p>
<dl class="method">
<dt id="scrapy.settings.Settings.set">
<tt class="descname">set</tt><big>(</big><em>name</em>, <em>value</em>, <em>priority='project'</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings.set" title="Permalink to this definition">¶</a></dt>
<dd><p>Store a key/value attribute with a given priority.</p>
<p>Settings should be populated <em>before</em> configuring the Crawler object
(through the <a class="reference internal" href="index.html#scrapy.crawler.Crawler.configure" title="scrapy.crawler.Crawler.configure"><tt class="xref py py-meth docutils literal"><span class="pre">configure()</span></tt></a> method),
otherwise they won&#8217;t have any effect.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) &#8211; the setting name</li>
<li><strong>value</strong> (<em>any</em>) &#8211; the value to associate with the setting</li>
<li><strong>priority</strong> (<em>string or int</em>) &#8211; the priority of the setting. Should be a key of
<a class="reference internal" href="index.html#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><tt class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></tt></a> or an integer</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.Settings.setdict">
<tt class="descname">setdict</tt><big>(</big><em>values</em>, <em>priority='project'</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings.setdict" title="Permalink to this definition">¶</a></dt>
<dd><p>Store key/value pairs with a given priority.</p>
<p>This is a helper function that calls
<a class="reference internal" href="index.html#scrapy.settings.Settings.set" title="scrapy.settings.Settings.set"><tt class="xref py py-meth docutils literal"><span class="pre">set()</span></tt></a> for every item of <tt class="docutils literal"><span class="pre">values</span></tt>
with the provided <tt class="docutils literal"><span class="pre">priority</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>values</strong> (<em>dict</em>) &#8211; the settings names and values</li>
<li><strong>priority</strong> (<em>string or int</em>) &#8211; the priority of the settings. Should be a key of
<a class="reference internal" href="index.html#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><tt class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></tt></a> or an integer</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.Settings.setmodule">
<tt class="descname">setmodule</tt><big>(</big><em>module</em>, <em>priority='project'</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings.setmodule" title="Permalink to this definition">¶</a></dt>
<dd><p>Store settings from a module with a given priority.</p>
<p>This is a helper function that calls
<a class="reference internal" href="index.html#scrapy.settings.Settings.set" title="scrapy.settings.Settings.set"><tt class="xref py py-meth docutils literal"><span class="pre">set()</span></tt></a> for every globally declared
uppercase variable of <tt class="docutils literal"><span class="pre">module</span></tt> with the provided <tt class="docutils literal"><span class="pre">priority</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>module</strong> (<em>module object or string</em>) &#8211; the module or the path of the module</li>
<li><strong>priority</strong> (<em>string or int</em>) &#8211; the priority of the settings. Should be a key of
<a class="reference internal" href="index.html#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><tt class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></tt></a> or an integer</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.Settings.get">
<tt class="descname">get</tt><big>(</big><em>name</em>, <em>default=None</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings.get" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value without affecting its original type.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) &#8211; the setting name</li>
<li><strong>default</strong> (<em>any</em>) &#8211; the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.Settings.getbool">
<tt class="descname">getbool</tt><big>(</big><em>name</em>, <em>default=False</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings.getbool" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a boolean. For example, both <tt class="docutils literal"><span class="pre">1</span></tt> and <tt class="docutils literal"><span class="pre">'1'</span></tt>, and
<tt class="docutils literal"><span class="pre">True</span></tt> return <tt class="docutils literal"><span class="pre">True</span></tt>, while <tt class="docutils literal"><span class="pre">0</span></tt>, <tt class="docutils literal"><span class="pre">'0'</span></tt>, <tt class="docutils literal"><span class="pre">False</span></tt> and <tt class="docutils literal"><span class="pre">None</span></tt>
return <tt class="docutils literal"><span class="pre">False``</span></tt></p>
<p>For example, settings populated through environment variables set to <tt class="docutils literal"><span class="pre">'0'</span></tt>
will return <tt class="docutils literal"><span class="pre">False</span></tt> when using this method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) &#8211; the setting name</li>
<li><strong>default</strong> (<em>any</em>) &#8211; the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.Settings.getint">
<tt class="descname">getint</tt><big>(</big><em>name</em>, <em>default=0</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings.getint" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as an int</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) &#8211; the setting name</li>
<li><strong>default</strong> (<em>any</em>) &#8211; the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.Settings.getfloat">
<tt class="descname">getfloat</tt><big>(</big><em>name</em>, <em>default=0.0</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings.getfloat" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a float</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) &#8211; the setting name</li>
<li><strong>default</strong> (<em>any</em>) &#8211; the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.Settings.getlist">
<tt class="descname">getlist</tt><big>(</big><em>name</em>, <em>default=None</em><big>)</big><a class="headerlink" href="#scrapy.settings.Settings.getlist" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a list. If the setting original type is a list it
will be returned verbatim. If it&#8217;s a string it will be split by &#8221;,&#8221;.</p>
<p>For example, settings populated through environment variables set to
<tt class="docutils literal"><span class="pre">'one,two'</span></tt> will return a list [&#8216;one&#8217;, &#8216;two&#8217;] when using this method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) &#8211; the setting name</li>
<li><strong>default</strong> (<em>any</em>) &#8211; the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-scrapy.signalmanager">
<span id="signals-api"></span><span id="topics-api-signals"></span><h4>Signals API<a class="headerlink" href="#module-scrapy.signalmanager" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.signalmanager.SignalManager">
<em class="property">class </em><tt class="descclassname">scrapy.signalmanager.</tt><tt class="descname">SignalManager</tt><a class="headerlink" href="#scrapy.signalmanager.SignalManager" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.signalmanager.SignalManager.connect">
<tt class="descname">connect</tt><big>(</big><em>receiver</em>, <em>signal</em><big>)</big><a class="headerlink" href="#scrapy.signalmanager.SignalManager.connect" title="Permalink to this definition">¶</a></dt>
<dd><p>Connect a receiver function to a signal.</p>
<p>The signal can be any object, although Scrapy comes with some
predefined signals that are documented in the <a class="reference internal" href="index.html#topics-signals"><em>Signals</em></a>
section.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>receiver</strong> (<em>callable</em>) &#8211; the function to be connected</li>
<li><strong>signal</strong> (<em>object</em>) &#8211; the signal to connect to</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.send_catch_log">
<tt class="descname">send_catch_log</tt><big>(</big><em>signal</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.signalmanager.SignalManager.send_catch_log" title="Permalink to this definition">¶</a></dt>
<dd><p>Send a signal, catch exceptions and log them.</p>
<p>The keyword arguments are passed to the signal handlers (connected
through the <a class="reference internal" href="index.html#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><tt class="xref py py-meth docutils literal"><span class="pre">connect()</span></tt></a> method).</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.send_catch_log_deferred">
<tt class="descname">send_catch_log_deferred</tt><big>(</big><em>signal</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.signalmanager.SignalManager.send_catch_log_deferred" title="Permalink to this definition">¶</a></dt>
<dd><p>Like <a class="reference internal" href="index.html#scrapy.signalmanager.SignalManager.send_catch_log" title="scrapy.signalmanager.SignalManager.send_catch_log"><tt class="xref py py-meth docutils literal"><span class="pre">send_catch_log()</span></tt></a> but supports returning <a class="reference external" href="http://twistedmatrix.com/documents/current/core/howto/defer.html">deferreds</a> from
signal handlers.</p>
<p>Returns a <a class="reference external" href="http://twistedmatrix.com/documents/current/core/howto/defer.html">deferred</a> that gets fired once all signal handlers
deferreds were fired. Send a signal, catch exceptions and log them.</p>
<p>The keyword arguments are passed to the signal handlers (connected
through the <a class="reference internal" href="index.html#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><tt class="xref py py-meth docutils literal"><span class="pre">connect()</span></tt></a> method).</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.disconnect">
<tt class="descname">disconnect</tt><big>(</big><em>receiver</em>, <em>signal</em><big>)</big><a class="headerlink" href="#scrapy.signalmanager.SignalManager.disconnect" title="Permalink to this definition">¶</a></dt>
<dd><p>Disconnect a receiver function from a signal. This has the opposite
effect of the <a class="reference internal" href="index.html#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><tt class="xref py py-meth docutils literal"><span class="pre">connect()</span></tt></a> method, and the arguments are the same.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.disconnect_all">
<tt class="descname">disconnect_all</tt><big>(</big><em>signal</em><big>)</big><a class="headerlink" href="#scrapy.signalmanager.SignalManager.disconnect_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Disconnect all receivers from the given signal.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>signal</strong> (<em>object</em>) &#8211; the signal to disconnect from</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="stats-collector-api">
<span id="topics-api-stats"></span><h4>Stats Collector API<a class="headerlink" href="#stats-collector-api" title="Permalink to this headline">¶</a></h4>
<p>There are several Stats Collectors available under the
<a class="reference internal" href="index.html#module-scrapy.statscol" title="scrapy.statscol: Stats Collectors"><tt class="xref py py-mod docutils literal"><span class="pre">scrapy.statscol</span></tt></a> module and they all implement the Stats
Collector API defined by the <a class="reference internal" href="index.html#scrapy.statscol.StatsCollector" title="scrapy.statscol.StatsCollector"><tt class="xref py py-class docutils literal"><span class="pre">StatsCollector</span></tt></a>
class (which they all inherit from).</p>
<span class="target" id="module-scrapy.statscol"></span><dl class="class">
<dt id="scrapy.statscol.StatsCollector">
<em class="property">class </em><tt class="descclassname">scrapy.statscol.</tt><tt class="descname">StatsCollector</tt><a class="headerlink" href="#scrapy.statscol.StatsCollector" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.statscol.StatsCollector.get_value">
<tt class="descname">get_value</tt><big>(</big><em>key</em>, <em>default=None</em><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.get_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the value for the given stats key or default if it doesn&#8217;t exist.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscol.StatsCollector.get_stats">
<tt class="descname">get_stats</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.get_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Get all stats from the currently running spider as a dict.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscol.StatsCollector.set_value">
<tt class="descname">set_value</tt><big>(</big><em>key</em>, <em>value</em><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.set_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the given value for the given stats key.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscol.StatsCollector.set_stats">
<tt class="descname">set_stats</tt><big>(</big><em>stats</em><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.set_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Override the current stats with the dict passed in <tt class="docutils literal"><span class="pre">stats</span></tt> argument.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscol.StatsCollector.inc_value">
<tt class="descname">inc_value</tt><big>(</big><em>key</em>, <em>count=1</em>, <em>start=0</em><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.inc_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Increment the value of the given stats key, by the given count,
assuming the start value given (when it&#8217;s not set).</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscol.StatsCollector.max_value">
<tt class="descname">max_value</tt><big>(</big><em>key</em>, <em>value</em><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.max_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the given value for the given key only if current value for the
same key is lower than value. If there is no current value for the
given key, the value is always set.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscol.StatsCollector.min_value">
<tt class="descname">min_value</tt><big>(</big><em>key</em>, <em>value</em><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.min_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the given value for the given key only if current value for the
same key is greater than value. If there is no current value for the
given key, the value is always set.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscol.StatsCollector.clear_stats">
<tt class="descname">clear_stats</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.clear_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Clear all stats.</p>
</dd></dl>

<p>The following methods are not part of the stats collection api but instead
used when implementing custom stats collectors:</p>
<dl class="method">
<dt id="scrapy.statscol.StatsCollector.open_spider">
<tt class="descname">open_spider</tt><big>(</big><em>spider</em><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.open_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>Open the given spider for stats collection.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscol.StatsCollector.close_spider">
<tt class="descname">close_spider</tt><big>(</big><em>spider</em><big>)</big><a class="headerlink" href="#scrapy.statscol.StatsCollector.close_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>Close the given spider. After this is called, no more specific stats
can be accessed or collected.</p>
</dd></dl>

</dd></dl>

</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-topics/architecture"><em>Architecture overview</em></a></dt>
<dd>Understand the Scrapy architecture.</dd>
<dt><a class="reference internal" href="index.html#document-topics/downloader-middleware"><em>Downloader Middleware</em></a></dt>
<dd>Customize how pages get requested and downloaded.</dd>
<dt><a class="reference internal" href="index.html#document-topics/spider-middleware"><em>Spider Middleware</em></a></dt>
<dd>Customize the input and output of your spiders.</dd>
<dt><a class="reference internal" href="index.html#document-topics/extensions"><em>Extensions</em></a></dt>
<dd>Extend Scrapy with your custom functionality</dd>
<dt><a class="reference internal" href="index.html#document-topics/api"><em>Core API</em></a></dt>
<dd>Use it on extensions and middlewares to extend Scrapy functionality</dd>
</dl>
</div>
<div class="section" id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-topics/request-response"></span><div class="section" id="module-scrapy.http">
<span id="requests-and-responses"></span><span id="topics-request-response"></span><h3>Requests and Responses<a class="headerlink" href="#module-scrapy.http" title="Permalink to this headline">¶</a></h3>
<p>Scrapy uses <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> and <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> objects for crawling web
sites.</p>
<p>Typically, <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> objects are generated in the spiders and pass
across the system until they reach the Downloader, which executes the request
and returns a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object which travels back to the spider that
issued the request.</p>
<p>Both <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> and <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> classes have subclasses which add
functionality not required in the base classes. These are described
below in <a class="reference internal" href="index.html#topics-request-response-ref-request-subclasses"><em>Request subclasses</em></a> and
<a class="reference internal" href="index.html#topics-request-response-ref-response-subclasses"><em>Response subclasses</em></a>.</p>
<div class="section" id="request-objects">
<h4>Request objects<a class="headerlink" href="#request-objects" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.http.Request">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">Request</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>callback</em>, <em>method='GET'</em>, <em>headers</em>, <em>body</em>, <em>cookies</em>, <em>meta</em>, <em>encoding='utf-8'</em>, <em>priority=0</em>, <em>dont_filter=False</em>, <em>errback</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.Request" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object represents an HTTP request, which is usually
generated in the Spider and executed by the Downloader, and thus generating
a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>url</strong> (<em>string</em>) &#8211; the URL of this request</li>
<li><strong>callback</strong> (<em>callable</em>) &#8211; the function that will be called with the response of this
request (once its downloaded) as its first parameter. For more information
see <a class="reference internal" href="index.html#topics-request-response-ref-request-callback-arguments"><em>Passing additional data to callback functions</em></a> below.
If a Request doesn&#8217;t specify a callback, the spider&#8217;s
<a class="reference internal" href="index.html#scrapy.spider.Spider.parse" title="scrapy.spider.Spider.parse"><tt class="xref py py-meth docutils literal"><span class="pre">parse()</span></tt></a> method will be used.
Note that if exceptions are raised during processing, errback is called instead.</li>
<li><strong>method</strong> (<em>string</em>) &#8211; the HTTP method of this request. Defaults to <tt class="docutils literal"><span class="pre">'GET'</span></tt>.</li>
<li><strong>meta</strong> (<em>dict</em>) &#8211; the initial values for the <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> attribute. If
given, the dict passed in this parameter will be shallow copied.</li>
<li><strong>body</strong> (<em>str or unicode</em>) &#8211; the request body. If a <tt class="docutils literal"><span class="pre">unicode</span></tt> is passed, then it&#8217;s encoded to
<tt class="docutils literal"><span class="pre">str</span></tt> using the <cite>encoding</cite> passed (which defaults to <tt class="docutils literal"><span class="pre">utf-8</span></tt>). If
<tt class="docutils literal"><span class="pre">body</span></tt> is not given,, an empty string is stored. Regardless of the
type of this argument, the final value stored will be a <tt class="docutils literal"><span class="pre">str</span></tt> (never
<tt class="docutils literal"><span class="pre">unicode</span></tt> or <tt class="docutils literal"><span class="pre">None</span></tt>).</li>
<li><strong>headers</strong> (<em>dict</em>) &#8211; the headers of this request. The dict values can be strings
(for single valued headers) or lists (for multi-valued headers). If
<tt class="docutils literal"><span class="pre">None</span></tt> is passed as value, the HTTP header will not be sent at all.</li>
<li><strong>cookies</strong> (<em>dict or list</em>) &#8211; <p>the request cookies. These can be sent in two forms.</p>
<ol class="arabic">
<li>Using a dict:<div class="highlight-python"><div class="highlight"><pre><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">&quot;http://www.example.com&quot;</span><span class="p">,</span>
                               <span class="n">cookies</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;currency&#39;</span><span class="p">:</span> <span class="s">&#39;USD&#39;</span><span class="p">,</span> <span class="s">&#39;country&#39;</span><span class="p">:</span> <span class="s">&#39;UY&#39;</span><span class="p">})</span>
</pre></div>
</div>
</li>
<li>Using a list of dicts:<div class="highlight-python"><div class="highlight"><pre><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">&quot;http://www.example.com&quot;</span><span class="p">,</span>
                               <span class="n">cookies</span><span class="o">=</span><span class="p">[{</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;currency&#39;</span><span class="p">,</span>
                                        <span class="s">&#39;value&#39;</span><span class="p">:</span> <span class="s">&#39;USD&#39;</span><span class="p">,</span>
                                        <span class="s">&#39;domain&#39;</span><span class="p">:</span> <span class="s">&#39;example.com&#39;</span><span class="p">,</span>
                                        <span class="s">&#39;path&#39;</span><span class="p">:</span> <span class="s">&#39;/currency&#39;</span><span class="p">}])</span>
</pre></div>
</div>
</li>
</ol>
<p>The latter form allows for customizing the <tt class="docutils literal"><span class="pre">domain</span></tt> and <tt class="docutils literal"><span class="pre">path</span></tt>
attributes of the cookie. This is only useful if the cookies are saved
for later requests.</p>
<p>When some site returns cookies (in a response) those are stored in the
cookies for that domain and will be sent again in future requests. That&#8217;s
the typical behaviour of any regular web browser. However, if, for some
reason, you want to avoid merging with existing cookies you can instruct
Scrapy to do so by setting the <tt class="docutils literal"><span class="pre">dont_merge_cookies</span></tt> key in the
<a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a>.</p>
<p>Example of request without merging cookies:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">&quot;http://www.example.com&quot;</span><span class="p">,</span>
                               <span class="n">cookies</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;currency&#39;</span><span class="p">:</span> <span class="s">&#39;USD&#39;</span><span class="p">,</span> <span class="s">&#39;country&#39;</span><span class="p">:</span> <span class="s">&#39;UY&#39;</span><span class="p">},</span>
                               <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;dont_merge_cookies&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">})</span>
</pre></div>
</div>
<p>For more info see <a class="reference internal" href="index.html#cookies-mw"><em>CookiesMiddleware</em></a>.</p>
</li>
<li><strong>encoding</strong> (<em>string</em>) &#8211; the encoding of this request (defaults to <tt class="docutils literal"><span class="pre">'utf-8'</span></tt>).
This encoding will be used to percent-encode the URL and to convert the
body to <tt class="docutils literal"><span class="pre">str</span></tt> (if given as <tt class="docutils literal"><span class="pre">unicode</span></tt>).</li>
<li><strong>priority</strong> (<em>int</em>) &#8211; the priority of this request (defaults to <tt class="docutils literal"><span class="pre">0</span></tt>).
The priority is used by the scheduler to define the order used to process
requests.  Requests with a higher priority value will execute earlier.
Negative values are allowed in order to indicate relatively low-priority.</li>
<li><strong>dont_filter</strong> (<em>boolean</em>) &#8211; indicates that this request should not be filtered by
the scheduler. This is used when you want to perform an identical
request multiple times, to ignore the duplicates filter. Use it with
care, or you will get into crawling loops. Default to <tt class="docutils literal"><span class="pre">False</span></tt>.</li>
<li><strong>errback</strong> (<em>callable</em>) &#8211; a function that will be called if any exception was
raised while processing the request. This includes pages that failed
with 404 HTTP errors and such. It receives a <a class="reference external" href="http://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Twisted Failure</a> instance
as first parameter.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="scrapy.http.Request.url">
<tt class="descname">url</tt><a class="headerlink" href="#scrapy.http.Request.url" title="Permalink to this definition">¶</a></dt>
<dd><p>A string containing the URL of this request. Keep in mind that this
attribute contains the escaped URL, so it can differ from the URL passed in
the constructor.</p>
<p>This attribute is read-only. To change the URL of a Request use
<a class="reference internal" href="index.html#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><tt class="xref py py-meth docutils literal"><span class="pre">replace()</span></tt></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.method">
<tt class="descname">method</tt><a class="headerlink" href="#scrapy.http.Request.method" title="Permalink to this definition">¶</a></dt>
<dd><p>A string representing the HTTP method in the request. This is guaranteed to
be uppercase. Example: <tt class="docutils literal"><span class="pre">&quot;GET&quot;</span></tt>, <tt class="docutils literal"><span class="pre">&quot;POST&quot;</span></tt>, <tt class="docutils literal"><span class="pre">&quot;PUT&quot;</span></tt>, etc</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.headers">
<tt class="descname">headers</tt><a class="headerlink" href="#scrapy.http.Request.headers" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary-like object which contains the request headers.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.body">
<tt class="descname">body</tt><a class="headerlink" href="#scrapy.http.Request.body" title="Permalink to this definition">¶</a></dt>
<dd><p>A str that contains the request body.</p>
<p>This attribute is read-only. To change the body of a Request use
<a class="reference internal" href="index.html#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><tt class="xref py py-meth docutils literal"><span class="pre">replace()</span></tt></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.meta">
<tt class="descname">meta</tt><a class="headerlink" href="#scrapy.http.Request.meta" title="Permalink to this definition">¶</a></dt>
<dd><p>A dict that contains arbitrary metadata for this request. This dict is
empty for new Requests, and is usually  populated by different Scrapy
components (extensions, middlewares, etc). So the data contained in this
dict depends on the extensions you have enabled.</p>
<p>See <a class="reference internal" href="index.html#topics-request-meta"><em>Request.meta special keys</em></a> for a list of special meta keys
recognized by Scrapy.</p>
<p>This dict is <a class="reference external" href="http://docs.python.org/library/copy.html">shallow copied</a> when the request is cloned using the
<tt class="docutils literal"><span class="pre">copy()</span></tt> or <tt class="docutils literal"><span class="pre">replace()</span></tt> methods, and can also be accessed, in your
spider, from the <tt class="docutils literal"><span class="pre">response.meta</span></tt> attribute.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Request.copy">
<tt class="descname">copy</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.http.Request.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new Request which is a copy of this Request. See also:
<a class="reference internal" href="index.html#topics-request-response-ref-request-callback-arguments"><em>Passing additional data to callback functions</em></a>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Request.replace">
<tt class="descname">replace</tt><big>(</big><span class="optional">[</span><em>url</em>, <em>method</em>, <em>headers</em>, <em>body</em>, <em>cookies</em>, <em>meta</em>, <em>encoding</em>, <em>dont_filter</em>, <em>callback</em>, <em>errback</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.Request.replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a Request object with the same members, except for those members
given new values by whichever keyword arguments are specified. The
attribute <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> is copied by default (unless a new value
is given in the <tt class="docutils literal"><span class="pre">meta</span></tt> argument). See also
<a class="reference internal" href="index.html#topics-request-response-ref-request-callback-arguments"><em>Passing additional data to callback functions</em></a>.</p>
</dd></dl>

</dd></dl>

<div class="section" id="passing-additional-data-to-callback-functions">
<span id="topics-request-response-ref-request-callback-arguments"></span><h5>Passing additional data to callback functions<a class="headerlink" href="#passing-additional-data-to-callback-functions" title="Permalink to this headline">¶</a></h5>
<p>The callback of a request is a function that will be called when the response
of that request is downloaded. The callback function will be called with the
downloaded <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object as its first argument.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">parse_page1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s">&quot;http://www.example.com/some_page.html&quot;</span><span class="p">,</span>
                          <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">parse_page2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="c"># this would log http://www.example.com/some_page.html</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s">&quot;Visited </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p>In some cases you may be interested in passing arguments to those callback
functions so you can receive the arguments later, in the second callback. You
can use the <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> attribute for that.</p>
<p>Here&#8217;s an example of how to pass an item using this mechanism, to populate
different fields from different pages:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">parse_page1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">item</span> <span class="o">=</span> <span class="n">MyItem</span><span class="p">()</span>
    <span class="n">item</span><span class="p">[</span><span class="s">&#39;main_url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
    <span class="n">request</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s">&quot;http://www.example.com/some_page.html&quot;</span><span class="p">,</span>
                             <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page2</span><span class="p">)</span>
    <span class="n">request</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s">&#39;item&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span>
    <span class="k">return</span> <span class="n">request</span>

<span class="k">def</span> <span class="nf">parse_page2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">item</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s">&#39;item&#39;</span><span class="p">]</span>
    <span class="n">item</span><span class="p">[</span><span class="s">&#39;other_url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
    <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="request-meta-special-keys">
<span id="topics-request-meta"></span><h4>Request.meta special keys<a class="headerlink" href="#request-meta-special-keys" title="Permalink to this headline">¶</a></h4>
<p>The <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> attribute can contain any arbitrary data, but there
are some special keys recognized by Scrapy and its built-in extensions.</p>
<p>Those are:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:reqmeta-dont_redirect"><tt class="xref std std-reqmeta docutils literal"><span class="pre">dont_redirect</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-dont_retry"><tt class="xref std std-reqmeta docutils literal"><span class="pre">dont_retry</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-handle_httpstatus_list"><tt class="xref std std-reqmeta docutils literal"><span class="pre">handle_httpstatus_list</span></tt></a></li>
<li><tt class="docutils literal"><span class="pre">dont_merge_cookies</span></tt> (see <tt class="docutils literal"><span class="pre">cookies</span></tt> parameter of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> constructor)</li>
<li><a class="reference internal" href="index.html#std:reqmeta-cookiejar"><tt class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-redirect_urls"><tt class="xref std std-reqmeta docutils literal"><span class="pre">redirect_urls</span></tt></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-bindaddress"><tt class="xref std std-reqmeta docutils literal"><span class="pre">bindaddress</span></tt></a></li>
</ul>
<div class="section" id="bindaddress">
<span id="std:reqmeta-bindaddress"></span><h5>bindaddress<a class="headerlink" href="#bindaddress" title="Permalink to this headline">¶</a></h5>
<p>The IP of the outgoing IP address to use for the performing the request.</p>
</div>
</div>
<div class="section" id="request-subclasses">
<span id="topics-request-response-ref-request-subclasses"></span><h4>Request subclasses<a class="headerlink" href="#request-subclasses" title="Permalink to this headline">¶</a></h4>
<p>Here is the list of built-in <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> subclasses. You can also subclass
it to implement your own custom functionality.</p>
<div class="section" id="formrequest-objects">
<h5>FormRequest objects<a class="headerlink" href="#formrequest-objects" title="Permalink to this headline">¶</a></h5>
<p>The FormRequest class extends the base <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> with functionality for
dealing with HTML forms. It uses <a class="reference external" href="http://lxml.de/lxmlhtml.html#forms">lxml.html forms</a>  to pre-populate form
fields with form data from <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> objects.</p>
<dl class="class">
<dt id="scrapy.http.FormRequest">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">FormRequest</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>formdata</em>, <em>...</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.FormRequest" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><tt class="xref py py-class docutils literal"><span class="pre">FormRequest</span></tt></a> class adds a new argument to the constructor. The
remaining arguments are the same as for the <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> class and are
not documented here.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>formdata</strong> (<em>dict or iterable of tuples</em>) &#8211; is a dictionary (or iterable of (key, value) tuples)
containing HTML Form data which will be url-encoded and assigned to the
body of the request.</td>
</tr>
</tbody>
</table>
<p>The <a class="reference internal" href="index.html#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><tt class="xref py py-class docutils literal"><span class="pre">FormRequest</span></tt></a> objects support the following class method in
addition to the standard <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> methods:</p>
<dl class="classmethod">
<dt id="scrapy.http.FormRequest.from_response">
<em class="property">classmethod </em><tt class="descname">from_response</tt><big>(</big><em>response</em><span class="optional">[</span>, <em>formname=None</em>, <em>formnumber=0</em>, <em>formdata=None</em>, <em>formxpath=None</em>, <em>clickdata=None</em>, <em>dont_click=False</em>, <em>...</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.FormRequest.from_response" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="index.html#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><tt class="xref py py-class docutils literal"><span class="pre">FormRequest</span></tt></a> object with its form field values
pre-populated with those found in the HTML <tt class="docutils literal"><span class="pre">&lt;form&gt;</span></tt> element contained
in the given response. For an example see
<a class="reference internal" href="index.html#topics-request-response-ref-request-userlogin"><em>Using FormRequest.from_response() to simulate a user login</em></a>.</p>
<p>The policy is to automatically simulate a click, by default, on any form
control that looks clickable, like a <tt class="docutils literal"><span class="pre">&lt;input</span> <span class="pre">type=&quot;submit&quot;&gt;</span></tt>.  Even
though this is quite convenient, and often the desired behaviour,
sometimes it can cause problems which could be hard to debug. For
example, when working with forms that are filled and/or submitted using
javascript, the default <a class="reference internal" href="index.html#scrapy.http.FormRequest.from_response" title="scrapy.http.FormRequest.from_response"><tt class="xref py py-meth docutils literal"><span class="pre">from_response()</span></tt></a> behaviour may not be the
most appropriate. To disable this behaviour you can set the
<tt class="docutils literal"><span class="pre">dont_click</span></tt> argument to <tt class="docutils literal"><span class="pre">True</span></tt>. Also, if you want to change the
control clicked (instead of disabling it) you can also use the
<tt class="docutils literal"><span class="pre">clickdata</span></tt> argument.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object) &#8211; the response containing a HTML form which will be used
to pre-populate the form fields</li>
<li><strong>formname</strong> (<em>string</em>) &#8211; if given, the form with name attribute set to this value will be used.</li>
<li><strong>formxpath</strong> (<em>string</em>) &#8211; if given, the first form that matches the xpath will be used.</li>
<li><strong>formnumber</strong> (<em>integer</em>) &#8211; the number of form to use, when the response contains
multiple forms. The first one (and also the default) is <tt class="docutils literal"><span class="pre">0</span></tt>.</li>
<li><strong>formdata</strong> (<em>dict</em>) &#8211; fields to override in the form data. If a field was
already present in the response <tt class="docutils literal"><span class="pre">&lt;form&gt;</span></tt> element, its value is
overridden by the one passed in this parameter.</li>
<li><strong>clickdata</strong> (<em>dict</em>) &#8211; attributes to lookup the control clicked. If it&#8217;s not
given, the form data will be submitted simulating a click on the
first clickable element. In addition to html attributes, the control
can be identified by its zero-based index relative to other
submittable inputs inside the form, via the <tt class="docutils literal"><span class="pre">nr</span></tt> attribute.</li>
<li><strong>dont_click</strong> (<em>boolean</em>) &#8211; If True, the form data will be submitted without
clicking in any element.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The other parameters of this class method are passed directly to the
<a class="reference internal" href="index.html#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><tt class="xref py py-class docutils literal"><span class="pre">FormRequest</span></tt></a> constructor.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.3: </span>The <tt class="docutils literal"><span class="pre">formname</span></tt> parameter.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span>The <tt class="docutils literal"><span class="pre">formxpath</span></tt> parameter.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="request-usage-examples">
<h5>Request usage examples<a class="headerlink" href="#request-usage-examples" title="Permalink to this headline">¶</a></h5>
<div class="section" id="using-formrequest-to-send-data-via-http-post">
<h6>Using FormRequest to send data via HTTP POST<a class="headerlink" href="#using-formrequest-to-send-data-via-http-post" title="Permalink to this headline">¶</a></h6>
<p>If you want to simulate a HTML Form POST in your spider and send a couple of
key-value fields, you can return a <a class="reference internal" href="index.html#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><tt class="xref py py-class docutils literal"><span class="pre">FormRequest</span></tt></a> object (from your
spider) like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">return</span> <span class="p">[</span><span class="n">FormRequest</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">&quot;http://www.example.com/post/action&quot;</span><span class="p">,</span>
                    <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;John Doe&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="s">&#39;27&#39;</span><span class="p">},</span>
                    <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">after_post</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<div class="section" id="using-formrequest-from-response-to-simulate-a-user-login">
<span id="topics-request-response-ref-request-userlogin"></span><h6>Using FormRequest.from_response() to simulate a user login<a class="headerlink" href="#using-formrequest-from-response-to-simulate-a-user-login" title="Permalink to this headline">¶</a></h6>
<p>It is usual for web sites to provide pre-populated form fields through <tt class="docutils literal"><span class="pre">&lt;input</span>
<span class="pre">type=&quot;hidden&quot;&gt;</span></tt> elements, such as session related data or authentication
tokens (for login pages). When scraping, you&#8217;ll want these fields to be
automatically pre-populated and only override a couple of them, such as the
user name and password. You can use the <a class="reference internal" href="index.html#scrapy.http.FormRequest.from_response" title="scrapy.http.FormRequest.from_response"><tt class="xref py py-meth docutils literal"><span class="pre">FormRequest.from_response()</span></tt></a>
method for this job. Here&#8217;s an example spider which uses it:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">LoginSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;example.com&#39;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/users/login.php&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">FormRequest</span><span class="o">.</span><span class="n">from_response</span><span class="p">(</span>
            <span class="n">response</span><span class="p">,</span>
            <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;username&#39;</span><span class="p">:</span> <span class="s">&#39;john&#39;</span><span class="p">,</span> <span class="s">&#39;password&#39;</span><span class="p">:</span> <span class="s">&#39;secret&#39;</span><span class="p">},</span>
            <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">after_login</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">after_login</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c"># check login succeed before going on</span>
        <span class="k">if</span> <span class="s">&quot;authentication failed&quot;</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s">&quot;Login failed&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">log</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="c"># continue scraping with authenticated session...</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="response-objects">
<h4>Response objects<a class="headerlink" href="#response-objects" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.http.Response">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">Response</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>status=200</em>, <em>headers</em>, <em>body</em>, <em>flags</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.Response" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object represents an HTTP response, which is usually
downloaded (by the Downloader) and fed to the Spiders for processing.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>url</strong> (<em>string</em>) &#8211; the URL of this response</li>
<li><strong>headers</strong> (<em>dict</em>) &#8211; the headers of this response. The dict values can be strings
(for single valued headers) or lists (for multi-valued headers).</li>
<li><strong>status</strong> (<em>integer</em>) &#8211; the HTTP status of the response. Defaults to <tt class="docutils literal"><span class="pre">200</span></tt>.</li>
<li><strong>body</strong> (<em>str</em>) &#8211; the response body. It must be str, not unicode, unless you&#8217;re
using a encoding-aware <a class="reference internal" href="index.html#topics-request-response-ref-response-subclasses"><em>Response subclass</em></a>, such as
<a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a>.</li>
<li><strong>meta</strong> (<em>dict</em>) &#8211; the initial values for the <a class="reference internal" href="index.html#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Response.meta</span></tt></a> attribute. If
given, the dict will be shallow copied.</li>
<li><strong>flags</strong> (<em>list</em>) &#8211; is a list containing the initial values for the
<a class="reference internal" href="index.html#scrapy.http.Response.flags" title="scrapy.http.Response.flags"><tt class="xref py py-attr docutils literal"><span class="pre">Response.flags</span></tt></a> attribute. If given, the list will be shallow
copied.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="scrapy.http.Response.url">
<tt class="descname">url</tt><a class="headerlink" href="#scrapy.http.Response.url" title="Permalink to this definition">¶</a></dt>
<dd><p>A string containing the URL of the response.</p>
<p>This attribute is read-only. To change the URL of a Response use
<a class="reference internal" href="index.html#scrapy.http.Response.replace" title="scrapy.http.Response.replace"><tt class="xref py py-meth docutils literal"><span class="pre">replace()</span></tt></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.status">
<tt class="descname">status</tt><a class="headerlink" href="#scrapy.http.Response.status" title="Permalink to this definition">¶</a></dt>
<dd><p>An integer representing the HTTP status of the response. Example: <tt class="docutils literal"><span class="pre">200</span></tt>,
<tt class="docutils literal"><span class="pre">404</span></tt>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.headers">
<tt class="descname">headers</tt><a class="headerlink" href="#scrapy.http.Response.headers" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary-like object which contains the response headers.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.body">
<tt class="descname">body</tt><a class="headerlink" href="#scrapy.http.Response.body" title="Permalink to this definition">¶</a></dt>
<dd><p>A str containing the body of this Response. Keep in mind that Response.body
is always a str. If you want the unicode version use
<a class="reference internal" href="index.html#scrapy.http.TextResponse.body_as_unicode" title="scrapy.http.TextResponse.body_as_unicode"><tt class="xref py py-meth docutils literal"><span class="pre">TextResponse.body_as_unicode()</span></tt></a> (only available in
<a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> and subclasses).</p>
<p>This attribute is read-only. To change the body of a Response use
<a class="reference internal" href="index.html#scrapy.http.Response.replace" title="scrapy.http.Response.replace"><tt class="xref py py-meth docutils literal"><span class="pre">replace()</span></tt></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.request">
<tt class="descname">request</tt><a class="headerlink" href="#scrapy.http.Response.request" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object that generated this response. This attribute is
assigned in the Scrapy engine, after the response and the request have passed
through all <a class="reference internal" href="index.html#topics-downloader-middleware"><em>Downloader Middlewares</em></a>.
In particular, this means that:</p>
<ul class="simple">
<li>HTTP redirections will cause the original request (to the URL before
redirection) to be assigned to the redirected response (with the final
URL after redirection).</li>
<li>Response.request.url doesn&#8217;t always equal Response.url</li>
<li>This attribute is only available in the spider code, and in the
<a class="reference internal" href="index.html#topics-spider-middleware"><em>Spider Middlewares</em></a>, but not in
Downloader Middlewares (although you have the Request available there by
other means) and handlers of the <a class="reference internal" href="index.html#std:signal-response_downloaded"><tt class="xref std std-signal docutils literal"><span class="pre">response_downloaded</span></tt></a> signal.</li>
</ul>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.meta">
<tt class="descname">meta</tt><a class="headerlink" href="#scrapy.http.Response.meta" title="Permalink to this definition">¶</a></dt>
<dd><p>A shortcut to the <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> attribute of the
<a class="reference internal" href="index.html#scrapy.http.Response.request" title="scrapy.http.Response.request"><tt class="xref py py-attr docutils literal"><span class="pre">Response.request</span></tt></a> object (ie. <tt class="docutils literal"><span class="pre">self.request.meta</span></tt>).</p>
<p>Unlike the <a class="reference internal" href="index.html#scrapy.http.Response.request" title="scrapy.http.Response.request"><tt class="xref py py-attr docutils literal"><span class="pre">Response.request</span></tt></a> attribute, the <a class="reference internal" href="index.html#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Response.meta</span></tt></a>
attribute is propagated along redirects and retries, so you will get
the original <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> sent from your spider.</p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> attribute</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.flags">
<tt class="descname">flags</tt><a class="headerlink" href="#scrapy.http.Response.flags" title="Permalink to this definition">¶</a></dt>
<dd><p>A list that contains flags for this response. Flags are labels used for
tagging Responses. For example: <cite>&#8216;cached&#8217;</cite>, <cite>&#8216;redirected</cite>&#8216;, etc. And
they&#8217;re shown on the string representation of the Response (<cite>__str__</cite>
method) which is used by the engine for logging.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Response.copy">
<tt class="descname">copy</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.http.Response.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new Response which is a copy of this Response.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Response.replace">
<tt class="descname">replace</tt><big>(</big><span class="optional">[</span><em>url</em>, <em>status</em>, <em>headers</em>, <em>body</em>, <em>request</em>, <em>flags</em>, <em>cls</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.Response.replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Response object with the same members, except for those members
given new values by whichever keyword arguments are specified. The
attribute <a class="reference internal" href="index.html#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Response.meta</span></tt></a> is copied by default.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="response-subclasses">
<span id="topics-request-response-ref-response-subclasses"></span><h4>Response subclasses<a class="headerlink" href="#response-subclasses" title="Permalink to this headline">¶</a></h4>
<p>Here is the list of available built-in Response subclasses. You can also
subclass the Response class to implement your own functionality.</p>
<div class="section" id="textresponse-objects">
<h5>TextResponse objects<a class="headerlink" href="#textresponse-objects" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.http.TextResponse">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">TextResponse</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>encoding</em><span class="optional">[</span>, <em>...</em><span class="optional">]</span><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.TextResponse" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> objects adds encoding capabilities to the base
<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> class, which is meant to be used only for binary data,
such as images, sounds or any media file.</p>
<p><a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> objects support a new constructor argument, in
addition to the base <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> objects. The remaining functionality
is the same as for the <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> class and is not documented here.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>encoding</strong> (<em>string</em>) &#8211; is a string which contains the encoding to use for this
response. If you create a <a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> object with a unicode
body, it will be encoded using this encoding (remember the body attribute
is always a string). If <tt class="docutils literal"><span class="pre">encoding</span></tt> is <tt class="docutils literal"><span class="pre">None</span></tt> (default value), the
encoding will be looked up in the response headers and body instead.</td>
</tr>
</tbody>
</table>
<p><a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> objects support the following attributes in addition
to the standard <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> ones:</p>
<dl class="attribute">
<dt id="scrapy.http.TextResponse.encoding">
<tt class="descname">encoding</tt><a class="headerlink" href="#scrapy.http.TextResponse.encoding" title="Permalink to this definition">¶</a></dt>
<dd><p>A string with the encoding of this response. The encoding is resolved by
trying the following mechanisms, in order:</p>
<ol class="arabic simple">
<li>the encoding passed in the constructor <cite>encoding</cite> argument</li>
<li>the encoding declared in the Content-Type HTTP header. If this
encoding is not valid (ie. unknown), it is ignored and the next
resolution mechanism is tried.</li>
<li>the encoding declared in the response body. The TextResponse class
doesn&#8217;t provide any special functionality for this. However, the
<a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></tt></a> and <a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></tt></a> classes do.</li>
<li>the encoding inferred by looking at the response body. This is the more
fragile method but also the last one tried.</li>
</ol>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.TextResponse.selector">
<tt class="descname">selector</tt><a class="headerlink" href="#scrapy.http.TextResponse.selector" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><tt class="xref py py-class docutils literal"><span class="pre">Selector</span></tt></a> instance using the response as
target. The selector is lazily instantiated on first access.</p>
</dd></dl>

<p><a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> objects support the following methods in addition to
the standard <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> ones:</p>
<dl class="method">
<dt id="scrapy.http.TextResponse.body_as_unicode">
<tt class="descname">body_as_unicode</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.http.TextResponse.body_as_unicode" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the body of the response as unicode. This is equivalent to:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">encoding</span><span class="p">)</span>
</pre></div>
</div>
<p>But <strong>not</strong> equivalent to:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="nb">unicode</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">)</span>
</pre></div>
</div>
<p>Since, in the latter case, you would be using you system default encoding
(typically <cite>ascii</cite>) to convert the body to unicode, instead of the response
encoding.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.TextResponse.xpath">
<tt class="descname">xpath</tt><big>(</big><em>query</em><big>)</big><a class="headerlink" href="#scrapy.http.TextResponse.xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>A shortcut to <tt class="docutils literal"><span class="pre">TextResponse.selector.xpath(query)</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">&#39;//p&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.TextResponse.css">
<tt class="descname">css</tt><big>(</big><em>query</em><big>)</big><a class="headerlink" href="#scrapy.http.TextResponse.css" title="Permalink to this definition">¶</a></dt>
<dd><p>A shortcut to <tt class="docutils literal"><span class="pre">TextResponse.selector.css(query)</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s">&#39;p&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="htmlresponse-objects">
<h5>HtmlResponse objects<a class="headerlink" href="#htmlresponse-objects" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.http.HtmlResponse">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">HtmlResponse</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>...</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.HtmlResponse" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></tt></a> class is a subclass of <a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a>
which adds encoding auto-discovering support by looking into the HTML <a class="reference external" href="http://www.w3schools.com/TAGS/att_meta_http_equiv.asp">meta
http-equiv</a> attribute.  See <a class="reference internal" href="index.html#scrapy.http.TextResponse.encoding" title="scrapy.http.TextResponse.encoding"><tt class="xref py py-attr docutils literal"><span class="pre">TextResponse.encoding</span></tt></a>.</p>
</dd></dl>

</div>
<div class="section" id="xmlresponse-objects">
<h5>XmlResponse objects<a class="headerlink" href="#xmlresponse-objects" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.http.XmlResponse">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">XmlResponse</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>...</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.XmlResponse" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></tt></a> class is a subclass of <a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> which
adds encoding auto-discovering support by looking into the XML declaration
line.  See <a class="reference internal" href="index.html#scrapy.http.TextResponse.encoding" title="scrapy.http.TextResponse.encoding"><tt class="xref py py-attr docutils literal"><span class="pre">TextResponse.encoding</span></tt></a>.</p>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/settings"></span><div class="section" id="settings">
<span id="topics-settings"></span><h3>Settings<a class="headerlink" href="#settings" title="Permalink to this headline">¶</a></h3>
<p>The Scrapy settings allows you to customize the behaviour of all Scrapy
components, including the core, extensions, pipelines and spiders themselves.</p>
<p>The infrastructure of the settings provides a global namespace of key-value mappings
that the code can use to pull configuration values from. The settings can be
populated through different mechanisms, which are described below.</p>
<p>The settings are also the mechanism for selecting the currently active Scrapy
project (in case you have many).</p>
<p>For a list of available built-in settings see: <a class="reference internal" href="index.html#topics-settings-ref"><em>Built-in settings reference</em></a>.</p>
<div class="section" id="designating-the-settings">
<h4>Designating the settings<a class="headerlink" href="#designating-the-settings" title="Permalink to this headline">¶</a></h4>
<p>When you use Scrapy, you have to tell it which settings you&#8217;re using. You can
do this by using an environment variable, <tt class="docutils literal"><span class="pre">SCRAPY_SETTINGS_MODULE</span></tt>.</p>
<p>The value of <tt class="docutils literal"><span class="pre">SCRAPY_SETTINGS_MODULE</span></tt> should be in Python path syntax, e.g.
<tt class="docutils literal"><span class="pre">myproject.settings</span></tt>. Note that the settings module should be on the
Python <a class="reference external" href="http://docs.python.org/2/tutorial/modules.html#the-module-search-path">import search path</a>.</p>
</div>
<div class="section" id="populating-the-settings">
<h4>Populating the settings<a class="headerlink" href="#populating-the-settings" title="Permalink to this headline">¶</a></h4>
<p>Settings can be populated using different mechanisms, each of which having a
different precedence. Here is the list of them in decreasing order of
precedence:</p>
<blockquote>
<div><ol class="arabic simple">
<li>Command line options (most precedence)</li>
<li>Project settings module</li>
<li>Default settings per-command</li>
<li>Default global settings (less precedence)</li>
</ol>
</div></blockquote>
<p>The population of these settings sources is taken care of internally, but a
manual handling is possible using API calls. See the
<a class="reference internal" href="index.html#topics-api-settings"><em>Settings API</em></a> topic for reference.</p>
<p>These mechanisms are described in more detail below.</p>
<div class="section" id="command-line-options">
<h5>1. Command line options<a class="headerlink" href="#command-line-options" title="Permalink to this headline">¶</a></h5>
<p>Arguments provided by the command line are the ones that take most precedence,
overriding any other options. You can explicitly override one (or more)
settings using the <tt class="docutils literal"><span class="pre">-s</span></tt> (or <tt class="docutils literal"><span class="pre">--set</span></tt>) command line option.</p>
<p>Example:</p>
<div class="highlight-sh"><div class="highlight"><pre>scrapy crawl myspider -s <span class="nv">LOG_FILE</span><span class="o">=</span>scrapy.log
</pre></div>
</div>
</div>
<div class="section" id="project-settings-module">
<h5>2. Project settings module<a class="headerlink" href="#project-settings-module" title="Permalink to this headline">¶</a></h5>
<p>The project settings module is the standard configuration file for your Scrapy
project.  It&#8217;s where most of your custom settings will be populated. For
example:: <tt class="docutils literal"><span class="pre">myproject.settings</span></tt>.</p>
</div>
<div class="section" id="default-settings-per-command">
<h5>3. Default settings per-command<a class="headerlink" href="#default-settings-per-command" title="Permalink to this headline">¶</a></h5>
<p>Each <a class="reference internal" href="index.html#document-topics/commands"><em>Scrapy tool</em></a> command can have its own default
settings, which override the global default settings. Those custom command
settings are specified in the <tt class="docutils literal"><span class="pre">default_settings</span></tt> attribute of the command
class.</p>
</div>
<div class="section" id="default-global-settings">
<h5>4. Default global settings<a class="headerlink" href="#default-global-settings" title="Permalink to this headline">¶</a></h5>
<p>The global defaults are located in the <tt class="docutils literal"><span class="pre">scrapy.settings.default_settings</span></tt>
module and documented in the <a class="reference internal" href="index.html#topics-settings-ref"><em>Built-in settings reference</em></a> section.</p>
</div>
</div>
<div class="section" id="how-to-access-settings">
<h4>How to access settings<a class="headerlink" href="#how-to-access-settings" title="Permalink to this headline">¶</a></h4>
<p>Settings can be accessed through the <a class="reference internal" href="index.html#scrapy.crawler.Crawler.settings" title="scrapy.crawler.Crawler.settings"><tt class="xref py py-attr docutils literal"><span class="pre">scrapy.crawler.Crawler.settings</span></tt></a>
attribute of the Crawler that is passed to <tt class="docutils literal"><span class="pre">from_crawler</span></tt> method in
extensions and middlewares:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">MyExtension</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="n">settings</span> <span class="o">=</span> <span class="n">crawler</span><span class="o">.</span><span class="n">settings</span>
        <span class="k">if</span> <span class="n">settings</span><span class="p">[</span><span class="s">&#39;LOG_ENABLED&#39;</span><span class="p">]:</span>
            <span class="k">print</span> <span class="s">&quot;log is enabled!&quot;</span>
</pre></div>
</div>
<p>In other words, settings can be accessed like a dict, but it&#8217;s usually preferred
to extract the setting in the format you need it to avoid type errors. In order
to do that you&#8217;ll have to use one of the methods provided the
<a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><tt class="xref py py-class docutils literal"><span class="pre">Settings</span></tt></a> API.</p>
</div>
<div class="section" id="rationale-for-setting-names">
<h4>Rationale for setting names<a class="headerlink" href="#rationale-for-setting-names" title="Permalink to this headline">¶</a></h4>
<p>Setting names are usually prefixed with the component that they configure. For
example, proper setting names for a fictional robots.txt extension would be
<tt class="docutils literal"><span class="pre">ROBOTSTXT_ENABLED</span></tt>, <tt class="docutils literal"><span class="pre">ROBOTSTXT_OBEY</span></tt>, <tt class="docutils literal"><span class="pre">ROBOTSTXT_CACHEDIR</span></tt>, etc.</p>
</div>
<div class="section" id="built-in-settings-reference">
<span id="topics-settings-ref"></span><h4>Built-in settings reference<a class="headerlink" href="#built-in-settings-reference" title="Permalink to this headline">¶</a></h4>
<p>Here&#8217;s a list of all available Scrapy settings, in alphabetical order, along
with their default values and the scope where they apply.</p>
<p>The scope, where available, shows where the setting is being used, if it&#8217;s tied
to any particular component. In that case the module of that component will be
shown, typically an extension, middleware or pipeline. It also means that the
component must be enabled in order for the setting to have any effect.</p>
<div class="section" id="aws-access-key-id">
<span id="std:setting-AWS_ACCESS_KEY_ID"></span><h5>AWS_ACCESS_KEY_ID<a class="headerlink" href="#aws-access-key-id" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">None</span></tt></p>
<p>The AWS access key used by code that requires access to <a class="reference external" href="http://aws.amazon.com/">Amazon Web services</a>,
such as the <a class="reference internal" href="index.html#topics-feed-storage-s3"><em>S3 feed storage backend</em></a>.</p>
</div>
<div class="section" id="aws-secret-access-key">
<span id="std:setting-AWS_SECRET_ACCESS_KEY"></span><h5>AWS_SECRET_ACCESS_KEY<a class="headerlink" href="#aws-secret-access-key" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">None</span></tt></p>
<p>The AWS secret key used by code that requires access to <a class="reference external" href="http://aws.amazon.com/">Amazon Web services</a>,
such as the <a class="reference internal" href="index.html#topics-feed-storage-s3"><em>S3 feed storage backend</em></a>.</p>
</div>
<div class="section" id="bot-name">
<span id="std:setting-BOT_NAME"></span><h5>BOT_NAME<a class="headerlink" href="#bot-name" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">'scrapybot'</span></tt></p>
<p>The name of the bot implemented by this Scrapy project (also known as the
project name). This will be used to construct the User-Agent by default, and
also for logging.</p>
<p>It&#8217;s automatically populated with your project name when you create your
project with the <a class="reference internal" href="index.html#std:command-startproject"><tt class="xref std std-command docutils literal"><span class="pre">startproject</span></tt></a> command.</p>
</div>
<div class="section" id="concurrent-items">
<span id="std:setting-CONCURRENT_ITEMS"></span><h5>CONCURRENT_ITEMS<a class="headerlink" href="#concurrent-items" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">100</span></tt></p>
<p>Maximum number of concurrent items (per response) to process in parallel in the
Item Processor (also known as the <a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a>).</p>
</div>
<div class="section" id="concurrent-requests">
<span id="std:setting-CONCURRENT_REQUESTS"></span><h5>CONCURRENT_REQUESTS<a class="headerlink" href="#concurrent-requests" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">16</span></tt></p>
<p>The maximum number of concurrent (ie. simultaneous) requests that will be
performed by the Scrapy downloader.</p>
</div>
<div class="section" id="concurrent-requests-per-domain">
<span id="std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"></span><h5>CONCURRENT_REQUESTS_PER_DOMAIN<a class="headerlink" href="#concurrent-requests-per-domain" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">8</span></tt></p>
<p>The maximum number of concurrent (ie. simultaneous) requests that will be
performed to any single domain.</p>
</div>
<div class="section" id="concurrent-requests-per-ip">
<span id="std:setting-CONCURRENT_REQUESTS_PER_IP"></span><h5>CONCURRENT_REQUESTS_PER_IP<a class="headerlink" href="#concurrent-requests-per-ip" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>The maximum number of concurrent (ie. simultaneous) requests that will be
performed to any single IP. If non-zero, the
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></tt></a> setting is ignored, and this one is
used instead. In other words, concurrency limits will be applied per IP, not
per domain.</p>
<p>This setting also affects <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></tt></a>:
if <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></tt></a> is non-zero, download delay is
enforced per IP, not per domain.</p>
</div>
<div class="section" id="default-item-class">
<span id="std:setting-DEFAULT_ITEM_CLASS"></span><h5>DEFAULT_ITEM_CLASS<a class="headerlink" href="#default-item-class" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">'scrapy.item.Item'</span></tt></p>
<p>The default class that will be used for instantiating items in the <a class="reference internal" href="index.html#topics-shell"><em>the
Scrapy shell</em></a>.</p>
</div>
<div class="section" id="default-request-headers">
<span id="std:setting-DEFAULT_REQUEST_HEADERS"></span><h5>DEFAULT_REQUEST_HEADERS<a class="headerlink" href="#default-request-headers" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">{</span>
    <span class="s">&#39;Accept&#39;</span><span class="p">:</span> <span class="s">&#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;</span><span class="p">,</span>
    <span class="s">&#39;Accept-Language&#39;</span><span class="p">:</span> <span class="s">&#39;en&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The default headers used for Scrapy HTTP Requests. They&#8217;re populated in the
<a class="reference internal" href="index.html#scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware" title="scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">DefaultHeadersMiddleware</span></tt></a>.</p>
</div>
<div class="section" id="depth-limit">
<span id="std:setting-DEPTH_LIMIT"></span><h5>DEPTH_LIMIT<a class="headerlink" href="#depth-limit" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>The maximum depth that will be allowed to crawl for any site. If zero, no limit
will be imposed.</p>
</div>
<div class="section" id="depth-priority">
<span id="std:setting-DEPTH_PRIORITY"></span><h5>DEPTH_PRIORITY<a class="headerlink" href="#depth-priority" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>An integer that is used to adjust the request priority based on its depth.</p>
<p>If zero, no priority adjustment is made from depth.</p>
</div>
<div class="section" id="depth-stats">
<span id="std:setting-DEPTH_STATS"></span><h5>DEPTH_STATS<a class="headerlink" href="#depth-stats" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Whether to collect maximum depth stats.</p>
</div>
<div class="section" id="depth-stats-verbose">
<span id="std:setting-DEPTH_STATS_VERBOSE"></span><h5>DEPTH_STATS_VERBOSE<a class="headerlink" href="#depth-stats-verbose" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Whether to collect verbose depth stats. If this is enabled, the number of
requests for each depth is collected in the stats.</p>
</div>
<div class="section" id="dnscache-enabled">
<span id="std:setting-DNSCACHE_ENABLED"></span><h5>DNSCACHE_ENABLED<a class="headerlink" href="#dnscache-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Whether to enable DNS in-memory cache.</p>
</div>
<div class="section" id="downloader">
<span id="std:setting-DOWNLOADER"></span><h5>DOWNLOADER<a class="headerlink" href="#downloader" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">'scrapy.core.downloader.Downloader'</span></tt></p>
<p>The downloader to use for crawling.</p>
</div>
<div class="section" id="downloader-middlewares">
<span id="std:setting-DOWNLOADER_MIDDLEWARES"></span><h5>DOWNLOADER_MIDDLEWARES<a class="headerlink" href="#downloader-middlewares" title="Permalink to this headline">¶</a></h5>
<p>Default:: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>A dict containing the downloader middlewares enabled in your project, and their
orders. For more info see <a class="reference internal" href="index.html#topics-downloader-middleware-setting"><em>Activating a downloader middleware</em></a>.</p>
</div>
<div class="section" id="downloader-middlewares-base">
<span id="std:setting-DOWNLOADER_MIDDLEWARES_BASE"></span><h5>DOWNLOADER_MIDDLEWARES_BASE<a class="headerlink" href="#downloader-middlewares-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">{</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware&#39;</span><span class="p">:</span> <span class="mi">350</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware&#39;</span><span class="p">:</span> <span class="mi">400</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.retry.RetryMiddleware&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware&#39;</span><span class="p">:</span> <span class="mi">550</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware&#39;</span><span class="p">:</span> <span class="mi">580</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware&#39;</span><span class="p">:</span> <span class="mi">590</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware&#39;</span><span class="p">:</span> <span class="mi">600</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware&#39;</span><span class="p">:</span> <span class="mi">700</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware&#39;</span><span class="p">:</span> <span class="mi">750</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware&#39;</span><span class="p">:</span> <span class="mi">830</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.stats.DownloaderStats&#39;</span><span class="p">:</span> <span class="mi">850</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware&#39;</span><span class="p">:</span> <span class="mi">900</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the downloader middlewares enabled by default in Scrapy. You
should never modify this setting in your project, modify
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></tt></a> instead.  For more info see
<a class="reference internal" href="index.html#topics-downloader-middleware-setting"><em>Activating a downloader middleware</em></a>.</p>
</div>
<div class="section" id="downloader-stats">
<span id="std:setting-DOWNLOADER_STATS"></span><h5>DOWNLOADER_STATS<a class="headerlink" href="#downloader-stats" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Whether to enable downloader stats collection.</p>
</div>
<div class="section" id="download-delay">
<span id="std:setting-DOWNLOAD_DELAY"></span><h5>DOWNLOAD_DELAY<a class="headerlink" href="#download-delay" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>The amount of time (in secs) that the downloader should wait before downloading
consecutive pages from the same website. This can be used to throttle the
crawling speed to avoid hitting servers too hard. Decimal numbers are
supported.  Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mf">0.25</span>    <span class="c"># 250 ms of delay</span>
</pre></div>
</div>
<p>This setting is also affected by the <a class="reference internal" href="index.html#std:setting-RANDOMIZE_DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">RANDOMIZE_DOWNLOAD_DELAY</span></tt></a>
setting (which is enabled by default). By default, Scrapy doesn&#8217;t wait a fixed
amount of time between requests, but uses a random interval between 0.5 and 1.5
* <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></tt></a>.</p>
<p>When <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></tt></a> is non-zero, delays are enforced
per ip address instead of per domain.</p>
<p>You can also change this setting per spider by setting <tt class="docutils literal"><span class="pre">download_delay</span></tt>
spider attribute.</p>
</div>
<div class="section" id="download-handlers">
<span id="std:setting-DOWNLOAD_HANDLERS"></span><h5>DOWNLOAD_HANDLERS<a class="headerlink" href="#download-handlers" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>A dict containing the request downloader handlers enabled in your project.
See <cite>DOWNLOAD_HANDLERS_BASE</cite> for example format.</p>
</div>
<div class="section" id="download-handlers-base">
<span id="std:setting-DOWNLOAD_HANDLERS_BASE"></span><h5>DOWNLOAD_HANDLERS_BASE<a class="headerlink" href="#download-handlers-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">{</span>
    <span class="s">&#39;file&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.core.downloader.handlers.file.FileDownloadHandler&#39;</span><span class="p">,</span>
    <span class="s">&#39;http&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.core.downloader.handlers.http.HttpDownloadHandler&#39;</span><span class="p">,</span>
    <span class="s">&#39;https&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.core.downloader.handlers.http.HttpDownloadHandler&#39;</span><span class="p">,</span>
    <span class="s">&#39;s3&#39;</span><span class="p">:</span> <span class="s">&#39;scrapy.core.downloader.handlers.s3.S3DownloadHandler&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the request download handlers enabled by default in Scrapy.
You should never modify this setting in your project, modify
<a class="reference internal" href="index.html#std:setting-DOWNLOAD_HANDLERS"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS</span></tt></a> instead.</p>
<p>If you want to disable any of the above download handlers you must define them
in your project&#8217;s <a class="reference internal" href="index.html#std:setting-DOWNLOAD_HANDLERS"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS</span></tt></a> setting and assign <cite>None</cite>
as their value.  For example, if you want to disable the file download
handler:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">DOWNLOAD_HANDLERS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">&#39;file&#39;</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="download-timeout">
<span id="std:setting-DOWNLOAD_TIMEOUT"></span><h5>DOWNLOAD_TIMEOUT<a class="headerlink" href="#download-timeout" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">180</span></tt></p>
<p>The amount of time (in secs) that the downloader will wait before timing out.</p>
</div>
<div class="section" id="dupefilter-class">
<span id="std:setting-DUPEFILTER_CLASS"></span><h5>DUPEFILTER_CLASS<a class="headerlink" href="#dupefilter-class" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">'scrapy.dupefilter.RFPDupeFilter'</span></tt></p>
<p>The class used to detect and filter duplicate requests.</p>
<p>The default (<tt class="docutils literal"><span class="pre">RFPDupeFilter</span></tt>) filters based on request fingerprint using
the <tt class="docutils literal"><span class="pre">scrapy.utils.request.request_fingerprint</span></tt> function. In order to change
the way duplicates are checked you could subclass <tt class="docutils literal"><span class="pre">RFPDupeFilter</span></tt> and
override its <tt class="docutils literal"><span class="pre">request_fingerprint</span></tt> method. This method should accept
scrapy <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object and return its fingerprint
(a string).</p>
</div>
<div class="section" id="dupefilter-debug">
<span id="std:setting-DUPEFILTER_DEBUG"></span><h5>DUPEFILTER_DEBUG<a class="headerlink" href="#dupefilter-debug" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>By default, <tt class="docutils literal"><span class="pre">RFPDupeFilter</span></tt> only logs the first duplicate request.
Setting <a class="reference internal" href="index.html#std:setting-DUPEFILTER_DEBUG"><tt class="xref std std-setting docutils literal"><span class="pre">DUPEFILTER_DEBUG</span></tt></a> to <tt class="docutils literal"><span class="pre">True</span></tt> will make it log all duplicate requests.</p>
</div>
<div class="section" id="editor">
<span id="std:setting-EDITOR"></span><h5>EDITOR<a class="headerlink" href="#editor" title="Permalink to this headline">¶</a></h5>
<p>Default: <cite>depends on the environment</cite></p>
<p>The editor to use for editing spiders with the <a class="reference internal" href="index.html#std:command-edit"><tt class="xref std std-command docutils literal"><span class="pre">edit</span></tt></a> command. It
defaults to the <tt class="docutils literal"><span class="pre">EDITOR</span></tt> environment variable, if set. Otherwise, it defaults
to <tt class="docutils literal"><span class="pre">vi</span></tt> (on Unix systems) or the IDLE editor (on Windows).</p>
</div>
<div class="section" id="extensions">
<span id="std:setting-EXTENSIONS"></span><h5>EXTENSIONS<a class="headerlink" href="#extensions" title="Permalink to this headline">¶</a></h5>
<p>Default:: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>A dict containing the extensions enabled in your project, and their orders.</p>
</div>
<div class="section" id="extensions-base">
<span id="std:setting-EXTENSIONS_BASE"></span><h5>EXTENSIONS_BASE<a class="headerlink" href="#extensions-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">{</span>
    <span class="s">&#39;scrapy.contrib.corestats.CoreStats&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.webservice.WebService&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.telnet.TelnetConsole&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.memusage.MemoryUsage&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.memdebug.MemoryDebugger&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.closespider.CloseSpider&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.feedexport.FeedExporter&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.logstats.LogStats&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.spiderstate.SpiderState&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.throttle.AutoThrottle&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The list of available extensions. Keep in mind that some of them need to
be enabled through a setting. By default, this setting contains all stable
built-in extensions.</p>
<p>For more information See the <a class="reference internal" href="index.html#topics-extensions"><em>extensions user guide</em></a>
and the <a class="reference internal" href="index.html#topics-extensions-ref"><em>list of available extensions</em></a>.</p>
</div>
<div class="section" id="item-pipelines">
<span id="std:setting-ITEM_PIPELINES"></span><h5>ITEM_PIPELINES<a class="headerlink" href="#item-pipelines" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>A dict containing the item pipelines to use, and their orders. The dict is
empty by default order values are arbitrary but it&#8217;s customary to define them
in the 0-1000 range.</p>
<p>Lists are supported in <a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><tt class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></tt></a> for backwards compatibility,
but they are deprecated.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">&#39;mybot.pipelines.validate.ValidateMyItem&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s">&#39;mybot.pipelines.validate.StoreMyItem&#39;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="item-pipelines-base">
<span id="std:setting-ITEM_PIPELINES_BASE"></span><h5>ITEM_PIPELINES_BASE<a class="headerlink" href="#item-pipelines-base" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>A dict containing the pipelines enabled by default in Scrapy. You should never
modify this setting in your project, modify <a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><tt class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></tt></a> instead.</p>
</div>
<div class="section" id="log-enabled">
<span id="std:setting-LOG_ENABLED"></span><h5>LOG_ENABLED<a class="headerlink" href="#log-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Whether to enable logging.</p>
</div>
<div class="section" id="log-encoding">
<span id="std:setting-LOG_ENCODING"></span><h5>LOG_ENCODING<a class="headerlink" href="#log-encoding" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">'utf-8'</span></tt></p>
<p>The encoding to use for logging.</p>
</div>
<div class="section" id="log-file">
<span id="std:setting-LOG_FILE"></span><h5>LOG_FILE<a class="headerlink" href="#log-file" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">None</span></tt></p>
<p>File name to use for logging output. If None, standard error will be used.</p>
</div>
<div class="section" id="log-level">
<span id="std:setting-LOG_LEVEL"></span><h5>LOG_LEVEL<a class="headerlink" href="#log-level" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">'DEBUG'</span></tt></p>
<p>Minimum level to log. Available levels are: CRITICAL, ERROR, WARNING,
INFO, DEBUG. For more info see <a class="reference internal" href="index.html#topics-logging"><em>Logging</em></a>.</p>
</div>
<div class="section" id="log-stdout">
<span id="std:setting-LOG_STDOUT"></span><h5>LOG_STDOUT<a class="headerlink" href="#log-stdout" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>If <tt class="docutils literal"><span class="pre">True</span></tt>, all standard output (and error) of your process will be redirected
to the log. For example if you <tt class="docutils literal"><span class="pre">print</span> <span class="pre">'hello'</span></tt> it will appear in the Scrapy
log.</p>
</div>
<div class="section" id="memdebug-enabled">
<span id="std:setting-MEMDEBUG_ENABLED"></span><h5>MEMDEBUG_ENABLED<a class="headerlink" href="#memdebug-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Whether to enable memory debugging.</p>
</div>
<div class="section" id="memdebug-notify">
<span id="std:setting-MEMDEBUG_NOTIFY"></span><h5>MEMDEBUG_NOTIFY<a class="headerlink" href="#memdebug-notify" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">[]</span></tt></p>
<p>When memory debugging is enabled a memory report will be sent to the specified
addresses if this setting is not empty, otherwise the report will be written to
the log.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">MEMDEBUG_NOTIFY</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;user@example.com&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="memusage-enabled">
<span id="std:setting-MEMUSAGE_ENABLED"></span><h5>MEMUSAGE_ENABLED<a class="headerlink" href="#memusage-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Scope: <tt class="docutils literal"><span class="pre">scrapy.contrib.memusage</span></tt></p>
<p>Whether to enable the memory usage extension that will shutdown the Scrapy
process when it exceeds a memory limit, and also notify by email when that
happened.</p>
<p>See <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><em>Memory usage extension</em></a>.</p>
</div>
<div class="section" id="memusage-limit-mb">
<span id="std:setting-MEMUSAGE_LIMIT_MB"></span><h5>MEMUSAGE_LIMIT_MB<a class="headerlink" href="#memusage-limit-mb" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>Scope: <tt class="docutils literal"><span class="pre">scrapy.contrib.memusage</span></tt></p>
<p>The maximum amount of memory to allow (in megabytes) before shutting down
Scrapy  (if MEMUSAGE_ENABLED is True). If zero, no check will be performed.</p>
<p>See <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><em>Memory usage extension</em></a>.</p>
</div>
<div class="section" id="memusage-notify-mail">
<span id="std:setting-MEMUSAGE_NOTIFY_MAIL"></span><h5>MEMUSAGE_NOTIFY_MAIL<a class="headerlink" href="#memusage-notify-mail" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Scope: <tt class="docutils literal"><span class="pre">scrapy.contrib.memusage</span></tt></p>
<p>A list of emails to notify if the memory limit has been reached.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">MEMUSAGE_NOTIFY_MAIL</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;user@example.com&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><em>Memory usage extension</em></a>.</p>
</div>
<div class="section" id="memusage-report">
<span id="std:setting-MEMUSAGE_REPORT"></span><h5>MEMUSAGE_REPORT<a class="headerlink" href="#memusage-report" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Scope: <tt class="docutils literal"><span class="pre">scrapy.contrib.memusage</span></tt></p>
<p>Whether to send a memory usage report after each spider has been closed.</p>
<p>See <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><em>Memory usage extension</em></a>.</p>
</div>
<div class="section" id="memusage-warning-mb">
<span id="std:setting-MEMUSAGE_WARNING_MB"></span><h5>MEMUSAGE_WARNING_MB<a class="headerlink" href="#memusage-warning-mb" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>Scope: <tt class="docutils literal"><span class="pre">scrapy.contrib.memusage</span></tt></p>
<p>The maximum amount of memory to allow (in megabytes) before sending a warning
email notifying about it. If zero, no warning will be produced.</p>
</div>
<div class="section" id="newspider-module">
<span id="std:setting-NEWSPIDER_MODULE"></span><h5>NEWSPIDER_MODULE<a class="headerlink" href="#newspider-module" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">''</span></tt></p>
<p>Module where to create new spiders using the <a class="reference internal" href="index.html#std:command-genspider"><tt class="xref std std-command docutils literal"><span class="pre">genspider</span></tt></a> command.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">NEWSPIDER_MODULE</span> <span class="o">=</span> <span class="s">&#39;mybot.spiders_dev&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="randomize-download-delay">
<span id="std:setting-RANDOMIZE_DOWNLOAD_DELAY"></span><h5>RANDOMIZE_DOWNLOAD_DELAY<a class="headerlink" href="#randomize-download-delay" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>If enabled, Scrapy will wait a random amount of time (between 0.5 and 1.5
* <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></tt></a>) while fetching requests from the same
website.</p>
<p>This randomization decreases the chance of the crawler being detected (and
subsequently blocked) by sites which analyze requests looking for statistically
significant similarities in the time between their requests.</p>
<p>The randomization policy is the same used by <a class="reference external" href="http://www.gnu.org/software/wget/manual/wget.html">wget</a> <tt class="docutils literal"><span class="pre">--random-wait</span></tt> option.</p>
<p>If <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></tt></a> is zero (default) this option has no effect.</p>
</div>
<div class="section" id="redirect-max-times">
<span id="std:setting-REDIRECT_MAX_TIMES"></span><h5>REDIRECT_MAX_TIMES<a class="headerlink" href="#redirect-max-times" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">20</span></tt></p>
<p>Defines the maximum times a request can be redirected. After this maximum the
request&#8217;s response is returned as is. We used Firefox default value for the
same task.</p>
</div>
<div class="section" id="redirect-max-metarefresh-delay">
<span id="std:setting-REDIRECT_MAX_METAREFRESH_DELAY"></span><h5>REDIRECT_MAX_METAREFRESH_DELAY<a class="headerlink" href="#redirect-max-metarefresh-delay" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">100</span></tt></p>
<p>Some sites use meta-refresh for redirecting to a session expired page, so we
restrict automatic redirection to a maximum delay (in seconds)</p>
</div>
<div class="section" id="redirect-priority-adjust">
<span id="std:setting-REDIRECT_PRIORITY_ADJUST"></span><h5>REDIRECT_PRIORITY_ADJUST<a class="headerlink" href="#redirect-priority-adjust" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">+2</span></tt></p>
<p>Adjust redirect request priority relative to original request.
A negative priority adjust means more priority.</p>
</div>
<div class="section" id="robotstxt-obey">
<span id="std:setting-ROBOTSTXT_OBEY"></span><h5>ROBOTSTXT_OBEY<a class="headerlink" href="#robotstxt-obey" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Scope: <tt class="docutils literal"><span class="pre">scrapy.contrib.downloadermiddleware.robotstxt</span></tt></p>
<p>If enabled, Scrapy will respect robots.txt policies. For more information see
<a class="reference internal" href="index.html#topics-dlmw-robots"><em>RobotsTxtMiddleware</em></a></p>
</div>
<div class="section" id="scheduler">
<span id="std:setting-SCHEDULER"></span><h5>SCHEDULER<a class="headerlink" href="#scheduler" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">'scrapy.core.scheduler.Scheduler'</span></tt></p>
<p>The scheduler to use for crawling.</p>
</div>
<div class="section" id="spider-contracts">
<span id="std:setting-SPIDER_CONTRACTS"></span><h5>SPIDER_CONTRACTS<a class="headerlink" href="#spider-contracts" title="Permalink to this headline">¶</a></h5>
<p>Default:: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>A dict containing the scrapy contracts enabled in your project, used for
testing spiders. For more info see <a class="reference internal" href="index.html#topics-contracts"><em>Spiders Contracts</em></a>.</p>
</div>
<div class="section" id="spider-contracts-base">
<span id="std:setting-SPIDER_CONTRACTS_BASE"></span><h5>SPIDER_CONTRACTS_BASE<a class="headerlink" href="#spider-contracts-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">{</span>
    <span class="s">&#39;scrapy.contracts.default.UrlContract&#39;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contracts.default.ReturnsContract&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contracts.default.ScrapesContract&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the scrapy contracts enabled by default in Scrapy. You should
never modify this setting in your project, modify <a class="reference internal" href="index.html#std:setting-SPIDER_CONTRACTS"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_CONTRACTS</span></tt></a>
instead. For more info see <a class="reference internal" href="index.html#topics-contracts"><em>Spiders Contracts</em></a>.</p>
</div>
<div class="section" id="spider-middlewares">
<span id="std:setting-SPIDER_MIDDLEWARES"></span><h5>SPIDER_MIDDLEWARES<a class="headerlink" href="#spider-middlewares" title="Permalink to this headline">¶</a></h5>
<p>Default:: <tt class="docutils literal"><span class="pre">{}</span></tt></p>
<p>A dict containing the spider middlewares enabled in your project, and their
orders. For more info see <a class="reference internal" href="index.html#topics-spider-middleware-setting"><em>Activating a spider middleware</em></a>.</p>
</div>
<div class="section" id="spider-middlewares-base">
<span id="std:setting-SPIDER_MIDDLEWARES_BASE"></span><h5>SPIDER_MIDDLEWARES_BASE<a class="headerlink" href="#spider-middlewares-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">{</span>
    <span class="s">&#39;scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.spidermiddleware.referer.RefererMiddleware&#39;</span><span class="p">:</span> <span class="mi">700</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware&#39;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.spidermiddleware.depth.DepthMiddleware&#39;</span><span class="p">:</span> <span class="mi">900</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the spider middlewares enabled by default in Scrapy. You
should never modify this setting in your project, modify
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES</span></tt></a> instead. For more info see
<a class="reference internal" href="index.html#topics-spider-middleware-setting"><em>Activating a spider middleware</em></a>.</p>
</div>
<div class="section" id="spider-modules">
<span id="std:setting-SPIDER_MODULES"></span><h5>SPIDER_MODULES<a class="headerlink" href="#spider-modules" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">[]</span></tt></p>
<p>A list of modules where Scrapy will look for spiders.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">SPIDER_MODULES</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;mybot.spiders_prod&#39;</span><span class="p">,</span> <span class="s">&#39;mybot.spiders_dev&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="stats-class">
<span id="std:setting-STATS_CLASS"></span><h5>STATS_CLASS<a class="headerlink" href="#stats-class" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">'scrapy.statscol.MemoryStatsCollector'</span></tt></p>
<p>The class to use for collecting stats, who must implement the
<a class="reference internal" href="index.html#topics-api-stats"><em>Stats Collector API</em></a>.</p>
</div>
<div class="section" id="stats-dump">
<span id="std:setting-STATS_DUMP"></span><h5>STATS_DUMP<a class="headerlink" href="#stats-dump" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Dump the <a class="reference internal" href="index.html#topics-stats"><em>Scrapy stats</em></a> (to the Scrapy log) once the spider
finishes.</p>
<p>For more info see: <a class="reference internal" href="index.html#topics-stats"><em>Stats Collection</em></a>.</p>
</div>
<div class="section" id="statsmailer-rcpts">
<span id="std:setting-STATSMAILER_RCPTS"></span><h5>STATSMAILER_RCPTS<a class="headerlink" href="#statsmailer-rcpts" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">[]</span></tt> (empty list)</p>
<p>Send Scrapy stats after spiders finish scraping. See
<tt class="xref py py-class docutils literal"><span class="pre">StatsMailer</span></tt> for more info.</p>
</div>
<div class="section" id="telnetconsole-enabled">
<span id="std:setting-TELNETCONSOLE_ENABLED"></span><h5>TELNETCONSOLE_ENABLED<a class="headerlink" href="#telnetconsole-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>A boolean which specifies if the <a class="reference internal" href="index.html#topics-telnetconsole"><em>telnet console</em></a>
will be enabled (provided its extension is also enabled).</p>
</div>
<div class="section" id="telnetconsole-port">
<span id="std:setting-TELNETCONSOLE_PORT"></span><h5>TELNETCONSOLE_PORT<a class="headerlink" href="#telnetconsole-port" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">[6023,</span> <span class="pre">6073]</span></tt></p>
<p>The port range to use for the telnet console. If set to <tt class="docutils literal"><span class="pre">None</span></tt> or <tt class="docutils literal"><span class="pre">0</span></tt>, a
dynamically assigned port is used. For more info see
<a class="reference internal" href="index.html#topics-telnetconsole"><em>Telnet Console</em></a>.</p>
</div>
<div class="section" id="templates-dir">
<span id="std:setting-TEMPLATES_DIR"></span><h5>TEMPLATES_DIR<a class="headerlink" href="#templates-dir" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">templates</span></tt> dir inside scrapy module</p>
<p>The directory where to look for templates when creating new projects with
<a class="reference internal" href="index.html#std:command-startproject"><tt class="xref std std-command docutils literal"><span class="pre">startproject</span></tt></a> command.</p>
</div>
<div class="section" id="urllength-limit">
<span id="std:setting-URLLENGTH_LIMIT"></span><h5>URLLENGTH_LIMIT<a class="headerlink" href="#urllength-limit" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">2083</span></tt></p>
<p>Scope: <tt class="docutils literal"><span class="pre">contrib.spidermiddleware.urllength</span></tt></p>
<p>The maximum URL length to allow for crawled URLs. For more information about
the default value for this setting see: <a class="reference external" href="http://www.boutell.com/newfaq/misc/urllength.html">http://www.boutell.com/newfaq/misc/urllength.html</a></p>
</div>
<div class="section" id="user-agent">
<span id="std:setting-USER_AGENT"></span><h5>USER_AGENT<a class="headerlink" href="#user-agent" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">&quot;Scrapy/VERSION</span> <span class="pre">(+http://scrapy.org)&quot;</span></tt></p>
<p>The default User-Agent to use when crawling, unless overridden.</p>
</div>
</div>
</div>
<span id="document-topics/signals"></span><div class="section" id="signals">
<span id="topics-signals"></span><h3>Signals<a class="headerlink" href="#signals" title="Permalink to this headline">¶</a></h3>
<p>Scrapy uses signals extensively to notify when certain events occur. You can
catch some of those signals in your Scrapy project (using an <a class="reference internal" href="index.html#topics-extensions"><em>extension</em></a>, for example) to perform additional tasks or extend Scrapy
to add functionality not provided out of the box.</p>
<p>Even though signals provide several arguments, the handlers that catch them
don&#8217;t need to accept all of them - the signal dispatching mechanism will only
deliver the arguments that the handler receives.</p>
<p>You can connect to signals (or send your own) through the
<a class="reference internal" href="index.html#topics-api-signals"><em>Signals API</em></a>.</p>
<div class="section" id="deferred-signal-handlers">
<h4>Deferred signal handlers<a class="headerlink" href="#deferred-signal-handlers" title="Permalink to this headline">¶</a></h4>
<p>Some signals support returning <a class="reference external" href="http://twistedmatrix.com/documents/current/core/howto/defer.html">Twisted deferreds</a> from their handlers, see
the <a class="reference internal" href="index.html#topics-signals-ref"><em>Built-in signals reference</em></a> below to know which ones.</p>
</div>
<div class="section" id="module-scrapy.signals">
<span id="built-in-signals-reference"></span><span id="topics-signals-ref"></span><h4>Built-in signals reference<a class="headerlink" href="#module-scrapy.signals" title="Permalink to this headline">¶</a></h4>
<p>Here&#8217;s the list of Scrapy built-in signals and their meaning.</p>
<div class="section" id="engine-started">
<h5>engine_started<a class="headerlink" href="#engine-started" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-engine_started"></span><dl class="function">
<dt id="scrapy.signals.engine_started">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">engine_started</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.signals.engine_started" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the Scrapy engine has started crawling.</p>
<p>This signal supports returning deferreds from their handlers.</p>
</dd></dl>

<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This signal may be fired <em>after</em> the <a class="reference internal" href="index.html#std:signal-spider_opened"><tt class="xref std std-signal docutils literal"><span class="pre">spider_opened</span></tt></a> signal,
depending on how the spider was started. So <strong>don&#8217;t</strong> rely on this signal
getting fired before <a class="reference internal" href="index.html#std:signal-spider_opened"><tt class="xref std std-signal docutils literal"><span class="pre">spider_opened</span></tt></a>.</p>
</div>
</div>
<div class="section" id="engine-stopped">
<h5>engine_stopped<a class="headerlink" href="#engine-stopped" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-engine_stopped"></span><dl class="function">
<dt id="scrapy.signals.engine_stopped">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">engine_stopped</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.signals.engine_stopped" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the Scrapy engine is stopped (for example, when a crawling
process has finished).</p>
<p>This signal supports returning deferreds from their handlers.</p>
</dd></dl>

</div>
<div class="section" id="item-scraped">
<h5>item_scraped<a class="headerlink" href="#item-scraped" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-item_scraped"></span><dl class="function">
<dt id="scrapy.signals.item_scraped">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">item_scraped</tt><big>(</big><em>item</em>, <em>response</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.signals.item_scraped" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when an item has been scraped, after it has passed all the
<a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a> stages (without being dropped).</p>
<p>This signal supports returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> object) &#8211; the item scraped</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider which scraped the item</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object) &#8211; the response from where the item was scraped</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="item-dropped">
<h5>item_dropped<a class="headerlink" href="#item-dropped" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-item_dropped"></span><dl class="function">
<dt id="scrapy.signals.item_dropped">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">item_dropped</tt><big>(</big><em>item</em>, <em>response</em>, <em>exception</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.signals.item_dropped" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent after an item has been dropped from the <a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a>
when some stage raised a <a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><tt class="xref py py-exc docutils literal"><span class="pre">DropItem</span></tt></a> exception.</p>
<p>This signal supports returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><tt class="xref py py-class docutils literal"><span class="pre">Item</span></tt></a> object) &#8211; the item dropped from the <a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a></li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider which scraped the item</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object) &#8211; the response from where the item was dropped</li>
<li><strong>exception</strong> (<a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><tt class="xref py py-exc docutils literal"><span class="pre">DropItem</span></tt></a> exception) &#8211; the exception (which must be a
<a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><tt class="xref py py-exc docutils literal"><span class="pre">DropItem</span></tt></a> subclass) which caused the item
to be dropped</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="spider-closed">
<h5>spider_closed<a class="headerlink" href="#spider-closed" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-spider_closed"></span><dl class="function">
<dt id="scrapy.signals.spider_closed">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">spider_closed</tt><big>(</big><em>spider</em>, <em>reason</em><big>)</big><a class="headerlink" href="#scrapy.signals.spider_closed" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent after a spider has been closed. This can be used to release per-spider
resources reserved on <a class="reference internal" href="index.html#std:signal-spider_opened"><tt class="xref std std-signal docutils literal"><span class="pre">spider_opened</span></tt></a>.</p>
<p>This signal supports returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider which has been closed</li>
<li><strong>reason</strong> (<em>str</em>) &#8211; a string which describes the reason why the spider was closed. If
it was closed because the spider has completed scraping, the reason
is <tt class="docutils literal"><span class="pre">'finished'</span></tt>. Otherwise, if the spider was manually closed by
calling the <tt class="docutils literal"><span class="pre">close_spider</span></tt> engine method, then the reason is the one
passed in the <tt class="docutils literal"><span class="pre">reason</span></tt> argument of that method (which defaults to
<tt class="docutils literal"><span class="pre">'cancelled'</span></tt>). If the engine was shutdown (for example, by hitting
Ctrl-C to stop it) the reason will be <tt class="docutils literal"><span class="pre">'shutdown'</span></tt>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="spider-opened">
<h5>spider_opened<a class="headerlink" href="#spider-opened" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-spider_opened"></span><dl class="function">
<dt id="scrapy.signals.spider_opened">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">spider_opened</tt><big>(</big><em>spider</em><big>)</big><a class="headerlink" href="#scrapy.signals.spider_opened" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent after a spider has been opened for crawling. This is typically used to
reserve per-spider resources, but can be used for any task that needs to be
performed when a spider is opened.</p>
<p>This signal supports returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider which has been opened</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="spider-idle">
<h5>spider_idle<a class="headerlink" href="#spider-idle" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-spider_idle"></span><dl class="function">
<dt id="scrapy.signals.spider_idle">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">spider_idle</tt><big>(</big><em>spider</em><big>)</big><a class="headerlink" href="#scrapy.signals.spider_idle" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when a spider has gone idle, which means the spider has no further:</p>
<blockquote>
<div><ul class="simple">
<li>requests waiting to be downloaded</li>
<li>requests scheduled</li>
<li>items being processed in the item pipeline</li>
</ul>
</div></blockquote>
<p>If the idle state persists after all handlers of this signal have finished,
the engine starts closing the spider. After the spider has finished
closing, the <a class="reference internal" href="index.html#std:signal-spider_closed"><tt class="xref std std-signal docutils literal"><span class="pre">spider_closed</span></tt></a> signal is sent.</p>
<p>You can, for example, schedule some requests in your <a class="reference internal" href="index.html#std:signal-spider_idle"><tt class="xref std std-signal docutils literal"><span class="pre">spider_idle</span></tt></a>
handler to prevent the spider from being closed.</p>
<p>This signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider which has gone idle</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="spider-error">
<h5>spider_error<a class="headerlink" href="#spider-error" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-spider_error"></span><dl class="function">
<dt id="scrapy.signals.spider_error">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">spider_error</tt><big>(</big><em>failure</em>, <em>response</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.signals.spider_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when a spider callback generates an error (ie. raises an exception).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>failure</strong> (<a class="reference external" href="http://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Failure</a> object) &#8211; the exception raised as a Twisted <a class="reference external" href="http://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Failure</a> object</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object) &#8211; the response being processed when the exception was raised</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider which raised the exception</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="request-scheduled">
<h5>request_scheduled<a class="headerlink" href="#request-scheduled" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-request_scheduled"></span><dl class="function">
<dt id="scrapy.signals.request_scheduled">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">request_scheduled</tt><big>(</big><em>request</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.signals.request_scheduled" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the engine schedules a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a>, to be
downloaded later.</p>
<p>The signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object) &#8211; the request that reached the scheduler</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider that yielded the request</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="response-received">
<h5>response_received<a class="headerlink" href="#response-received" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-response_received"></span><dl class="function">
<dt id="scrapy.signals.response_received">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">response_received</tt><big>(</big><em>response</em>, <em>request</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.signals.response_received" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the engine receives a new <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> from the
downloader.</p>
<p>This signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object) &#8211; the response received</li>
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object) &#8211; the request that generated the response</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider for which the response is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="response-downloaded">
<h5>response_downloaded<a class="headerlink" href="#response-downloaded" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-response_downloaded"></span><dl class="function">
<dt id="scrapy.signals.response_downloaded">
<tt class="descclassname">scrapy.signals.</tt><tt class="descname">response_downloaded</tt><big>(</big><em>response</em>, <em>request</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.signals.response_downloaded" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent by the downloader right after a <tt class="docutils literal"><span class="pre">HTTPResponse</span></tt> is downloaded.</p>
<p>This signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object) &#8211; the response downloaded</li>
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object) &#8211; the request that generated the response</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider for which the response is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/exceptions"></span><div class="section" id="module-scrapy.exceptions">
<span id="exceptions"></span><span id="topics-exceptions"></span><h3>Exceptions<a class="headerlink" href="#module-scrapy.exceptions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="built-in-exceptions-reference">
<span id="topics-exceptions-ref"></span><h4>Built-in Exceptions reference<a class="headerlink" href="#built-in-exceptions-reference" title="Permalink to this headline">¶</a></h4>
<p>Here&#8217;s a list of all exceptions included in Scrapy and their usage.</p>
<div class="section" id="dropitem">
<h5>DropItem<a class="headerlink" href="#dropitem" title="Permalink to this headline">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.DropItem">
<em class="property">exception </em><tt class="descclassname">scrapy.exceptions.</tt><tt class="descname">DropItem</tt><a class="headerlink" href="#scrapy.exceptions.DropItem" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>The exception that must be raised by item pipeline stages to stop processing an
Item. For more information see <a class="reference internal" href="index.html#topics-item-pipeline"><em>Item Pipeline</em></a>.</p>
</div>
<div class="section" id="closespider">
<h5>CloseSpider<a class="headerlink" href="#closespider" title="Permalink to this headline">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.CloseSpider">
<em class="property">exception </em><tt class="descclassname">scrapy.exceptions.</tt><tt class="descname">CloseSpider</tt><big>(</big><em>reason='cancelled'</em><big>)</big><a class="headerlink" href="#scrapy.exceptions.CloseSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>This exception can be raised from a spider callback to request the spider to be
closed/stopped. Supported arguments:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>reason</strong> (<em>str</em>) &#8211; the reason for closing</td>
</tr>
</tbody>
</table>
</dd></dl>

<p>For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">parse_page</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">if</span> <span class="s">&#39;Bandwidth exceeded&#39;</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">CloseSpider</span><span class="p">(</span><span class="s">&#39;bandwidth_exceeded&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="ignorerequest">
<h5>IgnoreRequest<a class="headerlink" href="#ignorerequest" title="Permalink to this headline">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.IgnoreRequest">
<em class="property">exception </em><tt class="descclassname">scrapy.exceptions.</tt><tt class="descname">IgnoreRequest</tt><a class="headerlink" href="#scrapy.exceptions.IgnoreRequest" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This exception can be raised by the Scheduler or any downloader middleware to
indicate that the request should be ignored.</p>
</div>
<div class="section" id="notconfigured">
<h5>NotConfigured<a class="headerlink" href="#notconfigured" title="Permalink to this headline">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.NotConfigured">
<em class="property">exception </em><tt class="descclassname">scrapy.exceptions.</tt><tt class="descname">NotConfigured</tt><a class="headerlink" href="#scrapy.exceptions.NotConfigured" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This exception can be raised by some components to indicate that they will
remain disabled. Those components include:</p>
<blockquote>
<div><ul class="simple">
<li>Extensions</li>
<li>Item pipelines</li>
<li>Downloader middlwares</li>
<li>Spider middlewares</li>
</ul>
</div></blockquote>
<p>The exception must be raised in the component constructor.</p>
</div>
<div class="section" id="notsupported">
<h5>NotSupported<a class="headerlink" href="#notsupported" title="Permalink to this headline">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.NotSupported">
<em class="property">exception </em><tt class="descclassname">scrapy.exceptions.</tt><tt class="descname">NotSupported</tt><a class="headerlink" href="#scrapy.exceptions.NotSupported" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This exception is raised to indicate an unsupported feature.</p>
</div>
</div>
</div>
<span id="document-topics/exporters"></span><div class="section" id="module-scrapy.contrib.exporter">
<span id="item-exporters"></span><span id="topics-exporters"></span><h3>Item Exporters<a class="headerlink" href="#module-scrapy.contrib.exporter" title="Permalink to this headline">¶</a></h3>
<p>Once you have scraped your Items, you often want to persist or export those
items, to use the data in some other application. That is, after all, the whole
purpose of the scraping process.</p>
<p>For this purpose Scrapy provides a collection of Item Exporters for different
output formats, such as XML, CSV or JSON.</p>
<div class="section" id="using-item-exporters">
<h4>Using Item Exporters<a class="headerlink" href="#using-item-exporters" title="Permalink to this headline">¶</a></h4>
<p>If you are in a hurry, and just want to use an Item Exporter to output scraped
data see the <a class="reference internal" href="index.html#topics-feed-exports"><em>Feed exports</em></a>. Otherwise, if you want to know how
Item Exporters work or need more custom functionality (not covered by the
default exports), continue reading below.</p>
<p>In order to use an Item Exporter, you  must instantiate it with its required
args. Each Item Exporter requires different arguments, so check each exporter
documentation to be sure, in <a class="reference internal" href="index.html#topics-exporters-reference"><em>Built-in Item Exporters reference</em></a>. After you have
instantiated your exporter, you have to:</p>
<p>1. call the method <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.start_exporting" title="scrapy.contrib.exporter.BaseItemExporter.start_exporting"><tt class="xref py py-meth docutils literal"><span class="pre">start_exporting()</span></tt></a> in order to
signal the beginning of the exporting process</p>
<p>2. call the <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.export_item" title="scrapy.contrib.exporter.BaseItemExporter.export_item"><tt class="xref py py-meth docutils literal"><span class="pre">export_item()</span></tt></a> method for each item you want
to export</p>
<p>3. and finally call the <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.finish_exporting" title="scrapy.contrib.exporter.BaseItemExporter.finish_exporting"><tt class="xref py py-meth docutils literal"><span class="pre">finish_exporting()</span></tt></a> to signal
the end of the exporting process</p>
<p>Here you can see an <a class="reference internal" href="index.html#document-topics/item-pipeline"><em>Item Pipeline</em></a> which uses an Item
Exporter to export scraped items to different files, one per spider:</p>
<div class="highlight-python"><div class="highlight"><pre>from scrapy import signals
from scrapy.contrib.exporter import XmlItemExporter

class XmlExportPipeline(object):

    def __init__(self):
        self.files = {}

     @classmethod
     def from_crawler(cls, crawler):
         pipeline = cls()
         crawler.signals.connect(pipeline.spider_opened, signals.spider_opened)
         crawler.signals.connect(pipeline.spider_closed, signals.spider_closed)
         return pipeline

    def spider_opened(self, spider):
        file = open(&#39;%s_products.xml&#39; % spider.name, &#39;w+b&#39;)
        self.files[spider] = file
        self.exporter = XmlItemExporter(file)
        self.exporter.start_exporting()

    def spider_closed(self, spider):
        self.exporter.finish_exporting()
        file = self.files.pop(spider)
        file.close()

    def process_item(self, item, spider):
        self.exporter.export_item(item)
        return item
</pre></div>
</div>
</div>
<div class="section" id="serialization-of-item-fields">
<span id="topics-exporters-field-serialization"></span><h4>Serialization of item fields<a class="headerlink" href="#serialization-of-item-fields" title="Permalink to this headline">¶</a></h4>
<p>By default, the field values are passed unmodified to the underlying
serialization library, and the decision of how to serialize them is delegated
to each particular serialization library.</p>
<p>However, you can customize how each field value is serialized <em>before it is
passed to the serialization library</em>.</p>
<p>There are two ways to customize how a field will be serialized, which are
described next.</p>
<div class="section" id="declaring-a-serializer-in-the-field">
<span id="topics-exporters-serializers"></span><h5>1. Declaring a serializer in the field<a class="headerlink" href="#declaring-a-serializer-in-the-field" title="Permalink to this headline">¶</a></h5>
<p>You can declare a serializer in the <a class="reference internal" href="index.html#topics-items-fields"><em>field metadata</em></a>. The serializer must be a callable which receives a
value and returns its serialized form.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">def</span> <span class="nf">serialize_price</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="k">return</span> <span class="s">&#39;$ </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Product</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">serializer</span><span class="o">=</span><span class="n">serialize_price</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="overriding-the-serialize-field-method">
<h5>2. Overriding the serialize_field() method<a class="headerlink" href="#overriding-the-serialize-field-method" title="Permalink to this headline">¶</a></h5>
<p>You can also override the <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.serialize_field" title="scrapy.contrib.exporter.BaseItemExporter.serialize_field"><tt class="xref py py-meth docutils literal"><span class="pre">serialize_field()</span></tt></a> method to
customize how your field value will be exported.</p>
<p>Make sure you call the base class <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.serialize_field" title="scrapy.contrib.exporter.BaseItemExporter.serialize_field"><tt class="xref py py-meth docutils literal"><span class="pre">serialize_field()</span></tt></a> method
after your custom code.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.exporter</span> <span class="kn">import</span> <span class="n">XmlItemExporter</span>

<span class="k">class</span> <span class="nc">ProductXmlExporter</span><span class="p">(</span><span class="n">XmlItemExporter</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">serialize_field</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">field</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">field</span> <span class="o">==</span> <span class="s">&#39;price&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="s">&#39;$ </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Product</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">serialize_field</span><span class="p">(</span><span class="n">field</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="built-in-item-exporters-reference">
<span id="topics-exporters-reference"></span><h4>Built-in Item Exporters reference<a class="headerlink" href="#built-in-item-exporters-reference" title="Permalink to this headline">¶</a></h4>
<p>Here is a list of the Item Exporters bundled with Scrapy. Some of them contain
output examples, which assume you&#8217;re exporting these two items:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">Item</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;Color TV&#39;</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="s">&#39;1200&#39;</span><span class="p">)</span>
<span class="n">Item</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;DVD player&#39;</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="s">&#39;200&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="baseitemexporter">
<h5>BaseItemExporter<a class="headerlink" href="#baseitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.exporter.BaseItemExporter">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.exporter.</tt><tt class="descname">BaseItemExporter</tt><big>(</big><em>fields_to_export=None</em>, <em>export_empty_fields=False</em>, <em>encoding='utf-8'</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.BaseItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the (abstract) base class for all Item Exporters. It provides
support for common features used by all (concrete) Item Exporters, such as
defining what fields to export, whether to export empty fields, or which
encoding to use.</p>
<p>These features can be configured through the constructor arguments which
populate their respective instance attributes: <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.fields_to_export" title="scrapy.contrib.exporter.BaseItemExporter.fields_to_export"><tt class="xref py py-attr docutils literal"><span class="pre">fields_to_export</span></tt></a>,
<a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.export_empty_fields" title="scrapy.contrib.exporter.BaseItemExporter.export_empty_fields"><tt class="xref py py-attr docutils literal"><span class="pre">export_empty_fields</span></tt></a>, <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.encoding" title="scrapy.contrib.exporter.BaseItemExporter.encoding"><tt class="xref py py-attr docutils literal"><span class="pre">encoding</span></tt></a>.</p>
<dl class="method">
<dt id="scrapy.contrib.exporter.BaseItemExporter.export_item">
<tt class="descname">export_item</tt><big>(</big><em>item</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.BaseItemExporter.export_item" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports the given item. This method must be implemented in subclasses.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.exporter.BaseItemExporter.serialize_field">
<tt class="descname">serialize_field</tt><big>(</big><em>field</em>, <em>name</em>, <em>value</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.BaseItemExporter.serialize_field" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the serialized value for the given field. You can override this
method (in your custom Item Exporters) if you want to control how a
particular field or value will be serialized/exported.</p>
<p>By default, this method looks for a serializer <a class="reference internal" href="index.html#topics-exporters-serializers"><em>declared in the item
field</em></a> and returns the result of applying
that serializer to the value. If no serializer is found, it returns the
value unchanged except for <tt class="docutils literal"><span class="pre">unicode</span></tt> values which are encoded to
<tt class="docutils literal"><span class="pre">str</span></tt> using the encoding declared in the <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.encoding" title="scrapy.contrib.exporter.BaseItemExporter.encoding"><tt class="xref py py-attr docutils literal"><span class="pre">encoding</span></tt></a> attribute.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>field</strong> (<a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><tt class="xref py py-class docutils literal"><span class="pre">Field</span></tt></a> object) &#8211; the field being serialized</li>
<li><strong>name</strong> (<em>str</em>) &#8211; the name of the field being serialized</li>
<li><strong>value</strong> &#8211; the value being serialized</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.exporter.BaseItemExporter.start_exporting">
<tt class="descname">start_exporting</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.BaseItemExporter.start_exporting" title="Permalink to this definition">¶</a></dt>
<dd><p>Signal the beginning of the exporting process. Some exporters may use
this to generate some required header (for example, the
<a class="reference internal" href="index.html#scrapy.contrib.exporter.XmlItemExporter" title="scrapy.contrib.exporter.XmlItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">XmlItemExporter</span></tt></a>). You must call this method before exporting any
items.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.exporter.BaseItemExporter.finish_exporting">
<tt class="descname">finish_exporting</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.BaseItemExporter.finish_exporting" title="Permalink to this definition">¶</a></dt>
<dd><p>Signal the end of the exporting process. Some exporters may use this to
generate some required footer (for example, the
<a class="reference internal" href="index.html#scrapy.contrib.exporter.XmlItemExporter" title="scrapy.contrib.exporter.XmlItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">XmlItemExporter</span></tt></a>). You must always call this method after you
have no more items to export.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.exporter.BaseItemExporter.fields_to_export">
<tt class="descname">fields_to_export</tt><a class="headerlink" href="#scrapy.contrib.exporter.BaseItemExporter.fields_to_export" title="Permalink to this definition">¶</a></dt>
<dd><p>A list with the name of the fields that will be exported, or None if you
want to export all fields. Defaults to None.</p>
<p>Some exporters (like <a class="reference internal" href="index.html#scrapy.contrib.exporter.CsvItemExporter" title="scrapy.contrib.exporter.CsvItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">CsvItemExporter</span></tt></a>) respect the order of the
fields defined in this attribute.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.exporter.BaseItemExporter.export_empty_fields">
<tt class="descname">export_empty_fields</tt><a class="headerlink" href="#scrapy.contrib.exporter.BaseItemExporter.export_empty_fields" title="Permalink to this definition">¶</a></dt>
<dd><p>Whether to include empty/unpopulated item fields in the exported data.
Defaults to <tt class="docutils literal"><span class="pre">False</span></tt>. Some exporters (like <a class="reference internal" href="index.html#scrapy.contrib.exporter.CsvItemExporter" title="scrapy.contrib.exporter.CsvItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">CsvItemExporter</span></tt></a>)
ignore this attribute and always export all empty fields.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.contrib.exporter.BaseItemExporter.encoding">
<tt class="descname">encoding</tt><a class="headerlink" href="#scrapy.contrib.exporter.BaseItemExporter.encoding" title="Permalink to this definition">¶</a></dt>
<dd><p>The encoding that will be used to encode unicode values. This only
affects unicode values (which are always serialized to str using this
encoding). Other value types are passed unchanged to the specific
serialization library.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="xmlitemexporter">
<h5>XmlItemExporter<a class="headerlink" href="#xmlitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.exporter.XmlItemExporter">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.exporter.</tt><tt class="descname">XmlItemExporter</tt><big>(</big><em>file</em>, <em>item_element='item'</em>, <em>root_element='items'</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.XmlItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in XML format to the specified file object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>file</strong> &#8211; the file-like object to use for exporting the data.</li>
<li><strong>root_element</strong> (<em>str</em>) &#8211; The name of root element in the exported XML.</li>
<li><strong>item_element</strong> (<em>str</em>) &#8211; The name of each item element in the exported XML.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The additional keyword arguments of this constructor are passed to the
<a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter" title="scrapy.contrib.exporter.BaseItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></tt></a> constructor.</p>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none"><div class="highlight"><pre>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;items&gt;
  &lt;item&gt;
    &lt;name&gt;Color TV&lt;/name&gt;
    &lt;price&gt;1200&lt;/price&gt;
 &lt;/item&gt;
  &lt;item&gt;
    &lt;name&gt;DVD player&lt;/name&gt;
    &lt;price&gt;200&lt;/price&gt;
 &lt;/item&gt;
&lt;/items&gt;
</pre></div>
</div>
<p>Unless overridden in the <tt class="xref py py-meth docutils literal"><span class="pre">serialize_field()</span></tt> method, multi-valued fields are
exported by serializing each value inside a <tt class="docutils literal"><span class="pre">&lt;value&gt;</span></tt> element. This is for
convenience, as multi-valued fields are very common.</p>
<p>For example, the item:</p>
<div class="highlight-none"><div class="highlight"><pre>Item(name=[&#39;John&#39;, &#39;Doe&#39;], age=&#39;23&#39;)
</pre></div>
</div>
<p>Would be serialized as:</p>
<div class="highlight-none"><div class="highlight"><pre>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;items&gt;
  &lt;item&gt;
    &lt;name&gt;
      &lt;value&gt;John&lt;/value&gt;
      &lt;value&gt;Doe&lt;/value&gt;
    &lt;/name&gt;
    &lt;age&gt;23&lt;/age&gt;
  &lt;/item&gt;
&lt;/items&gt;
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="csvitemexporter">
<h5>CsvItemExporter<a class="headerlink" href="#csvitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.exporter.CsvItemExporter">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.exporter.</tt><tt class="descname">CsvItemExporter</tt><big>(</big><em>file</em>, <em>include_headers_line=True</em>, <em>join_multivalued='</em>, <em>'</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.CsvItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in CSV format to the given file-like object. If the
<tt class="xref py py-attr docutils literal"><span class="pre">fields_to_export</span></tt> attribute is set, it will be used to define the
CSV columns and their order. The <tt class="xref py py-attr docutils literal"><span class="pre">export_empty_fields</span></tt> attribute has
no effect on this exporter.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>file</strong> &#8211; the file-like object to use for exporting the data.</li>
<li><strong>include_headers_line</strong> (<em>str</em>) &#8211; If enabled, makes the exporter output a header
line with the field names taken from
<a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter.fields_to_export" title="scrapy.contrib.exporter.BaseItemExporter.fields_to_export"><tt class="xref py py-attr docutils literal"><span class="pre">BaseItemExporter.fields_to_export</span></tt></a> or the first exported item fields.</li>
<li><strong>join_multivalued</strong> &#8211; The char (or chars) that will be used for joining
multi-valued fields, if found.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The additional keyword arguments of this constructor are passed to the
<a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter" title="scrapy.contrib.exporter.BaseItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></tt></a> constructor, and the leftover arguments to the
<a class="reference external" href="http://docs.python.org/library/csv.html#csv.writer">csv.writer</a> constructor, so you can use any <cite>csv.writer</cite> constructor
argument to customize this exporter.</p>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none"><div class="highlight"><pre>product,price
Color TV,1200
DVD player,200
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pickleitemexporter">
<h5>PickleItemExporter<a class="headerlink" href="#pickleitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.exporter.PickleItemExporter">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.exporter.</tt><tt class="descname">PickleItemExporter</tt><big>(</big><em>file</em>, <em>protocol=0</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.PickleItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in pickle format to the given file-like object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>file</strong> &#8211; the file-like object to use for exporting the data.</li>
<li><strong>protocol</strong> (<em>int</em>) &#8211; The pickle protocol to use.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>For more information, refer to the <a class="reference external" href="http://docs.python.org/library/pickle.html">pickle module documentation</a>.</p>
<p>The additional keyword arguments of this constructor are passed to the
<a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter" title="scrapy.contrib.exporter.BaseItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></tt></a> constructor.</p>
<p>Pickle isn&#8217;t a human readable format, so no output examples are provided.</p>
</dd></dl>

</div>
<div class="section" id="pprintitemexporter">
<h5>PprintItemExporter<a class="headerlink" href="#pprintitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.exporter.PprintItemExporter">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.exporter.</tt><tt class="descname">PprintItemExporter</tt><big>(</big><em>file</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.PprintItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in pretty print format to the specified file object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>file</strong> &#8211; the file-like object to use for exporting the data.</td>
</tr>
</tbody>
</table>
<p>The additional keyword arguments of this constructor are passed to the
<a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter" title="scrapy.contrib.exporter.BaseItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></tt></a> constructor.</p>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none"><div class="highlight"><pre>{&#39;name&#39;: &#39;Color TV&#39;, &#39;price&#39;: &#39;1200&#39;}
{&#39;name&#39;: &#39;DVD player&#39;, &#39;price&#39;: &#39;200&#39;}
</pre></div>
</div>
<p>Longer lines (when present) are pretty-formatted.</p>
</dd></dl>

</div>
<div class="section" id="jsonitemexporter">
<h5>JsonItemExporter<a class="headerlink" href="#jsonitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.exporter.JsonItemExporter">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.exporter.</tt><tt class="descname">JsonItemExporter</tt><big>(</big><em>file</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.JsonItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in JSON format to the specified file-like object, writing all
objects as a list of objects. The additional constructor arguments are
passed to the <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter" title="scrapy.contrib.exporter.BaseItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></tt></a> constructor, and the leftover
arguments to the <a class="reference external" href="http://docs.python.org/library/json.html#json.JSONEncoder">JSONEncoder</a> constructor, so you can use any
<a class="reference external" href="http://docs.python.org/library/json.html#json.JSONEncoder">JSONEncoder</a> constructor argument to customize this exporter.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>file</strong> &#8211; the file-like object to use for exporting the data.</td>
</tr>
</tbody>
</table>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none"><div class="highlight"><pre>[{&quot;name&quot;: &quot;Color TV&quot;, &quot;price&quot;: &quot;1200&quot;},
{&quot;name&quot;: &quot;DVD player&quot;, &quot;price&quot;: &quot;200&quot;}]
</pre></div>
</div>
<div class="admonition warning" id="json-with-large-data">
<p class="first admonition-title">Warning</p>
<p class="last">JSON is very simple and flexible serialization format, but it
doesn&#8217;t scale well for large amounts of data since incremental (aka.
stream-mode) parsing is not well supported (if at all) among JSON parsers
(on any language), and most of them just parse the entire object in
memory. If you want the power and simplicity of JSON with a more
stream-friendly format, consider using <a class="reference internal" href="index.html#scrapy.contrib.exporter.JsonLinesItemExporter" title="scrapy.contrib.exporter.JsonLinesItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">JsonLinesItemExporter</span></tt></a>
instead, or splitting the output in multiple chunks.</p>
</div>
</dd></dl>

</div>
<div class="section" id="jsonlinesitemexporter">
<h5>JsonLinesItemExporter<a class="headerlink" href="#jsonlinesitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.contrib.exporter.JsonLinesItemExporter">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.exporter.</tt><tt class="descname">JsonLinesItemExporter</tt><big>(</big><em>file</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#scrapy.contrib.exporter.JsonLinesItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in JSON format to the specified file-like object, writing one
JSON-encoded item per line. The additional constructor arguments are passed
to the <a class="reference internal" href="index.html#scrapy.contrib.exporter.BaseItemExporter" title="scrapy.contrib.exporter.BaseItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></tt></a> constructor, and the leftover arguments to
the <a class="reference external" href="http://docs.python.org/library/json.html#json.JSONEncoder">JSONEncoder</a> constructor, so you can use any <a class="reference external" href="http://docs.python.org/library/json.html#json.JSONEncoder">JSONEncoder</a>
constructor argument to customize this exporter.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>file</strong> &#8211; the file-like object to use for exporting the data.</td>
</tr>
</tbody>
</table>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none"><div class="highlight"><pre>{&quot;name&quot;: &quot;Color TV&quot;, &quot;price&quot;: &quot;1200&quot;}
{&quot;name&quot;: &quot;DVD player&quot;, &quot;price&quot;: &quot;200&quot;}
</pre></div>
</div>
<p>Unlike the one produced by <a class="reference internal" href="index.html#scrapy.contrib.exporter.JsonItemExporter" title="scrapy.contrib.exporter.JsonItemExporter"><tt class="xref py py-class docutils literal"><span class="pre">JsonItemExporter</span></tt></a>, the format produced by
this exporter is well suited for serializing large amounts of data.</p>
</dd></dl>

</div>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-topics/commands"><em>Command line tool</em></a></dt>
<dd>Learn about the command-line tool and see all <a class="reference internal" href="index.html#topics-commands-ref"><em>available commands</em></a>.</dd>
<dt><a class="reference internal" href="index.html#document-topics/request-response"><em>Requests and Responses</em></a></dt>
<dd>Understand the classes used to represent HTTP requests and responses.</dd>
<dt><a class="reference internal" href="index.html#document-topics/settings"><em>Settings</em></a></dt>
<dd>Learn how to configure Scrapy and see all <a class="reference internal" href="index.html#topics-settings-ref"><em>available settings</em></a>.</dd>
<dt><a class="reference internal" href="index.html#document-topics/signals"><em>Signals</em></a></dt>
<dd>See all available signals and how to work with them.</dd>
<dt><a class="reference internal" href="index.html#document-topics/exceptions"><em>Exceptions</em></a></dt>
<dd>See all available exceptions and their meaning.</dd>
<dt><a class="reference internal" href="index.html#document-topics/exporters"><em>Item Exporters</em></a></dt>
<dd>Quickly export your scraped items to a file (XML, CSV, etc).</dd>
</dl>
</div>
<div class="section" id="all-the-rest">
<h2>All the rest<a class="headerlink" href="#all-the-rest" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-news"></span><div class="section" id="release-notes">
<span id="news"></span><h3>Release notes<a class="headerlink" href="#release-notes" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id1">
<h4>0.24.0 (2014-06-26)<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<div class="section" id="enhancements">
<h5>Enhancements<a class="headerlink" href="#enhancements" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Improve Scrapy top-level namespace (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/494">issue 494</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/684">issue 684</a>)</li>
<li>Add selector shortcuts to responses (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/554">issue 554</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/690">issue 690</a>)</li>
<li>Add new lxml based LinkExtractor to replace unmantained SgmlLinkExtractor
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/559">issue 559</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/761">issue 761</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/763">issue 763</a>)</li>
<li>Cleanup settings API - part of per-spider settings <strong>GSoC project</strong> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/737">issue 737</a>)</li>
<li>Add UTF8 encoding header to templates (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/688">issue 688</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/762">issue 762</a>)</li>
<li>Telnet console now binds to 127.0.0.1 by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/699">issue 699</a>)</li>
<li>Update debian/ubuntu install instructions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/509">issue 509</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/549">issue 549</a>)</li>
<li>Disable smart strings in lxml XPath evaluations (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/535">issue 535</a>)</li>
<li>Restore filesystem based cache as default for http
cache middleware (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/541">issue 541</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/500">issue 500</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/571">issue 571</a>)</li>
<li>Expose current crawler in Scrapy shell (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/557">issue 557</a>)</li>
<li>Improve testsuite comparing CSV and XML exporters (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/570">issue 570</a>)</li>
<li>New <cite>offsite/filtered</cite> and <cite>offsite/domains</cite> stats (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/566">issue 566</a>)</li>
<li>Support process_links as generator in CrawlSpider (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/555">issue 555</a>)</li>
<li>Verbose logging and new stats counters for DupeFilter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/553">issue 553</a>)</li>
<li>Add a mimetype parameter to <cite>MailSender.send()</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/602">issue 602</a>)</li>
<li>Generalize file pipeline log messages (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/622">issue 622</a>)</li>
<li>Replace unencodeable codepoints with html entities in SGMLLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/565">issue 565</a>)</li>
<li>Converted SEP documents to rst format (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/629">issue 629</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/630">issue 630</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/638">issue 638</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/632">issue 632</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/636">issue 636</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/640">issue 640</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/635">issue 635</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/634">issue 634</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/639">issue 639</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/637">issue 637</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/631">issue 631</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/633">issue 633</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/641">issue 641</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/642">issue 642</a>)</li>
<li>Tests and docs for clickdata&#8217;s nr index in FormRequest (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/646">issue 646</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/645">issue 645</a>)</li>
<li>Allow to disable a downloader handler just like any other component (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/650">issue 650</a>)</li>
<li>Log when a request is discarded after too many redirections (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/654">issue 654</a>)</li>
<li>Log error responses if they are not handled by spider callbacks
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/612">issue 612</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/656">issue 656</a>)</li>
<li>Add content-type check to http compression mw (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/193">issue 193</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/660">issue 660</a>)</li>
<li>Run pypy tests using latest pypi from ppa (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/674">issue 674</a>)</li>
<li>Run test suite using pytest instead of trial (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/679">issue 679</a>)</li>
<li>Build docs and check for dead links in tox environment (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/687">issue 687</a>)</li>
<li>Make scrapy.version_info a tuple of integers (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/681">issue 681</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/692">issue 692</a>)</li>
<li>Infer exporter&#8217;s output format from filename extensions
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/546">issue 546</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/659">issue 659</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/760">issue 760</a>)</li>
<li>Support case-insensitive domains in <cite>url_is_from_any_domain()</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/693">issue 693</a>)</li>
<li>Remove pep8 warnings in project and spider templates (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/698">issue 698</a>)</li>
<li>Tests and docs for <cite>request_fingerprint</cite> function (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/597">issue 597</a>)</li>
<li>Update SEP-19 for GSoC project <cite>per-spider settings</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/705">issue 705</a>)</li>
<li>Set exit code to non-zero when contracts fails (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/727">issue 727</a>)</li>
<li>Add a setting to control what class is instanciated as Downloader component
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/738">issue 738</a>)</li>
<li>Pass response in <cite>item_dropped</cite> signal (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/724">issue 724</a>)</li>
<li>Improve <cite>scrapy check</cite> contracts command (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/733">issue 733</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/752">issue 752</a>)</li>
<li>Document <cite>spider.closed()</cite> shortcut (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/719">issue 719</a>)</li>
<li>Document <cite>request_scheduled</cite> signal (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/746">issue 746</a>)</li>
<li>Add a note about reporting security issues (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/697">issue 697</a>)</li>
<li>Add LevelDB http cache storage backend (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/626">issue 626</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/500">issue 500</a>)</li>
<li>Sort spider list output of <cite>scrapy list</cite> command (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/742">issue 742</a>)</li>
<li>Multiple documentation enhancemens and fixes
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/575">issue 575</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/587">issue 587</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/590">issue 590</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/596">issue 596</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/610">issue 610</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/617">issue 617</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/618">issue 618</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/627">issue 627</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/613">issue 613</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/643">issue 643</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/654">issue 654</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/675">issue 675</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/663">issue 663</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/711">issue 711</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/714">issue 714</a>)</li>
</ul>
</div>
<div class="section" id="bugfixes">
<h5>Bugfixes<a class="headerlink" href="#bugfixes" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Encode unicode URL value when creating Links in RegexLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/561">issue 561</a>)</li>
<li>Ignore None values in ItemLoader processors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/556">issue 556</a>)</li>
<li>Fix link text when there is an inner tag in SGMLLinkExtractor and
HtmlParserLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/485">issue 485</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/574">issue 574</a>)</li>
<li>Fix wrong checks on subclassing of deprecated classes
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/581">issue 581</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/584">issue 584</a>)</li>
<li>Handle errors caused by inspect.stack() failures (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/582">issue 582</a>)</li>
<li>Fix a reference to unexistent engine attribute (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/593">issue 593</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/594">issue 594</a>)</li>
<li>Fix dynamic itemclass example usage of type() (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/603">issue 603</a>)</li>
<li>Use lucasdemarchi/codespell to fix typos (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/628">issue 628</a>)</li>
<li>Fix default value of attrs argument in SgmlLinkExtractor to be tuple (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/661">issue 661</a>)</li>
<li>Fix XXE flaw in sitemap reader (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/676">issue 676</a>)</li>
<li>Fix engine to support filtered start requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/707">issue 707</a>)</li>
<li>Fix offsite middleware case on urls with no hostnames (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/745">issue 745</a>)</li>
<li>Testsuite doesn&#8217;t require PIL anymore (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/585">issue 585</a>)</li>
</ul>
</div>
</div>
<div class="section" id="released-2014-02-14">
<h4>0.22.2 (released 2014-02-14)<a class="headerlink" href="#released-2014-02-14" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>fix a reference to unexistent engine.slots. closes #593 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/13c099a">commit 13c099a</a>)</li>
<li>downloaderMW doc typo (spiderMW doc copy remnant) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8ae11bf">commit 8ae11bf</a>)</li>
<li>Correct typos (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1346037">commit 1346037</a>)</li>
</ul>
</div>
<div class="section" id="released-2014-02-08">
<h4>0.22.1 (released 2014-02-08)<a class="headerlink" href="#released-2014-02-08" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>localhost666 can resolve under certain circumstances (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2ec2279">commit 2ec2279</a>)</li>
<li>test inspect.stack failure (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cc3eda3">commit cc3eda3</a>)</li>
<li>Handle cases when inspect.stack() fails (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8cb44f9">commit 8cb44f9</a>)</li>
<li>Fix wrong checks on subclassing of deprecated classes. closes #581 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/46d98d6">commit 46d98d6</a>)</li>
<li>Docs: 4-space indent for final spider example (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/13846de">commit 13846de</a>)</li>
<li>Fix HtmlParserLinkExtractor and tests after #485 merge (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/368a946">commit 368a946</a>)</li>
<li>BaseSgmlLinkExtractor: Fixed the missing space when the link has an inner tag (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b566388">commit b566388</a>)</li>
<li>BaseSgmlLinkExtractor: Added unit test of a link with an inner tag (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c1cb418">commit c1cb418</a>)</li>
<li>BaseSgmlLinkExtractor: Fixed unknown_endtag() so that it only set current_link=None when the end tag match the opening tag (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7e4d627">commit 7e4d627</a>)</li>
<li>Fix tests for Travis-CI build (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/76c7e20">commit 76c7e20</a>)</li>
<li>replace unencodeable codepoints with html entities. fixes #562 and #285 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5f87b17">commit 5f87b17</a>)</li>
<li>RegexLinkExtractor: encode URL unicode value when creating Links (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d0ee545">commit d0ee545</a>)</li>
<li>Updated the tutorial crawl output with latest output. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8da65de">commit 8da65de</a>)</li>
<li>Updated shell docs with the crawler reference and fixed the actual shell output. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/875b9ab">commit 875b9ab</a>)</li>
<li>PEP8 minor edits. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f89efaf">commit f89efaf</a>)</li>
<li>Expose current crawler in the scrapy shell. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5349cec">commit 5349cec</a>)</li>
<li>Unused re import and PEP8 minor edits. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/387f414">commit 387f414</a>)</li>
<li>Ignore None&#8217;s values when using the ItemLoader. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0632546">commit 0632546</a>)</li>
<li>DOC Fixed HTTPCACHE_STORAGE typo in the default value which is now Filesystem instead Dbm. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cde9a8c">commit cde9a8c</a>)</li>
<li>show ubuntu setup instructions as literal code (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fb5c9c5">commit fb5c9c5</a>)</li>
<li>Update Ubuntu installation instructions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/70fb105">commit 70fb105</a>)</li>
<li>Merge pull request #550 from stray-leone/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6f70b6a">commit 6f70b6a</a>)</li>
<li>modify the version of scrapy ubuntu package (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/725900d">commit 725900d</a>)</li>
<li>fix 0.22.0 release date (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/af0219a">commit af0219a</a>)</li>
<li>fix typos in news.rst and remove (not released yet) header (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b7f58f4">commit b7f58f4</a>)</li>
</ul>
</div>
<div class="section" id="released-2014-01-17">
<h4>0.22.0 (released 2014-01-17)<a class="headerlink" href="#released-2014-01-17" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id2">
<h5>Enhancements<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>[<strong>Backwards incompatible</strong>] Switched HTTPCacheMiddleware backend to filesystem (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/541">issue 541</a>)
To restore old backend set <cite>HTTPCACHE_STORAGE</cite> to <cite>scrapy.contrib.httpcache.DbmCacheStorage</cite></li>
<li>Proxy https:// urls using CONNECT method (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/392">issue 392</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/397">issue 397</a>)</li>
<li>Add a middleware to crawl ajax crawleable pages as defined by google (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/343">issue 343</a>)</li>
<li>Rename scrapy.spider.BaseSpider to scrapy.spider.Spider (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/510">issue 510</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/519">issue 519</a>)</li>
<li>Selectors register EXSLT namespaces by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/472">issue 472</a>)</li>
<li>Unify item loaders similar to selectors renaming (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/461">issue 461</a>)</li>
<li>Make <cite>RFPDupeFilter</cite> class easily subclassable (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/533">issue 533</a>)</li>
<li>Improve test coverage and forthcoming Python 3 support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/525">issue 525</a>)</li>
<li>Promote startup info on settings and middleware to INFO level (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/520">issue 520</a>)</li>
<li>Support partials in <cite>get_func_args</cite> util (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/506">issue 506</a>, issue:<cite>504</cite>)</li>
<li>Allow running indiviual tests via tox (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/503">issue 503</a>)</li>
<li>Update extensions ignored by link extractors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/498">issue 498</a>)</li>
<li>Add middleware methods to get files/images/thumbs paths (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/490">issue 490</a>)</li>
<li>Improve offsite middleware tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/478">issue 478</a>)</li>
<li>Add a way to skip default Referer header set by RefererMiddleware (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/475">issue 475</a>)</li>
<li>Do not send <cite>x-gzip</cite> in default <cite>Accept-Encoding</cite> header (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/469">issue 469</a>)</li>
<li>Support defining http error handling using settings (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/466">issue 466</a>)</li>
<li>Use modern python idioms wherever you find legacies (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/497">issue 497</a>)</li>
<li>Improve and correct documentation
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/527">issue 527</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/524">issue 524</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/521">issue 521</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/517">issue 517</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/512">issue 512</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/505">issue 505</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/502">issue 502</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/489">issue 489</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/465">issue 465</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/460">issue 460</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/425">issue 425</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/536">issue 536</a>)</li>
</ul>
</div>
<div class="section" id="fixes">
<h5>Fixes<a class="headerlink" href="#fixes" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Update Selector class imports in CrawlSpider template (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/484">issue 484</a>)</li>
<li>Fix unexistent reference to <cite>engine.slots</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/464">issue 464</a>)</li>
<li>Do not try to call <cite>body_as_unicode()</cite> on a non-TextResponse instance (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/462">issue 462</a>)</li>
<li>Warn when subclassing XPathItemLoader, previously it only warned on
instantiation. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/523">issue 523</a>)</li>
<li>Warn when subclassing XPathSelector, previously it only warned on
instantiation. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/537">issue 537</a>)</li>
<li>Multiple fixes to memory stats (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/531">issue 531</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/530">issue 530</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/529">issue 529</a>)</li>
<li>Fix overriding url in <cite>FormRequest.from_response()</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/507">issue 507</a>)</li>
<li>Fix tests runner under pip 1.5 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/513">issue 513</a>)</li>
<li>Fix logging error when spider name is unicode (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/479">issue 479</a>)</li>
</ul>
</div>
</div>
<div class="section" id="released-2013-12-09">
<h4>0.20.2 (released 2013-12-09)<a class="headerlink" href="#released-2013-12-09" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Update CrawlSpider Template with Selector changes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6d1457d">commit 6d1457d</a>)</li>
<li>fix method name in tutorial. closes GH-480 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b4fc359">commit b4fc359</a></li>
</ul>
</div>
<div class="section" id="released-2013-11-28">
<h4>0.20.1 (released 2013-11-28)<a class="headerlink" href="#released-2013-11-28" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>include_package_data is required to build wheels from published sources (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5ba1ad5">commit 5ba1ad5</a>)</li>
<li>process_parallel was leaking the failures on its internal deferreds.  closes #458 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/419a780">commit 419a780</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-11-08">
<h4>0.20.0 (released 2013-11-08)<a class="headerlink" href="#released-2013-11-08" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id3">
<h5>Enhancements<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>New Selector&#8217;s API including CSS selectors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/395">issue 395</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/426">issue 426</a>),</li>
<li>Request/Response url/body attributes are now immutable
(modifying them had been deprecated for a long time)</li>
<li><a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><tt class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></tt></a> is now defined as a dict (instead of a list)</li>
<li>Sitemap spider can fetch alternate URLs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/360">issue 360</a>)</li>
<li><cite>Selector.remove_namespaces()</cite> now remove namespaces from element&#8217;s attributes. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/416">issue 416</a>)</li>
<li>Paved the road for Python 3.3+ (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/435">issue 435</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/436">issue 436</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/431">issue 431</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/452">issue 452</a>)</li>
<li>New item exporter using native python types with nesting support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/366">issue 366</a>)</li>
<li>Tune HTTP1.1 pool size so it matches concurrency defined by settings (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b43b5f575">commit b43b5f575</a>)</li>
<li>scrapy.mail.MailSender now can connect over TLS or upgrade using STARTTLS (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/327">issue 327</a>)</li>
<li>New FilesPipeline with functionality factored out from ImagesPipeline (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/370">issue 370</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/409">issue 409</a>)</li>
<li>Recommend Pillow instead of PIL for image handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/317">issue 317</a>)</li>
<li>Added debian packages for Ubuntu quantal and raring (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/86230c0">commit 86230c0</a>)</li>
<li>Mock server (used for tests) can listen for HTTPS requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/410">issue 410</a>)</li>
<li>Remove multi spider support from multiple core components
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/422">issue 422</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/421">issue 421</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/420">issue 420</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/419">issue 419</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/423">issue 423</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/418">issue 418</a>)</li>
<li>Travis-CI now tests Scrapy changes against development versions of <cite>w3lib</cite> and <cite>queuelib</cite> python packages.</li>
<li>Add pypy 2.1 to continuous integration tests (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ecfa7431">commit ecfa7431</a>)</li>
<li>Pylinted, pep8 and removed old-style exceptions from source (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/430">issue 430</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/432">issue 432</a>)</li>
<li>Use importlib for parametric imports (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/445">issue 445</a>)</li>
<li>Handle a regression introduced in Python 2.7.5 that affects XmlItemExporter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/372">issue 372</a>)</li>
<li>Bugfix crawling shutdown on SIGINT (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/450">issue 450</a>)</li>
<li>Do not submit <cite>reset</cite> type inputs in FormRequest.from_response (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b326b87">commit b326b87</a>)</li>
<li>Do not silence download errors when request errback raises an exception (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/684cfc0">commit 684cfc0</a>)</li>
</ul>
</div>
<div class="section" id="id4">
<h5>Bugfixes<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Fix tests under Django 1.6 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b6bed44c">commit b6bed44c</a>)</li>
<li>Lot of bugfixes to retry middleware under disconnections using HTTP 1.1 download handler</li>
<li>Fix inconsistencies among Twisted releases (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/406">issue 406</a>)</li>
<li>Fix scrapy shell bugs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/418">issue 418</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/407">issue 407</a>)</li>
<li>Fix invalid variable name in setup.py (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/429">issue 429</a>)</li>
<li>Fix tutorial references (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/387">issue 387</a>)</li>
<li>Improve request-response docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/391">issue 391</a>)</li>
<li>Improve best practices docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/399">issue 399</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/400">issue 400</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/401">issue 401</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/402">issue 402</a>)</li>
<li>Improve django integration docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/404">issue 404</a>)</li>
<li>Document <cite>bindaddress</cite> request meta (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/37c24e01d7">commit 37c24e01d7</a>)</li>
<li>Improve <cite>Request</cite> class documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/226">issue 226</a>)</li>
</ul>
</div>
<div class="section" id="other">
<h5>Other<a class="headerlink" href="#other" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Dropped Python 2.6 support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/448">issue 448</a>)</li>
<li>Add <a class="reference external" href="https://github.com/SimonSapin/cssselect">cssselect</a> python package as install dependency</li>
<li>Drop libxml2 and multi selector&#8217;s backend support, <a class="reference external" href="http://lxml.de/">lxml</a> is required from now on.</li>
<li>Minimum Twisted version increased to 10.0.0, dropped Twisted 8.0 support.</li>
<li>Running test suite now requires <cite>mock</cite> python library (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/390">issue 390</a>)</li>
</ul>
</div>
<div class="section" id="thanks">
<h5>Thanks<a class="headerlink" href="#thanks" title="Permalink to this headline">¶</a></h5>
<p>Thanks to everyone who contribute to this release!</p>
<p>List of contributors sorted by number of commits:</p>
<div class="highlight-none"><div class="highlight"><pre>69 Daniel Graña &lt;dangra@...&gt;
37 Pablo Hoffman &lt;pablo@...&gt;
13 Mikhail Korobov &lt;kmike84@...&gt;
 9 Alex Cepoi &lt;alex.cepoi@...&gt;
 9 alexanderlukanin13 &lt;alexander.lukanin.13@...&gt;
 8 Rolando Espinoza La fuente &lt;darkrho@...&gt;
 8 Lukasz Biedrycki &lt;lukasz.biedrycki@...&gt;
 6 Nicolas Ramirez &lt;nramirez.uy@...&gt;
 3 Paul Tremberth &lt;paul.tremberth@...&gt;
 2 Martin Olveyra &lt;molveyra@...&gt;
 2 Stefan &lt;misc@...&gt;
 2 Rolando Espinoza &lt;darkrho@...&gt;
 2 Loren Davie &lt;loren@...&gt;
 2 irgmedeiros &lt;irgmedeiros@...&gt;
 1 Stefan Koch &lt;taikano@...&gt;
 1 Stefan &lt;cct@...&gt;
 1 scraperdragon &lt;dragon@...&gt;
 1 Kumara Tharmalingam &lt;ktharmal@...&gt;
 1 Francesco Piccinno &lt;stack.box@...&gt;
 1 Marcos Campal &lt;duendex@...&gt;
 1 Dragon Dave &lt;dragon@...&gt;
 1 Capi Etheriel &lt;barraponto@...&gt;
 1 cacovsky &lt;amarquesferraz@...&gt;
 1 Berend Iwema &lt;berend@...&gt;
</pre></div>
</div>
</div>
</div>
<div class="section" id="released-2013-10-10">
<h4>0.18.4 (released 2013-10-10)<a class="headerlink" href="#released-2013-10-10" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>IPython refuses to update the namespace. fix #396 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3d32c4f">commit 3d32c4f</a>)</li>
<li>Fix AlreadyCalledError replacing a request in shell command. closes #407 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b1d8919">commit b1d8919</a>)</li>
<li>Fix start_requests laziness and early hangs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/89faf52">commit 89faf52</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-10-03">
<h4>0.18.3 (released 2013-10-03)<a class="headerlink" href="#released-2013-10-03" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>fix regression on lazy evaluation of start requests (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/12693a5">commit 12693a5</a>)</li>
<li>forms: do not submit reset inputs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e429f63">commit e429f63</a>)</li>
<li>increase unittest timeouts to decrease travis false positive failures (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/912202e">commit 912202e</a>)</li>
<li>backport master fixes to json exporter (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cfc2d46">commit cfc2d46</a>)</li>
<li>Fix permission and set umask before generating sdist tarball (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/06149e0">commit 06149e0</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-09-03">
<h4>0.18.2 (released 2013-09-03)<a class="headerlink" href="#released-2013-09-03" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Backport <cite>scrapy check</cite> command fixes and backward compatible multi
crawler process(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/339">issue 339</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-08-27">
<h4>0.18.1 (released 2013-08-27)<a class="headerlink" href="#released-2013-08-27" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>remove extra import added by cherry picked changes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d20304e">commit d20304e</a>)</li>
<li>fix crawling tests under twisted pre 11.0.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1994f38">commit 1994f38</a>)</li>
<li>py26 can not format zero length fields {} (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/abf756f">commit abf756f</a>)</li>
<li>test PotentiaDataLoss errors on unbound responses (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b15470d">commit b15470d</a>)</li>
<li>Treat responses without content-length or Transfer-Encoding as good responses (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c4bf324">commit c4bf324</a>)</li>
<li>do no include ResponseFailed if http11 handler is not enabled (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6cbe684">commit 6cbe684</a>)</li>
<li>New HTTP client wraps connection losts in ResponseFailed exception. fix #373 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1a20bba">commit 1a20bba</a>)</li>
<li>limit travis-ci build matrix (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3b01bb8">commit 3b01bb8</a>)</li>
<li>Merge pull request #375 from peterarenot/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fa766d7">commit fa766d7</a>)</li>
<li>Fixed so it refers to the correct folder (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3283809">commit 3283809</a>)</li>
<li>added quantal &amp; raring to support ubuntu releases (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1411923">commit 1411923</a>)</li>
<li>fix retry middleware which didn&#8217;t retry certain connection errors after the upgrade to http1 client, closes GH-373 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bb35ed0">commit bb35ed0</a>)</li>
<li>fix XmlItemExporter in Python 2.7.4 and 2.7.5 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/de3e451">commit de3e451</a>)</li>
<li>minor updates to 0.18 release notes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c45e5f1">commit c45e5f1</a>)</li>
<li>fix contributters list format (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0b60031">commit 0b60031</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-08-09">
<h4>0.18.0 (released 2013-08-09)<a class="headerlink" href="#released-2013-08-09" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Lot of improvements to testsuite run using Tox, including a way to test on pypi</li>
<li>Handle GET parameters for AJAX crawleable urls (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3fe2a32">commit 3fe2a32</a>)</li>
<li>Use lxml recover option to parse sitemaps (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/347">issue 347</a>)</li>
<li>Bugfix cookie merging by hostname and not by netloc (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/352">issue 352</a>)</li>
<li>Support disabling <cite>HttpCompressionMiddleware</cite> using a flag setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/359">issue 359</a>)</li>
<li>Support xml namespaces using <cite>iternodes</cite> parser in <cite>XMLFeedSpider</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/12">issue 12</a>)</li>
<li>Support <cite>dont_cache</cite> request meta flag (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/19">issue 19</a>)</li>
<li>Bugfix <cite>scrapy.utils.gz.gunzip</cite> broken by changes in python 2.7.4 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4dc76e">commit 4dc76e</a>)</li>
<li>Bugfix url encoding on <cite>SgmlLinkExtractor</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/24">issue 24</a>)</li>
<li>Bugfix <cite>TakeFirst</cite> processor shouldn&#8217;t discard zero (0) value (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/59">issue 59</a>)</li>
<li>Support nested items in xml exporter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/66">issue 66</a>)</li>
<li>Improve cookies handling performance (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/77">issue 77</a>)</li>
<li>Log dupe filtered requests once (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/105">issue 105</a>)</li>
<li>Split redirection middleware into status and meta based middlewares (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/78">issue 78</a>)</li>
<li>Use HTTP1.1 as default downloader handler (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/109">issue 109</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/318">issue 318</a>)</li>
<li>Support xpath form selection on <cite>FormRequest.from_response</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/185">issue 185</a>)</li>
<li>Bugfix unicode decoding error on <cite>SgmlLinkExtractor</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/199">issue 199</a>)</li>
<li>Bugfix signal dispatching on pypi interpreter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/205">issue 205</a>)</li>
<li>Improve request delay and concurrency handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/206">issue 206</a>)</li>
<li>Add RFC2616 cache policy to <cite>HttpCacheMiddleware</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/212">issue 212</a>)</li>
<li>Allow customization of messages logged by engine (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/214">issue 214</a>)</li>
<li>Multiples improvements to <cite>DjangoItem</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/217">issue 217</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/218">issue 218</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/221">issue 221</a>)</li>
<li>Extend Scrapy commands using setuptools entry points (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/260">issue 260</a>)</li>
<li>Allow spider <cite>allowed_domains</cite> value to be set/tuple (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/261">issue 261</a>)</li>
<li>Support <cite>settings.getdict</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/269">issue 269</a>)</li>
<li>Simplify internal <cite>scrapy.core.scraper</cite> slot handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/271">issue 271</a>)</li>
<li>Added <cite>Item.copy</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/290">issue 290</a>)</li>
<li>Collect idle downloader slots (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/297">issue 297</a>)</li>
<li>Add <cite>ftp://</cite> scheme downloader handler (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/329">issue 329</a>)</li>
<li>Added downloader benchmark webserver and spider tools <a class="reference internal" href="index.html#benchmarking"><em>Benchmarking</em></a></li>
<li>Moved persistent (on disk) queues to a separate project (<a class="reference external" href="https://github.com/scrapy/queuelib">queuelib</a>) which scrapy now depends on</li>
<li>Add scrapy commands using external libraries (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/260">issue 260</a>)</li>
<li>Added <tt class="docutils literal"><span class="pre">--pdb</span></tt> option to <tt class="docutils literal"><span class="pre">scrapy</span></tt> command line tool</li>
<li>Added <tt class="xref py py-meth docutils literal"><span class="pre">XPathSelector.remove_namespaces()</span></tt> which allows to remove all namespaces from XML documents for convenience (to work with namespace-less XPaths). Documented in <a class="reference internal" href="index.html#topics-selectors"><em>Selectors</em></a>.</li>
<li>Several improvements to spider contracts</li>
<li>New default middleware named MetaRefreshMiddldeware that handles meta-refresh html tag redirections,</li>
<li>MetaRefreshMiddldeware and RedirectMiddleware have different priorities to address #62</li>
<li>added from_crawler method to spiders</li>
<li>added system tests with mock server</li>
<li>more improvements to Mac OS compatibility (thanks Alex Cepoi)</li>
<li>several more cleanups to singletons and multi-spider support (thanks Nicolas Ramirez)</li>
<li>support custom download slots</li>
<li>added &#8211;spider option to &#8220;shell&#8221; command.</li>
<li>log overridden settings when scrapy starts</li>
</ul>
<p>Thanks to everyone who contribute to this release. Here is a list of
contributors sorted by number of commits:</p>
<div class="highlight-none"><div class="highlight"><pre>130 Pablo Hoffman &lt;pablo@...&gt;
 97 Daniel Graña &lt;dangra@...&gt;
 20 Nicolás Ramírez &lt;nramirez.uy@...&gt;
 13 Mikhail Korobov &lt;kmike84@...&gt;
 12 Pedro Faustino &lt;pedrobandim@...&gt;
 11 Steven Almeroth &lt;sroth77@...&gt;
  5 Rolando Espinoza La fuente &lt;darkrho@...&gt;
  4 Michal Danilak &lt;mimino.coder@...&gt;
  4 Alex Cepoi &lt;alex.cepoi@...&gt;
  4 Alexandr N Zamaraev (aka tonal) &lt;tonal@...&gt;
  3 paul &lt;paul.tremberth@...&gt;
  3 Martin Olveyra &lt;molveyra@...&gt;
  3 Jordi Llonch &lt;llonchj@...&gt;
  3 arijitchakraborty &lt;myself.arijit@...&gt;
  2 Shane Evans &lt;shane.evans@...&gt;
  2 joehillen &lt;joehillen@...&gt;
  2 Hart &lt;HartSimha@...&gt;
  2 Dan &lt;ellisd23@...&gt;
  1 Zuhao Wan &lt;wanzuhao@...&gt;
  1 whodatninja &lt;blake@...&gt;
  1 vkrest &lt;v.krestiannykov@...&gt;
  1 tpeng &lt;pengtaoo@...&gt;
  1 Tom Mortimer-Jones &lt;tom@...&gt;
  1 Rocio Aramberri &lt;roschegel@...&gt;
  1 Pedro &lt;pedro@...&gt;
  1 notsobad &lt;wangxiaohugg@...&gt;
  1 Natan L &lt;kuyanatan.nlao@...&gt;
  1 Mark Grey &lt;mark.grey@...&gt;
  1 Luan &lt;luanpab@...&gt;
  1 Libor Nenadál &lt;libor.nenadal@...&gt;
  1 Juan M Uys &lt;opyate@...&gt;
  1 Jonas Brunsgaard &lt;jonas.brunsgaard@...&gt;
  1 Ilya Baryshev &lt;baryshev@...&gt;
  1 Hasnain Lakhani &lt;m.hasnain.lakhani@...&gt;
  1 Emanuel Schorsch &lt;emschorsch@...&gt;
  1 Chris Tilden &lt;chris.tilden@...&gt;
  1 Capi Etheriel &lt;barraponto@...&gt;
  1 cacovsky &lt;amarquesferraz@...&gt;
  1 Berend Iwema &lt;berend@...&gt;
</pre></div>
</div>
</div>
<div class="section" id="released-2013-05-30">
<h4>0.16.5 (released 2013-05-30)<a class="headerlink" href="#released-2013-05-30" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>obey request method when scrapy deploy is redirected to a new endpoint (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8c4fcee">commit 8c4fcee</a>)</li>
<li>fix inaccurate downloader middleware documentation. refs #280 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/40667cb">commit 40667cb</a>)</li>
<li>doc: remove links to diveintopython.org, which is no longer available. closes #246 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bd58bfa">commit bd58bfa</a>)</li>
<li>Find form nodes in invalid html5 documents (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e3d6945">commit e3d6945</a>)</li>
<li>Fix typo labeling attrs type bool instead of list (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a274276">commit a274276</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-01-23">
<h4>0.16.4 (released 2013-01-23)<a class="headerlink" href="#released-2013-01-23" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>fixes spelling errors in documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6d2b3aa">commit 6d2b3aa</a>)</li>
<li>add doc about disabling an extension. refs #132 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c90de33">commit c90de33</a>)</li>
<li>Fixed error message formatting. log.err() doesn&#8217;t support cool formatting and when error occurred, the message was:    &#8220;ERROR: Error processing %(item)s&#8221; (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c16150c">commit c16150c</a>)</li>
<li>lint and improve images pipeline error logging (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/56b45fc">commit 56b45fc</a>)</li>
<li>fixed doc typos (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/243be84">commit 243be84</a>)</li>
<li>add documentation topics: Broad Crawls &amp; Common Practies (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1fbb715">commit 1fbb715</a>)</li>
<li>fix bug in scrapy parse command when spider is not specified explicitly. closes #209 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c72e682">commit c72e682</a>)</li>
<li>Update docs/topics/commands.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/28eac7a">commit 28eac7a</a>)</li>
</ul>
</div>
<div class="section" id="released-2012-12-07">
<h4>0.16.3 (released 2012-12-07)<a class="headerlink" href="#released-2012-12-07" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Remove concurrency limitation when using download delays and still ensure inter-request delays are enforced (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/487b9b5">commit 487b9b5</a>)</li>
<li>add error details when image pipeline fails (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8232569">commit 8232569</a>)</li>
<li>improve mac os compatibility (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8dcf8aa">commit 8dcf8aa</a>)</li>
<li>setup.py: use README.rst to populate long_description (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7b5310d">commit 7b5310d</a>)</li>
<li>doc: removed obsolete references to ClientForm (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/80f9bb6">commit 80f9bb6</a>)</li>
<li>correct docs for default storage backend (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2aa491b">commit 2aa491b</a>)</li>
<li>doc: removed broken proxyhub link from FAQ (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bdf61c4">commit bdf61c4</a>)</li>
<li>Fixed docs typo in SpiderOpenCloseLogging example (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7184094">commit 7184094</a>)</li>
</ul>
</div>
<div class="section" id="released-2012-11-09">
<h4>0.16.2 (released 2012-11-09)<a class="headerlink" href="#released-2012-11-09" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>scrapy contracts: python2.6 compat (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a4a9199">commit a4a9199</a>)</li>
<li>scrapy contracts verbose option (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ec41673">commit ec41673</a>)</li>
<li>proper unittest-like output for scrapy contracts (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/86635e4">commit 86635e4</a>)</li>
<li>added open_in_browser to debugging doc (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c9b690d">commit c9b690d</a>)</li>
<li>removed reference to global scrapy stats from settings doc (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/dd55067">commit dd55067</a>)</li>
<li>Fix SpiderState bug in Windows platforms (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/58998f4">commit 58998f4</a>)</li>
</ul>
</div>
<div class="section" id="released-2012-10-26">
<h4>0.16.1 (released 2012-10-26)<a class="headerlink" href="#released-2012-10-26" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>fixed LogStats extension, which got broken after a wrong merge before the 0.16 release (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8c780fd">commit 8c780fd</a>)</li>
<li>better backwards compatibility for scrapy.conf.settings (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3403089">commit 3403089</a>)</li>
<li>extended documentation on how to access crawler stats from extensions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c4da0b5">commit c4da0b5</a>)</li>
<li>removed .hgtags (no longer needed now that scrapy uses git) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d52c188">commit d52c188</a>)</li>
<li>fix dashes under rst headers (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fa4f7f9">commit fa4f7f9</a>)</li>
<li>set release date for 0.16.0 in news (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e292246">commit e292246</a>)</li>
</ul>
</div>
<div class="section" id="released-2012-10-18">
<h4>0.16.0 (released 2012-10-18)<a class="headerlink" href="#released-2012-10-18" title="Permalink to this headline">¶</a></h4>
<p>Scrapy changes:</p>
<ul class="simple">
<li>added <a class="reference internal" href="index.html#topics-contracts"><em>Spiders Contracts</em></a>, a mechanism for testing spiders in a formal/reproducible way</li>
<li>added options <tt class="docutils literal"><span class="pre">-o</span></tt> and <tt class="docutils literal"><span class="pre">-t</span></tt> to the <a class="reference internal" href="index.html#std:command-runspider"><tt class="xref std std-command docutils literal"><span class="pre">runspider</span></tt></a> command</li>
<li>documented <a class="reference internal" href="index.html#document-topics/autothrottle"><em>AutoThrottle extension</em></a> and added to extensions installed by default. You still need to enable it with <a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_ENABLED</span></tt></a></li>
<li>major Stats Collection refactoring: removed separation of global/per-spider stats, removed stats-related signals (<tt class="docutils literal"><span class="pre">stats_spider_opened</span></tt>, etc). Stats are much simpler now, backwards compatibility is kept on the Stats Collector API and signals.</li>
<li>added <a class="reference internal" href="index.html#scrapy.contrib.spidermiddleware.SpiderMiddleware.process_start_requests" title="scrapy.contrib.spidermiddleware.SpiderMiddleware.process_start_requests"><tt class="xref py py-meth docutils literal"><span class="pre">process_start_requests()</span></tt></a> method to spider middlewares</li>
<li>dropped Signals singleton. Signals should now be accesed through the Crawler.signals attribute. See the signals documentation for more info.</li>
<li>dropped Signals singleton. Signals should now be accesed through the Crawler.signals attribute. See the signals documentation for more info.</li>
<li>dropped Stats Collector singleton. Stats can now be accessed through the Crawler.stats attribute. See the stats collection documentation for more info.</li>
<li>documented <a class="reference internal" href="index.html#topics-api"><em>Core API</em></a></li>
<li><cite>lxml</cite> is now the default selectors backend instead of <cite>libxml2</cite></li>
<li>ported FormRequest.from_response() to use <a class="reference external" href="http://lxml.de/">lxml</a> instead of <a class="reference external" href="http://wwwsearch.sourceforge.net/old/ClientForm/">ClientForm</a></li>
<li>removed modules: <tt class="docutils literal"><span class="pre">scrapy.xlib.BeautifulSoup</span></tt> and <tt class="docutils literal"><span class="pre">scrapy.xlib.ClientForm</span></tt></li>
<li>SitemapSpider: added support for sitemap urls ending in .xml and .xml.gz, even if they advertise a wrong content type (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/10ed28b">commit 10ed28b</a>)</li>
<li>StackTraceDump extension: also dump trackref live references (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fe2ce93">commit fe2ce93</a>)</li>
<li>nested items now fully supported in JSON and JSONLines exporters</li>
<li>added <a class="reference internal" href="index.html#std:reqmeta-cookiejar"><tt class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></tt></a> Request meta key to support multiple cookie sessions per spider</li>
<li>decoupled encoding detection code to <a class="reference external" href="https://github.com/scrapy/w3lib/blob/master/w3lib/encoding.py">w3lib.encoding</a>, and ported Scrapy code to use that mdule</li>
<li>dropped support for Python 2.5. See <a class="reference external" href="http://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/">http://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/</a></li>
<li>dropped support for Twisted 2.5</li>
<li>added <a class="reference internal" href="index.html#std:setting-REFERER_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">REFERER_ENABLED</span></tt></a> setting, to control referer middleware</li>
<li>changed default user agent to: <tt class="docutils literal"><span class="pre">Scrapy/VERSION</span> <span class="pre">(+http://scrapy.org)</span></tt></li>
<li>removed (undocumented) <tt class="docutils literal"><span class="pre">HTMLImageLinkExtractor</span></tt> class from <tt class="docutils literal"><span class="pre">scrapy.contrib.linkextractors.image</span></tt></li>
<li>removed per-spider settings (to be replaced by instantiating multiple crawler objects)</li>
<li><tt class="docutils literal"><span class="pre">USER_AGENT</span></tt> spider attribute will no longer work, use <tt class="docutils literal"><span class="pre">user_agent</span></tt> attribute instead</li>
<li><tt class="docutils literal"><span class="pre">DOWNLOAD_TIMEOUT</span></tt> spider attribute will no longer work, use <tt class="docutils literal"><span class="pre">download_timeout</span></tt> attribute instead</li>
<li>removed <tt class="docutils literal"><span class="pre">ENCODING_ALIASES</span></tt> setting, as encoding auto-detection has been moved to the <a class="reference external" href="https://github.com/scrapy/w3lib">w3lib</a> library</li>
<li>promoted <a class="reference internal" href="index.html#topics-djangoitem"><em>DjangoItem</em></a> to main contrib</li>
<li>LogFormatter method now return dicts(instead of strings) to support lazy formatting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/164">issue 164</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/commit/dcef7b0">commit dcef7b0</a>)</li>
<li>downloader handlers (<a class="reference internal" href="index.html#std:setting-DOWNLOAD_HANDLERS"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS</span></tt></a> setting) now receive settings as the first argument of the constructor</li>
<li>replaced memory usage acounting with (more portable) <a class="reference external" href="http://docs.python.org/library/resource.html">resource</a> module, removed <tt class="docutils literal"><span class="pre">scrapy.utils.memory</span></tt> module</li>
<li>removed signal: <tt class="docutils literal"><span class="pre">scrapy.mail.mail_sent</span></tt></li>
<li>removed <tt class="docutils literal"><span class="pre">TRACK_REFS</span></tt> setting, now <a class="reference internal" href="index.html#topics-leaks-trackrefs"><em>trackrefs</em></a> is always enabled</li>
<li>DBM is now the default storage backend for HTTP cache middleware</li>
<li>number of log messages (per level) are now tracked through Scrapy stats (stat name: <tt class="docutils literal"><span class="pre">log_count/LEVEL</span></tt>)</li>
<li>number received responses are now tracked through Scrapy stats (stat name: <tt class="docutils literal"><span class="pre">response_received_count</span></tt>)</li>
<li>removed <tt class="docutils literal"><span class="pre">scrapy.log.started</span></tt> attribute</li>
</ul>
</div>
<div class="section" id="id5">
<h4>0.14.4<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>added precise to supported ubuntu distros (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b7e46df">commit b7e46df</a>)</li>
<li>fixed bug in json-rpc webservice reported in <a class="reference external" href="https://groups.google.com/d/topic/scrapy-users/qgVBmFybNAQ/discussion">https://groups.google.com/d/topic/scrapy-users/qgVBmFybNAQ/discussion</a>. also removed no longer supported &#8216;run&#8217; command from extras/scrapy-ws.py (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/340fbdb">commit 340fbdb</a>)</li>
<li>meta tag attributes for content-type http equiv can be in any order. #123 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0cb68af">commit 0cb68af</a>)</li>
<li>replace &#8220;import Image&#8221; by more standard &#8220;from PIL import Image&#8221;. closes #88 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4d17048">commit 4d17048</a>)</li>
<li>return trial status as bin/runtests.sh exit value. #118 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b7b2e7f">commit b7b2e7f</a>)</li>
</ul>
</div>
<div class="section" id="id6">
<h4>0.14.3<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>forgot to include pydispatch license. #118 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fd85f9c">commit fd85f9c</a>)</li>
<li>include egg files used by testsuite in source distribution. #118 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c897793">commit c897793</a>)</li>
<li>update docstring in project template to avoid confusion with genspider command, which may be considered as an advanced feature. refs #107 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2548dcc">commit 2548dcc</a>)</li>
<li>added note to docs/topics/firebug.rst about google directory being shut down (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/668e352">commit 668e352</a>)</li>
<li>dont discard slot when empty, just save in another dict in order to recycle if needed again. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8e9f607">commit 8e9f607</a>)</li>
<li>do not fail handling unicode xpaths in libxml2 backed selectors (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b830e95">commit b830e95</a>)</li>
<li>fixed minor mistake in Request objects documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bf3c9ee">commit bf3c9ee</a>)</li>
<li>fixed minor defect in link extractors documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ba14f38">commit ba14f38</a>)</li>
<li>removed some obsolete remaining code related to sqlite support in scrapy (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0665175">commit 0665175</a>)</li>
</ul>
</div>
<div class="section" id="id7">
<h4>0.14.2<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>move buffer pointing to start of file before computing checksum. refs #92 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6a5bef2">commit 6a5bef2</a>)</li>
<li>Compute image checksum before persisting images. closes #92 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/9817df1">commit 9817df1</a>)</li>
<li>remove leaking references in cached failures (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/673a120">commit 673a120</a>)</li>
<li>fixed bug in MemoryUsage extension: get_engine_status() takes exactly 1 argument (0 given) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/11133e9">commit 11133e9</a>)</li>
<li>fixed struct.error on http compression middleware. closes #87 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1423140">commit 1423140</a>)</li>
<li>ajax crawling wasn&#8217;t expanding for unicode urls (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0de3fb4">commit 0de3fb4</a>)</li>
<li>Catch start_requests iterator errors. refs #83 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/454a21d">commit 454a21d</a>)</li>
<li>Speed-up libxml2 XPathSelector (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2fbd662">commit 2fbd662</a>)</li>
<li>updated versioning doc according to recent changes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0a070f5">commit 0a070f5</a>)</li>
<li>scrapyd: fixed documentation link (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2b4e4c3">commit 2b4e4c3</a>)</li>
<li>extras/makedeb.py: no longer obtaining version from git (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/caffe0e">commit caffe0e</a>)</li>
</ul>
</div>
<div class="section" id="id8">
<h4>0.14.1<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>extras/makedeb.py: no longer obtaining version from git (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/caffe0e">commit caffe0e</a>)</li>
<li>bumped version to 0.14.1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6cb9e1c">commit 6cb9e1c</a>)</li>
<li>fixed reference to tutorial directory (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4b86bd6">commit 4b86bd6</a>)</li>
<li>doc: removed duplicated callback argument from Request.replace() (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1aeccdd">commit 1aeccdd</a>)</li>
<li>fixed formatting of scrapyd doc (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8bf19e6">commit 8bf19e6</a>)</li>
<li>Dump stacks for all running threads and fix engine status dumped by StackTraceDump extension (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/14a8e6e">commit 14a8e6e</a>)</li>
<li>added comment about why we disable ssl on boto images upload (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5223575">commit 5223575</a>)</li>
<li>SSL handshaking hangs when doing too many parallel connections to S3 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/63d583d">commit 63d583d</a>)</li>
<li>change tutorial to follow changes on dmoz site (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bcb3198">commit bcb3198</a>)</li>
<li>Avoid _disconnectedDeferred AttributeError exception in Twisted&gt;=11.1.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/98f3f87">commit 98f3f87</a>)</li>
<li>allow spider to set autothrottle max concurrency (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/175a4b5">commit 175a4b5</a>)</li>
</ul>
</div>
<div class="section" id="id9">
<h4>0.14<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h4>
<div class="section" id="new-features-and-settings">
<h5>New features and settings<a class="headerlink" href="#new-features-and-settings" title="Permalink to this headline">¶</a></h5>
<ul>
<li><p class="first">Support for <a class="reference external" href="http://code.google.com/web/ajaxcrawling/docs/getting-started.html">AJAX crawleable urls</a></p>
</li>
<li><p class="first">New persistent scheduler that stores requests on disk, allowing to suspend and resume crawls (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2737">r2737</a>)</p>
</li>
<li><p class="first">added <tt class="docutils literal"><span class="pre">-o</span></tt> option to <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">crawl</span></tt>, a shortcut for dumping scraped items into a file (or standard output using <tt class="docutils literal"><span class="pre">-</span></tt>)</p>
</li>
<li><p class="first">Added support for passing custom settings to Scrapyd <tt class="docutils literal"><span class="pre">schedule.json</span></tt> api (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2779">r2779</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2783">r2783</a>)</p>
</li>
<li><p class="first">New <tt class="docutils literal"><span class="pre">ChunkedTransferMiddleware</span></tt> (enabled by default) to support <a class="reference external" href="http://en.wikipedia.org/wiki/Chunked_transfer_encoding">chunked transfer encoding</a> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2769">r2769</a>)</p>
</li>
<li><p class="first">Add boto 2.0 support for S3 downloader handler (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2763">r2763</a>)</p>
</li>
<li><p class="first">Added <a class="reference external" href="http://docs.python.org/library/marshal.html">marshal</a> to formats supported by feed exports (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2744">r2744</a>)</p>
</li>
<li><p class="first">In request errbacks, offending requests are now received in <cite>failure.request</cite> attribute (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2738">r2738</a>)</p>
</li>
<li><dl class="first docutils">
<dt>Big downloader refactoring to support per domain/ip concurrency limits (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2732">r2732</a>)</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_SPIDER</span></tt> setting has been deprecated and replaced by:</dt>
<dd><ul class="first last simple">
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS</span></tt></a>, <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></tt></a>, <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><tt class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></tt></a></li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">check the documentation for more details</p>
</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Added builtin caching DNS resolver (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2728">r2728</a>)</p>
</li>
<li><p class="first">Moved Amazon AWS-related components/extensions (SQS spider queue, SimpleDB stats collector) to a separate project: [scaws](<a class="reference external" href="https://github.com/scrapinghub/scaws">https://github.com/scrapinghub/scaws</a>) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2706">r2706</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2714">r2714</a>)</p>
</li>
<li><p class="first">Moved spider queues to scrapyd: <cite>scrapy.spiderqueue</cite> -&gt; <cite>scrapyd.spiderqueue</cite> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2708">r2708</a>)</p>
</li>
<li><p class="first">Moved sqlite utils to scrapyd: <cite>scrapy.utils.sqlite</cite> -&gt; <cite>scrapyd.sqlite</cite> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2781">r2781</a>)</p>
</li>
<li><p class="first">Real support for returning iterators on <cite>start_requests()</cite> method. The iterator is now consumed during the crawl when the spider is getting idle (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>)</p>
</li>
<li><p class="first">Added <a class="reference internal" href="index.html#std:setting-REDIRECT_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">REDIRECT_ENABLED</span></tt></a> setting to quickly enable/disable the redirect middleware (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2697">r2697</a>)</p>
</li>
<li><p class="first">Added <a class="reference internal" href="index.html#std:setting-RETRY_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">RETRY_ENABLED</span></tt></a> setting to quickly enable/disable the retry middleware (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2694">r2694</a>)</p>
</li>
<li><p class="first">Added <tt class="docutils literal"><span class="pre">CloseSpider</span></tt> exception to manually close spiders (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2691">r2691</a>)</p>
</li>
<li><p class="first">Improved encoding detection by adding support for HTML5 meta charset declaration (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2690">r2690</a>)</p>
</li>
<li><p class="first">Refactored close spider behavior to wait for all downloads to finish and be processed by spiders, before closing the spider (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2688">r2688</a>)</p>
</li>
<li><p class="first">Added <tt class="docutils literal"><span class="pre">SitemapSpider</span></tt> (see documentation in Spiders page) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2658">r2658</a>)</p>
</li>
<li><p class="first">Added <tt class="docutils literal"><span class="pre">LogStats</span></tt> extension for periodically logging basic stats (like crawled pages and scraped items) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2657">r2657</a>)</p>
</li>
<li><p class="first">Make handling of gzipped responses more robust (#319, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2643">r2643</a>). Now Scrapy will try and decompress as much as possible from a gzipped response, instead of failing with an <cite>IOError</cite>.</p>
</li>
<li><p class="first">Simplified !MemoryDebugger extension to use stats for dumping memory debugging info (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2639">r2639</a>)</p>
</li>
<li><p class="first">Added new command to edit spiders: <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">edit</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2636">r2636</a>) and <cite>-e</cite> flag to <cite>genspider</cite> command that uses it (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2653">r2653</a>)</p>
</li>
<li><p class="first">Changed default representation of items to pretty-printed dicts. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2631">r2631</a>). This improves default logging by making log more readable in the default case, for both Scraped and Dropped lines.</p>
</li>
<li><p class="first">Added <a class="reference internal" href="index.html#std:signal-spider_error"><tt class="xref std std-signal docutils literal"><span class="pre">spider_error</span></tt></a> signal (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2628">r2628</a>)</p>
</li>
<li><p class="first">Added <a class="reference internal" href="index.html#std:setting-COOKIES_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">COOKIES_ENABLED</span></tt></a> setting (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2625">r2625</a>)</p>
</li>
<li><p class="first">Stats are now dumped to Scrapy log (default value of <a class="reference internal" href="index.html#std:setting-STATS_DUMP"><tt class="xref std std-setting docutils literal"><span class="pre">STATS_DUMP</span></tt></a> setting has been changed to <cite>True</cite>). This is to make Scrapy users more aware of Scrapy stats and the data that is collected there.</p>
</li>
<li><p class="first">Added support for dynamically adjusting download delay and maximum concurrent requests (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2599">r2599</a>)</p>
</li>
<li><p class="first">Added new DBM HTTP cache storage backend (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2576">r2576</a>)</p>
</li>
<li><p class="first">Added <tt class="docutils literal"><span class="pre">listjobs.json</span></tt> API to Scrapyd (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2571">r2571</a>)</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">CsvItemExporter</span></tt>: added <tt class="docutils literal"><span class="pre">join_multivalued</span></tt> parameter (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2578">r2578</a>)</p>
</li>
<li><p class="first">Added namespace support to <tt class="docutils literal"><span class="pre">xmliter_lxml</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2552">r2552</a>)</p>
</li>
<li><p class="first">Improved cookies middleware by making <cite>COOKIES_DEBUG</cite> nicer and documenting it (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2579">r2579</a>)</p>
</li>
<li><p class="first">Several improvements to Scrapyd and Link extractors</p>
</li>
</ul>
</div>
<div class="section" id="code-rearranged-and-removed">
<h5>Code rearranged and removed<a class="headerlink" href="#code-rearranged-and-removed" title="Permalink to this headline">¶</a></h5>
<ul>
<li><dl class="first docutils">
<dt>Merged item passed and item scraped concepts, as they have often proved confusing in the past. This means: (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2630">r2630</a>)</dt>
<dd><ul class="first last simple">
<li>original item_scraped signal was removed</li>
<li>original item_passed signal was renamed to item_scraped</li>
<li>old log lines <tt class="docutils literal"><span class="pre">Scraped</span> <span class="pre">Item...</span></tt> were removed</li>
<li>old log lines <tt class="docutils literal"><span class="pre">Passed</span> <span class="pre">Item...</span></tt> were renamed to <tt class="docutils literal"><span class="pre">Scraped</span> <span class="pre">Item...</span></tt> lines and downgraded to <tt class="docutils literal"><span class="pre">DEBUG</span></tt> level</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Reduced Scrapy codebase by striping part of Scrapy code into two new libraries:</dt>
<dd><ul class="first last simple">
<li><a class="reference external" href="https://github.com/scrapy/w3lib">w3lib</a> (several functions from <tt class="docutils literal"><span class="pre">scrapy.utils.{http,markup,multipart,response,url}</span></tt>, done in <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2584">r2584</a>)</li>
<li><a class="reference external" href="https://github.com/scrapy/scrapely">scrapely</a> (was <tt class="docutils literal"><span class="pre">scrapy.contrib.ibl</span></tt>, done in <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2586">r2586</a>)</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Removed unused function: <cite>scrapy.utils.request.request_info()</cite> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2577">r2577</a>)</p>
</li>
<li><p class="first">Removed googledir project from <cite>examples/googledir</cite>. There&#8217;s now a new example project called <cite>dirbot</cite> available on github: <a class="reference external" href="https://github.com/scrapy/dirbot">https://github.com/scrapy/dirbot</a></p>
</li>
<li><p class="first">Removed support for default field values in Scrapy items (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2616">r2616</a>)</p>
</li>
<li><p class="first">Removed experimental crawlspider v2 (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2632">r2632</a>)</p>
</li>
<li><p class="first">Removed scheduler middleware to simplify architecture. Duplicates filter is now done in the scheduler itself, using the same dupe fltering class as before (<cite>DUPEFILTER_CLASS</cite> setting) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2640">r2640</a>)</p>
</li>
<li><p class="first">Removed support for passing urls to <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">crawl</span></tt> command (use <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">parse</span></tt> instead) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>)</p>
</li>
<li><p class="first">Removed deprecated Execution Queue (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>)</p>
</li>
<li><p class="first">Removed (undocumented) spider context extension (from scrapy.contrib.spidercontext) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2780">r2780</a>)</p>
</li>
<li><p class="first">removed <tt class="docutils literal"><span class="pre">CONCURRENT_SPIDERS</span></tt> setting (use scrapyd maxproc instead) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2789">r2789</a>)</p>
</li>
<li><p class="first">Renamed attributes of core components: downloader.sites -&gt; downloader.slots, scraper.sites -&gt; scraper.slots (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2717">r2717</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2718">r2718</a>)</p>
</li>
<li><p class="first">Renamed setting <tt class="docutils literal"><span class="pre">CLOSESPIDER_ITEMPASSED</span></tt> to <a class="reference internal" href="index.html#std:setting-CLOSESPIDER_ITEMCOUNT"><tt class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_ITEMCOUNT</span></tt></a> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2655">r2655</a>). Backwards compatibility kept.</p>
</li>
</ul>
</div>
</div>
<div class="section" id="id10">
<h4>0.12<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<div class="section" id="new-features-and-improvements">
<h5>New features and improvements<a class="headerlink" href="#new-features-and-improvements" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Passed item is now sent in the <tt class="docutils literal"><span class="pre">item</span></tt> argument of the <tt class="xref std std-signal docutils literal"><span class="pre">item_passed</span></tt> (#273)</li>
<li>Added verbose option to <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">version</span></tt> command, useful for bug reports (#298)</li>
<li>HTTP cache now stored by default in the project data dir (#279)</li>
<li>Added project data storage directory (#276, #277)</li>
<li>Documented file structure of Scrapy projects (see command-line tool doc)</li>
<li>New lxml backend for XPath selectors (#147)</li>
<li>Per-spider settings (#245)</li>
<li>Support exit codes to signal errors in Scrapy commands (#248)</li>
<li>Added <tt class="docutils literal"><span class="pre">-c</span></tt> argument to <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">shell</span></tt> command</li>
<li>Made <tt class="docutils literal"><span class="pre">libxml2</span></tt> optional (#260)</li>
<li>New <tt class="docutils literal"><span class="pre">deploy</span></tt> command (#261)</li>
<li>Added <a class="reference internal" href="index.html#std:setting-CLOSESPIDER_PAGECOUNT"><tt class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_PAGECOUNT</span></tt></a> setting (#253)</li>
<li>Added <a class="reference internal" href="index.html#std:setting-CLOSESPIDER_ERRORCOUNT"><tt class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_ERRORCOUNT</span></tt></a> setting (#254)</li>
</ul>
</div>
<div class="section" id="scrapyd-changes">
<h5>Scrapyd changes<a class="headerlink" href="#scrapyd-changes" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Scrapyd now uses one process per spider</li>
<li>It stores one log file per spider run, and rotate them keeping the lastest 5 logs per spider (by default)</li>
<li>A minimal web ui was added, available at <a class="reference external" href="http://localhost:6800">http://localhost:6800</a> by default</li>
<li>There is now a <cite>scrapy server</cite> command to start a Scrapyd server of the current project</li>
</ul>
</div>
<div class="section" id="changes-to-settings">
<h5>Changes to settings<a class="headerlink" href="#changes-to-settings" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>added <cite>HTTPCACHE_ENABLED</cite> setting (False by default) to enable HTTP cache middleware</li>
<li>changed <cite>HTTPCACHE_EXPIRATION_SECS</cite> semantics: now zero means &#8220;never expire&#8221;.</li>
</ul>
</div>
<div class="section" id="deprecated-obsoleted-functionality">
<h5>Deprecated/obsoleted functionality<a class="headerlink" href="#deprecated-obsoleted-functionality" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Deprecated <tt class="docutils literal"><span class="pre">runserver</span></tt> command in favor of <tt class="docutils literal"><span class="pre">server</span></tt> command which starts a Scrapyd server. See also: Scrapyd changes</li>
<li>Deprecated <tt class="docutils literal"><span class="pre">queue</span></tt> command in favor of using Scrapyd <tt class="docutils literal"><span class="pre">schedule.json</span></tt> API. See also: Scrapyd changes</li>
<li>Removed the !LxmlItemLoader (experimental contrib which never graduated to main contrib)</li>
</ul>
</div>
</div>
<div class="section" id="id11">
<h4>0.10<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<div class="section" id="id12">
<h5>New features and improvements<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>New Scrapy service called <tt class="docutils literal"><span class="pre">scrapyd</span></tt> for deploying Scrapy crawlers in production (#218) (documentation available)</li>
<li>Simplified Images pipeline usage which doesn&#8217;t require subclassing your own images pipeline now (#217)</li>
<li>Scrapy shell now shows the Scrapy log by default (#206)</li>
<li>Refactored execution queue in a common base code and pluggable backends called &#8220;spider queues&#8221; (#220)</li>
<li>New persistent spider queue (based on SQLite) (#198), available by default, which allows to start Scrapy in server mode and then schedule spiders to run.</li>
<li>Added documentation for Scrapy command-line tool and all its available sub-commands. (documentation available)</li>
<li>Feed exporters with pluggable backends (#197) (documentation available)</li>
<li>Deferred signals (#193)</li>
<li>Added two new methods to item pipeline open_spider(), close_spider() with deferred support (#195)</li>
<li>Support for overriding default request headers per spider (#181)</li>
<li>Replaced default Spider Manager with one with similar functionality but not depending on Twisted Plugins (#186)</li>
<li>Splitted Debian package into two packages - the library and the service (#187)</li>
<li>Scrapy log refactoring (#188)</li>
<li>New extension for keeping persistent spider contexts among different runs (#203)</li>
<li>Added <cite>dont_redirect</cite> request.meta key for avoiding redirects (#233)</li>
<li>Added <cite>dont_retry</cite> request.meta key for avoiding retries (#234)</li>
</ul>
</div>
<div class="section" id="command-line-tool-changes">
<h5>Command-line tool changes<a class="headerlink" href="#command-line-tool-changes" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>New <cite>scrapy</cite> command which replaces the old <cite>scrapy-ctl.py</cite> (#199)
- there is only one global <cite>scrapy</cite> command now, instead of one <cite>scrapy-ctl.py</cite> per project
- Added <cite>scrapy.bat</cite> script for running more conveniently from Windows</li>
<li>Added bash completion to command-line tool (#210)</li>
<li>Renamed command <cite>start</cite> to <cite>runserver</cite> (#209)</li>
</ul>
</div>
<div class="section" id="api-changes">
<h5>API changes<a class="headerlink" href="#api-changes" title="Permalink to this headline">¶</a></h5>
<ul>
<li><p class="first"><tt class="docutils literal"><span class="pre">url</span></tt> and <tt class="docutils literal"><span class="pre">body</span></tt> attributes of Request objects are now read-only (#230)</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">Request.copy()</span></tt> and <tt class="docutils literal"><span class="pre">Request.replace()</span></tt> now also copies their <tt class="docutils literal"><span class="pre">callback</span></tt> and <tt class="docutils literal"><span class="pre">errback</span></tt> attributes (#231)</p>
</li>
<li><p class="first">Removed <tt class="docutils literal"><span class="pre">UrlFilterMiddleware</span></tt> from <tt class="docutils literal"><span class="pre">scrapy.contrib</span></tt> (already disabled by default)</p>
</li>
<li><p class="first">Offsite middelware doesn&#8217;t filter out any request coming from a spider that doesn&#8217;t have a allowed_domains attribute (#225)</p>
</li>
<li><p class="first">Removed Spider Manager <tt class="docutils literal"><span class="pre">load()</span></tt> method. Now spiders are loaded in the constructor itself.</p>
</li>
<li><dl class="first docutils">
<dt>Changes to Scrapy Manager (now called &#8220;Crawler&#8221;):</dt>
<dd><ul class="first last simple">
<li><tt class="docutils literal"><span class="pre">scrapy.core.manager.ScrapyManager</span></tt> class renamed to <tt class="docutils literal"><span class="pre">scrapy.crawler.Crawler</span></tt></li>
<li><tt class="docutils literal"><span class="pre">scrapy.core.manager.scrapymanager</span></tt> singleton moved to <tt class="docutils literal"><span class="pre">scrapy.project.crawler</span></tt></li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Moved module: <tt class="docutils literal"><span class="pre">scrapy.contrib.spidermanager</span></tt> to <tt class="docutils literal"><span class="pre">scrapy.spidermanager</span></tt></p>
</li>
<li><p class="first">Spider Manager singleton moved from <tt class="docutils literal"><span class="pre">scrapy.spider.spiders</span></tt> to the <tt class="docutils literal"><span class="pre">spiders`</span> <span class="pre">attribute</span> <span class="pre">of</span> <span class="pre">``scrapy.project.crawler</span></tt> singleton.</p>
</li>
<li><dl class="first docutils">
<dt>moved Stats Collector classes: (#204)</dt>
<dd><ul class="first last simple">
<li><tt class="docutils literal"><span class="pre">scrapy.stats.collector.StatsCollector</span></tt> to <tt class="docutils literal"><span class="pre">scrapy.statscol.StatsCollector</span></tt></li>
<li><tt class="docutils literal"><span class="pre">scrapy.stats.collector.SimpledbStatsCollector</span></tt> to <tt class="docutils literal"><span class="pre">scrapy.contrib.statscol.SimpledbStatsCollector</span></tt></li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">default per-command settings are now specified in the <tt class="docutils literal"><span class="pre">default_settings</span></tt> attribute of command object class (#201)</p>
</li>
<li><dl class="first docutils">
<dt>changed arguments of Item pipeline <tt class="docutils literal"><span class="pre">process_item()</span></tt> method from <tt class="docutils literal"><span class="pre">(spider,</span> <span class="pre">item)</span></tt> to <tt class="docutils literal"><span class="pre">(item,</span> <span class="pre">spider)</span></tt></dt>
<dd><ul class="first last simple">
<li>backwards compatibility kept (with deprecation warning)</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>moved <tt class="docutils literal"><span class="pre">scrapy.core.signals</span></tt> module to <tt class="docutils literal"><span class="pre">scrapy.signals</span></tt></dt>
<dd><ul class="first last simple">
<li>backwards compatibility kept (with deprecation warning)</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>moved <tt class="docutils literal"><span class="pre">scrapy.core.exceptions</span></tt> module to <tt class="docutils literal"><span class="pre">scrapy.exceptions</span></tt></dt>
<dd><ul class="first last simple">
<li>backwards compatibility kept (with deprecation warning)</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">added <tt class="docutils literal"><span class="pre">handles_request()</span></tt> class method to <tt class="docutils literal"><span class="pre">BaseSpider</span></tt></p>
</li>
<li><p class="first">dropped <tt class="docutils literal"><span class="pre">scrapy.log.exc()</span></tt> function (use <tt class="docutils literal"><span class="pre">scrapy.log.err()</span></tt> instead)</p>
</li>
<li><p class="first">dropped <tt class="docutils literal"><span class="pre">component</span></tt> argument of <tt class="docutils literal"><span class="pre">scrapy.log.msg()</span></tt> function</p>
</li>
<li><p class="first">dropped <tt class="docutils literal"><span class="pre">scrapy.log.log_level</span></tt> attribute</p>
</li>
<li><p class="first">Added <tt class="docutils literal"><span class="pre">from_settings()</span></tt> class methods to Spider Manager, and Item Pipeline Manager</p>
</li>
</ul>
</div>
<div class="section" id="id13">
<h5>Changes to settings<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Added <tt class="docutils literal"><span class="pre">HTTPCACHE_IGNORE_SCHEMES</span></tt> setting to ignore certain schemes on !HttpCacheMiddleware (#225)</li>
<li>Added <tt class="docutils literal"><span class="pre">SPIDER_QUEUE_CLASS</span></tt> setting which defines the spider queue to use (#220)</li>
<li>Added <tt class="docutils literal"><span class="pre">KEEP_ALIVE</span></tt> setting (#220)</li>
<li>Removed <tt class="docutils literal"><span class="pre">SERVICE_QUEUE</span></tt> setting (#220)</li>
<li>Removed <tt class="docutils literal"><span class="pre">COMMANDS_SETTINGS_MODULE</span></tt> setting (#201)</li>
<li>Renamed <tt class="docutils literal"><span class="pre">REQUEST_HANDLERS</span></tt> to <tt class="docutils literal"><span class="pre">DOWNLOAD_HANDLERS</span></tt> and make download handlers classes (instead of functions)</li>
</ul>
</div>
</div>
<div class="section" id="id14">
<h4>0.9<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<div class="section" id="id15">
<h5>New features and improvements<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Added SMTP-AUTH support to scrapy.mail</li>
<li>New settings added: <tt class="docutils literal"><span class="pre">MAIL_USER</span></tt>, <tt class="docutils literal"><span class="pre">MAIL_PASS</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2065">r2065</a> | #149)</li>
<li>Added new scrapy-ctl view command - To view URL in the browser, as seen by Scrapy (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2039">r2039</a>)</li>
<li>Added web service for controlling Scrapy process (this also deprecates the web console. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2053">r2053</a> | #167)</li>
<li>Support for running Scrapy as a service, for production systems (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1988">r1988</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2054">r2054</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2055">r2055</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2056">r2056</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2057">r2057</a> | #168)</li>
<li>Added wrapper induction library (documentation only available in source code for now). (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2011">r2011</a>)</li>
<li>Simplified and improved response encoding support (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1961">r1961</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1969">r1969</a>)</li>
<li>Added <tt class="docutils literal"><span class="pre">LOG_ENCODING</span></tt> setting (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1956">r1956</a>, documentation available)</li>
<li>Added <tt class="docutils literal"><span class="pre">RANDOMIZE_DOWNLOAD_DELAY</span></tt> setting (enabled by default) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1923">r1923</a>, doc available)</li>
<li><tt class="docutils literal"><span class="pre">MailSender</span></tt> is no longer IO-blocking (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1955">r1955</a> | #146)</li>
<li>Linkextractors and new Crawlspider now handle relative base tag urls (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1960">r1960</a> | #148)</li>
<li>Several improvements to Item Loaders and processors (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2022">r2022</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2023">r2023</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2024">r2024</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2025">r2025</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2026">r2026</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2027">r2027</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2028">r2028</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2029">r2029</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2030">r2030</a>)</li>
<li>Added support for adding variables to telnet console (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2047">r2047</a> | #165)</li>
<li>Support for requests without callbacks (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2050">r2050</a> | #166)</li>
</ul>
</div>
<div class="section" id="id16">
<h5>API changes<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Change <tt class="docutils literal"><span class="pre">Spider.domain_name</span></tt> to <tt class="docutils literal"><span class="pre">Spider.name</span></tt> (SEP-012, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1975">r1975</a>)</li>
<li><tt class="docutils literal"><span class="pre">Response.encoding</span></tt> is now the detected encoding (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1961">r1961</a>)</li>
<li><tt class="docutils literal"><span class="pre">HttpErrorMiddleware</span></tt> now returns None or raises an exception (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2006">r2006</a> | #157)</li>
<li><tt class="docutils literal"><span class="pre">scrapy.command</span></tt> modules relocation (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2035">r2035</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2036">r2036</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2037">r2037</a>)</li>
<li>Added <tt class="docutils literal"><span class="pre">ExecutionQueue</span></tt> for feeding spiders to scrape (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2034">r2034</a>)</li>
<li>Removed <tt class="docutils literal"><span class="pre">ExecutionEngine</span></tt> singleton (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2039">r2039</a>)</li>
<li>Ported <tt class="docutils literal"><span class="pre">S3ImagesStore</span></tt> (images pipeline) to use boto and threads (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2033">r2033</a>)</li>
<li>Moved module: <tt class="docutils literal"><span class="pre">scrapy.management.telnet</span></tt> to <tt class="docutils literal"><span class="pre">scrapy.telnet</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2047">r2047</a>)</li>
</ul>
</div>
<div class="section" id="changes-to-default-settings">
<h5>Changes to default settings<a class="headerlink" href="#changes-to-default-settings" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Changed default <tt class="docutils literal"><span class="pre">SCHEDULER_ORDER</span></tt> to <tt class="docutils literal"><span class="pre">DFO</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1939">r1939</a>)</li>
</ul>
</div>
</div>
<div class="section" id="id17">
<h4>0.8<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<div class="section" id="new-features">
<h5>New features<a class="headerlink" href="#new-features" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Added DEFAULT_RESPONSE_ENCODING setting (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1809">r1809</a>)</li>
<li>Added <tt class="docutils literal"><span class="pre">dont_click</span></tt> argument to <tt class="docutils literal"><span class="pre">FormRequest.from_response()</span></tt> method (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1813">r1813</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1816">r1816</a>)</li>
<li>Added <tt class="docutils literal"><span class="pre">clickdata</span></tt> argument to <tt class="docutils literal"><span class="pre">FormRequest.from_response()</span></tt> method (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1802">r1802</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1803">r1803</a>)</li>
<li>Added support for HTTP proxies (<tt class="docutils literal"><span class="pre">HttpProxyMiddleware</span></tt>) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1781">r1781</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1785">r1785</a>)</li>
<li>Offiste spider middleware now logs messages when filtering out requests (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1841">r1841</a>)</li>
</ul>
</div>
<div class="section" id="backwards-incompatible-changes">
<h5>Backwards-incompatible changes<a class="headerlink" href="#backwards-incompatible-changes" title="Permalink to this headline">¶</a></h5>
<ul>
<li><p class="first">Changed <tt class="docutils literal"><span class="pre">scrapy.utils.response.get_meta_refresh()</span></tt> signature (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1804">r1804</a>)</p>
</li>
<li><p class="first">Removed deprecated <tt class="docutils literal"><span class="pre">scrapy.item.ScrapedItem</span></tt> class - use <tt class="docutils literal"><span class="pre">scrapy.item.Item</span> <span class="pre">instead</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1838">r1838</a>)</p>
</li>
<li><p class="first">Removed deprecated <tt class="docutils literal"><span class="pre">scrapy.xpath</span></tt> module - use <tt class="docutils literal"><span class="pre">scrapy.selector</span></tt> instead. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1836">r1836</a>)</p>
</li>
<li><p class="first">Removed deprecated <tt class="docutils literal"><span class="pre">core.signals.domain_open</span></tt> signal - use <tt class="docutils literal"><span class="pre">core.signals.domain_opened</span></tt> instead (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1822">r1822</a>)</p>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">log.msg()</span></tt> now receives a <tt class="docutils literal"><span class="pre">spider</span></tt> argument (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1822">r1822</a>)</dt>
<dd><ul class="first last simple">
<li>Old domain argument has been deprecated and will be removed in 0.9. For spiders, you should always use the <tt class="docutils literal"><span class="pre">spider</span></tt> argument and pass spider references. If you really want to pass a string, use the <tt class="docutils literal"><span class="pre">component</span></tt> argument instead.</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Changed core signals <tt class="docutils literal"><span class="pre">domain_opened</span></tt>, <tt class="docutils literal"><span class="pre">domain_closed</span></tt>, <tt class="docutils literal"><span class="pre">domain_idle</span></tt></p>
</li>
<li><dl class="first docutils">
<dt>Changed Item pipeline to use spiders instead of domains</dt>
<dd><ul class="first last simple">
<li>The <tt class="docutils literal"><span class="pre">domain</span></tt> argument of  <tt class="docutils literal"><span class="pre">process_item()</span></tt> item pipeline method was changed to  <tt class="docutils literal"><span class="pre">spider</span></tt>, the new signature is: <tt class="docutils literal"><span class="pre">process_item(spider,</span> <span class="pre">item)</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1827">r1827</a> | #105)</li>
<li>To quickly port your code (to work with Scrapy 0.8) just use <tt class="docutils literal"><span class="pre">spider.domain_name</span></tt> where you previously used <tt class="docutils literal"><span class="pre">domain</span></tt>.</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Changed Stats API to use spiders instead of domains (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1849">r1849</a> | #113)</dt>
<dd><ul class="first last simple">
<li><tt class="docutils literal"><span class="pre">StatsCollector</span></tt> was changed to receive spider references (instead of domains) in its methods (<tt class="docutils literal"><span class="pre">set_value</span></tt>, <tt class="docutils literal"><span class="pre">inc_value</span></tt>, etc).</li>
<li>added <tt class="docutils literal"><span class="pre">StatsCollector.iter_spider_stats()</span></tt> method</li>
<li>removed <tt class="docutils literal"><span class="pre">StatsCollector.list_domains()</span></tt> method</li>
<li>Also, Stats signals were renamed and now pass around spider references (instead of domains). Here&#8217;s a summary of the changes:</li>
<li>To quickly port your code (to work with Scrapy 0.8) just use <tt class="docutils literal"><span class="pre">spider.domain_name</span></tt> where you previously used <tt class="docutils literal"><span class="pre">domain</span></tt>. <tt class="docutils literal"><span class="pre">spider_stats</span></tt> contains exactly the same data as <tt class="docutils literal"><span class="pre">domain_stats</span></tt>.</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">CloseDomain</span></tt> extension moved to <tt class="docutils literal"><span class="pre">scrapy.contrib.closespider.CloseSpider</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1833">r1833</a>)</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt>Its settings were also renamed:</dt>
<dd><ul class="first last simple">
<li><tt class="docutils literal"><span class="pre">CLOSEDOMAIN_TIMEOUT</span></tt> to <tt class="docutils literal"><span class="pre">CLOSESPIDER_TIMEOUT</span></tt></li>
<li><tt class="docutils literal"><span class="pre">CLOSEDOMAIN_ITEMCOUNT</span></tt> to <tt class="docutils literal"><span class="pre">CLOSESPIDER_ITEMCOUNT</span></tt></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Removed deprecated <tt class="docutils literal"><span class="pre">SCRAPYSETTINGS_MODULE</span></tt> environment variable - use <tt class="docutils literal"><span class="pre">SCRAPY_SETTINGS_MODULE</span></tt> instead (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1840">r1840</a>)</p>
</li>
<li><p class="first">Renamed setting: <tt class="docutils literal"><span class="pre">REQUESTS_PER_DOMAIN</span></tt> to <tt class="docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_SPIDER</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1830">r1830</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1844">r1844</a>)</p>
</li>
<li><p class="first">Renamed setting: <tt class="docutils literal"><span class="pre">CONCURRENT_DOMAINS</span></tt> to <tt class="docutils literal"><span class="pre">CONCURRENT_SPIDERS</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1830">r1830</a>)</p>
</li>
<li><p class="first">Refactored HTTP Cache middleware</p>
</li>
<li><p class="first">HTTP Cache middleware has been heavilty refactored, retaining the same functionality except for the domain sectorization which was removed. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1843">r1843</a> )</p>
</li>
<li><p class="first">Renamed exception: <tt class="docutils literal"><span class="pre">DontCloseDomain</span></tt> to <tt class="docutils literal"><span class="pre">DontCloseSpider</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1859">r1859</a> | #120)</p>
</li>
<li><p class="first">Renamed extension: <tt class="docutils literal"><span class="pre">DelayedCloseDomain</span></tt> to <tt class="docutils literal"><span class="pre">SpiderCloseDelay</span></tt> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1861">r1861</a> | #121)</p>
</li>
<li><p class="first">Removed obsolete <tt class="docutils literal"><span class="pre">scrapy.utils.markup.remove_escape_chars</span></tt> function - use <tt class="docutils literal"><span class="pre">scrapy.utils.markup.replace_escape_chars</span></tt> instead (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1865">r1865</a>)</p>
</li>
</ul>
</div>
</div>
<div class="section" id="id18">
<h4>0.7<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h4>
<p>First release of Scrapy.</p>
</div>
</div>
<span id="document-contributing"></span><div class="section" id="contributing-to-scrapy">
<span id="topics-contributing"></span><h3>Contributing to Scrapy<a class="headerlink" href="#contributing-to-scrapy" title="Permalink to this headline">¶</a></h3>
<p>There are many ways to contribute to Scrapy. Here are some of them:</p>
<ul class="simple">
<li>Blog about Scrapy. Tell the world how you&#8217;re using Scrapy. This will help
newcomers with more examples and the Scrapy project to increase its
visibility.</li>
<li>Report bugs and request features in the <a class="reference external" href="https://github.com/scrapy/scrapy/issues">issue tracker</a>, trying to follow
the guidelines detailed in <a class="reference internal" href="#reporting-bugs">Reporting bugs</a> below.</li>
<li>Submit patches for new functionality and/or bug fixes. Please read
<a class="reference internal" href="#writing-patches">Writing patches</a> and <a class="reference internal" href="#submitting-patches">Submitting patches</a> below for details on how to
write and submit a patch.</li>
<li>Join the <a class="reference external" href="http://groups.google.com/group/scrapy-users">scrapy-users</a> mailing list and share your ideas on how to
improve Scrapy. We&#8217;re always open to suggestions.</li>
</ul>
<div class="section" id="reporting-bugs">
<h4>Reporting bugs<a class="headerlink" href="#reporting-bugs" title="Permalink to this headline">¶</a></h4>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Please report security issues <strong>only</strong> to
<a class="reference external" href="mailto:scrapy-security&#37;&#52;&#48;googlegroups&#46;com">scrapy-security<span>&#64;</span>googlegroups<span>&#46;</span>com</a>. This is a private list only open to
trusted Scrapy developers, and its archives are not public.</p>
</div>
<p>Well-written bug reports are very helpful, so keep in mind the following
guidelines when reporting a new bug.</p>
<ul class="simple">
<li>check the <a class="reference internal" href="index.html#faq"><em>FAQ</em></a> first to see if your issue is addressed in a
well-known question</li>
<li>check the <a class="reference external" href="https://github.com/scrapy/scrapy/issues">open issues</a> to see if it has already been reported. If it has,
don&#8217;t dismiss the report but check the ticket history and comments, you may
find additional useful information to contribute.</li>
<li>search the <a class="reference external" href="http://groups.google.com/group/scrapy-users">scrapy-users</a> list to see if it has been discussed there, or
if you&#8217;re not sure if what you&#8217;re seeing is a bug. You can also ask in the
<cite>#scrapy</cite> IRC channel.</li>
<li>write complete, reproducible, specific bug reports. The smaller the test
case, the better. Remember that other developers won&#8217;t have your project to
reproduce the bug, so please include all relevant files required to reproduce
it.</li>
<li>include the output of <tt class="docutils literal"><span class="pre">scrapy</span> <span class="pre">version</span> <span class="pre">-v</span></tt> so developers working on your bug
know exactly which version and platform it occurred on, which is often very
helpful for reproducing it, or knowing if it was already fixed.</li>
</ul>
</div>
<div class="section" id="writing-patches">
<h4>Writing patches<a class="headerlink" href="#writing-patches" title="Permalink to this headline">¶</a></h4>
<p>The better written a patch is, the higher chance that it&#8217;ll get accepted and
the sooner that will be merged.</p>
<p>Well-written patches should:</p>
<ul class="simple">
<li>contain the minimum amount of code required for the specific change. Small
patches are easier to review and merge. So, if you&#8217;re doing more than one
change (or bug fix), please consider submitting one patch per change. Do not
collapse multiple changes into a single patch. For big changes consider using
a patch queue.</li>
<li>pass all unit-tests. See <a class="reference internal" href="#running-tests">Running tests</a> below.</li>
<li>include one (or more) test cases that check the bug fixed or the new
functionality added. See <a class="reference internal" href="#writing-tests">Writing tests</a> below.</li>
<li>if you&#8217;re adding or changing a public (documented) API, please include
the documentation changes in the same patch.  See <a class="reference internal" href="#documentation-policies">Documentation policies</a>
below.</li>
</ul>
</div>
<div class="section" id="submitting-patches">
<h4>Submitting patches<a class="headerlink" href="#submitting-patches" title="Permalink to this headline">¶</a></h4>
<p>The best way to submit a patch is to issue a <a class="reference external" href="http://help.github.com/send-pull-requests/">pull request</a> on Github,
optionally creating a new issue first.</p>
<p>Remember to explain what was fixed or the new functionality (what it is, why
it&#8217;s needed, etc). The more info you include, the easier will be for core
developers to understand and accept your patch.</p>
<p>You can also discuss the new functionality (or bug fix) before creating the
patch, but it&#8217;s always good to have a patch ready to illustrate your arguments
and show that you have put some additional thought into the subject. A good
starting point is to send a pull request on Github. It can be simple enough to
illustrate your idea, and leave documentation/tests for later, after the idea
has been validated and proven useful. Alternatively, you can send an email to
<a class="reference external" href="http://groups.google.com/group/scrapy-users">scrapy-users</a> to discuss your idea first.</p>
<p>Finally, try to keep aesthetic changes (<span class="target" id="index-0"></span><a class="pep reference external" href="http://www.python.org/dev/peps/pep-0008"><strong>PEP 8</strong></a> compliance, unused imports
removal, etc) in separate commits than functional changes. This will make pull
requests easier to review and more likely to get merged.</p>
</div>
<div class="section" id="coding-style">
<h4>Coding style<a class="headerlink" href="#coding-style" title="Permalink to this headline">¶</a></h4>
<p>Please follow these coding conventions when writing code for inclusion in
Scrapy:</p>
<ul class="simple">
<li>Unless otherwise specified, follow <span class="target" id="index-1"></span><a class="pep reference external" href="http://www.python.org/dev/peps/pep-0008"><strong>PEP 8</strong></a>.</li>
<li>It&#8217;s OK to use lines longer than 80 chars if it improves the code
readability.</li>
<li>Don&#8217;t put your name in the code you contribute. Our policy is to keep
the contributor&#8217;s name in the <a class="reference external" href="https://github.com/scrapy/scrapy/blob/master/AUTHORS">AUTHORS</a> file distributed with Scrapy.</li>
</ul>
</div>
<div class="section" id="scrapy-contrib">
<h4>Scrapy Contrib<a class="headerlink" href="#scrapy-contrib" title="Permalink to this headline">¶</a></h4>
<p>Scrapy contrib shares a similar rationale as Django contrib, which is explained
in <a class="reference external" href="http://jacobian.org/writing/what-is-django-contrib/">this post</a>. If you
are working on a new functionality, please follow that rationale to decide
whether it should be a Scrapy contrib. If unsure, you can ask in
<a class="reference external" href="http://groups.google.com/group/scrapy-users">scrapy-users</a>.</p>
</div>
<div class="section" id="documentation-policies">
<h4>Documentation policies<a class="headerlink" href="#documentation-policies" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><strong>Don&#8217;t</strong> use docstrings for documenting classes, or methods which are
already documented in the official (sphinx) documentation. For example, the
<tt class="xref py py-meth docutils literal"><span class="pre">ItemLoader.add_value()</span></tt> method should be documented in the sphinx
documentation, not its docstring.</li>
<li><strong>Do</strong> use docstrings for documenting functions not present in the official
(sphinx) documentation, such as functions from <tt class="docutils literal"><span class="pre">scrapy.utils</span></tt> package and
its sub-modules.</li>
</ul>
</div>
<div class="section" id="tests">
<h4>Tests<a class="headerlink" href="#tests" title="Permalink to this headline">¶</a></h4>
<p>Tests are implemented using the <a class="reference external" href="http://twistedmatrix.com/documents/current/core/development/policy/test-standard.html">Twisted unit-testing framework</a> called
<tt class="docutils literal"><span class="pre">trial</span></tt>.</p>
<div class="section" id="running-tests">
<h5>Running tests<a class="headerlink" href="#running-tests" title="Permalink to this headline">¶</a></h5>
<p>To run all tests go to the root directory of Scrapy source code and run:</p>
<blockquote>
<div><p><tt class="docutils literal"><span class="pre">bin/runtests.sh</span></tt> (on unix)</p>
<p><tt class="docutils literal"><span class="pre">bin\runtests.bat</span></tt> (on windows)</p>
</div></blockquote>
<p>To run a specific test (say <tt class="docutils literal"><span class="pre">scrapy.tests.test_contrib_loader</span></tt>) use:</p>
<blockquote>
<div><p><tt class="docutils literal"><span class="pre">bin/runtests.sh</span> <span class="pre">scrapy.tests.test_contrib_loader</span></tt> (on unix)</p>
<p><tt class="docutils literal"><span class="pre">bin\runtests.bat</span> <span class="pre">scrapy.tests.test_contrib_loader</span></tt> (on windows)</p>
</div></blockquote>
</div>
<div class="section" id="writing-tests">
<h5>Writing tests<a class="headerlink" href="#writing-tests" title="Permalink to this headline">¶</a></h5>
<p>All functionality (including new features and bug fixes) must include a test
case to check that it works as expected, so please include tests for your
patches if you want them to get accepted sooner.</p>
<p>Scrapy uses unit-tests, which are located in the <tt class="docutils literal"><span class="pre">scrapy.tests</span></tt> package
(<a class="reference external" href="https://github.com/scrapy/scrapy/tree/master/scrapy/tests">scrapy/tests</a> directory). Their module name typically resembles the full
path of the module they&#8217;re testing. For example, the item loaders code is in:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy.contrib.loader
</pre></div>
</div>
<p>And their unit-tests are in:</p>
<div class="highlight-none"><div class="highlight"><pre>scrapy.tests.test_contrib_loader
</pre></div>
</div>
</div>
</div>
</div>
<span id="document-versioning"></span><div class="section" id="versioning-and-api-stability">
<span id="versioning"></span><h3>Versioning and API Stability<a class="headerlink" href="#versioning-and-api-stability" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id1">
<h4>Versioning<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>Scrapy uses the <a class="reference external" href="http://en.wikipedia.org/wiki/Software_versioning#Odd-numbered_versions_for_development_releases">odd-numbered versions for development releases</a>.</p>
<p>There are 3 numbers in a Scrapy version: <em>A.B.C</em></p>
<ul class="simple">
<li><em>A</em> is the major version. This will rarely change and will signify very
large changes. So far, only zero is available for <em>A</em> as Scrapy hasn&#8217;t yet
reached 1.0.</li>
<li><em>B</em> is the release number. This will include many changes including features
and things that possibly break backwards compatibility. Even Bs will be
stable branches, and odd Bs will be development.</li>
<li><em>C</em> is the bugfix release number.</li>
</ul>
<p>For example:</p>
<ul class="simple">
<li><em>0.14.1</em> is the first bugfix release of the <em>0.14</em> series (safe to use in
production)</li>
</ul>
</div>
<div class="section" id="api-stability">
<h4>API Stability<a class="headerlink" href="#api-stability" title="Permalink to this headline">¶</a></h4>
<p>API stability is one of Scrapy major goals for the <em>1.0</em> release, which doesn&#8217;t
have a due date scheduled yet.</p>
<p>Methods or functions that start with a single dash (<tt class="docutils literal"><span class="pre">_</span></tt>) are private and
should never be relied as stable. Besides those, the plan is to stabilize and
document the entire API, as we approach the 1.0 release.</p>
<p>Also, keep in mind that stable doesn&#8217;t mean complete: stable APIs could grow
new methods or functionality but the existing methods should keep working the
same way.</p>
</div>
</div>
<span id="document-experimental/index"></span><div class="section" id="experimental-features">
<span id="experimental"></span><h3>Experimental features<a class="headerlink" href="#experimental-features" title="Permalink to this headline">¶</a></h3>
<p>This section documents experimental Scrapy features that may become stable in
future releases, but whose API is not yet stable. Use them with caution, and
subscribe to the <a class="reference external" href="http://scrapy.org/community/">mailing lists</a> to get
notified of any changes.</p>
<p>Since it&#8217;s not revised so frequently, this section may contain documentation
which is outdated, incomplete or overlapping with stable documentation (until
it&#8217;s properly merged) . Use at your own risk.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This documentation is a work in progress. Use at your own risk.</p>
</div>
<div class="section" id="add-commands-using-external-libraries">
<h4>Add commands using external libraries<a class="headerlink" href="#add-commands-using-external-libraries" title="Permalink to this headline">¶</a></h4>
<p>You can also add Scrapy commands from an external library by adding <cite>scrapy.commands</cite> section into entry_points in the <cite>setup.py</cite>.</p>
<p>The following example adds <cite>my_command</cite> command:</p>
<div class="highlight-none"><div class="highlight"><pre>from setuptools import setup, find_packages

setup(name=&#39;scrapy-mymodule&#39;,
  entry_points={
    &#39;scrapy.commands&#39;: [
      &#39;my_command=my_scrapy_module.commands:MyCommand&#39;,
    ],
  },
 )
</pre></div>
</div>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-news"><em>Release notes</em></a></dt>
<dd>See what has changed in recent Scrapy versions.</dd>
<dt><a class="reference internal" href="index.html#document-contributing"><em>Contributing to Scrapy</em></a></dt>
<dd>Learn how to contribute to the Scrapy project.</dd>
<dt><a class="reference internal" href="index.html#document-versioning"><em>Versioning and API Stability</em></a></dt>
<dd>Understand Scrapy versioning and API stability.</dd>
<dt><a class="reference internal" href="index.html#document-experimental/index"><em>Experimental features</em></a></dt>
<dd>Learn about bleeding-edge features.</dd>
</dl>
</div>
</div>


          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2008-2013, Scrapy developers.
      Last updated on Jun 26, 2014.
    </p>
  </div>

  <a href="https://github.com/snide/sphinx_rtd_theme">Sphinx theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>
</footer>
        </div>
      </div>

    </section>

  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: 0.24
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      <dl>
        <dt>Versions</dt>
        
          <dd><a href="/en/master/">master</a></dd>
        
          <dd><a href="/en/latest/">latest</a></dd>
        
          <dd><a href="/en/0.24/">0.24</a></dd>
        
          <dd><a href="/en/0.20/">0.20</a></dd>
        
          <dd><a href="/en/0.18/">0.18</a></dd>
        
          <dd><a href="/en/0.16/">0.16</a></dd>
        
          <dd><a href="/en/0.14/">0.14</a></dd>
        
          <dd><a href="/en/0.12/">0.12</a></dd>
        
          <dd><a href="/en/0.10.3/">0.10.3</a></dd>
        
          <dd><a href="/en/0.9/">0.9</a></dd>
        
          <dd><a href="/en/0.8/">0.8</a></dd>
        
          <dd><a href="/en/0.7/">0.7</a></dd>
        
      </dl>
      <dl>
        <dt>Downloads</dt>
        
      </dl>
      <dl>
        <dt>On Read the Docs</dt>
          <dd>
            <a href="//readthedocs.org/projects/scrapy/?fromdocs=scrapy">Project Home</a>
          </dd>
          <dd>
            <a href="//readthedocs.org/builds/scrapy/?fromdocs=scrapy">Builds</a>
          </dd>
      </dl>
      <hr/>
      Free document hosting provided by <a href="http://www.readthedocs.org">Read the Docs</a>.

    </div>
  </div>



  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.24.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>